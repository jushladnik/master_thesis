{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If cuda is not working first try reinstalling torch and restarting computer, if this does not work you are fucked yet again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from top2vec import Top2Vec\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top2vec on news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ROOT_PATH = '/home/jhladnik/'\n",
    "\n",
    "NUM_SAMPLES = 5000\n",
    "df = pd.read_parquet(\n",
    "    f'{ROOT_PATH}/data/eventregistry/df_news_lemmas_{NUM_SAMPLES}.parquet.gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_text_string\"] = df.lemmatized_text.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 20:19:09,604 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-01-24 20:19:11,973 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-01-24 20:21:49,926 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-01-24 20:22:16,130 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-01-24 20:22:16,262 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "# 3 min on remote  for 5k samples\n",
    "model = Top2Vec(documents=list(df[\"lemmatized_text_string\"]), speed=\"learn\", workers=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluation import get_top_words_for_topics_bertopic, topic_diversity\n",
    "\n",
    "topics_representations = topic_words\n",
    "#topics_representations = get_top_words_for_topics_bertopic(topic_model, 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate topic diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "topic_diversity(topics_representations, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calcualte topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1300551227167047"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def topic_coherence(trained_model, topic_top_words, top_k_words=25):\n",
    "    if topic_top_words is None:\n",
    "        return -1\n",
    "    \n",
    "    if top_k_words > len(topic_top_words[0]):\n",
    "        print(\"top_k_words is larger than the number of words in the topic\")\n",
    "        return -1\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        topics=topic_top_words,\n",
    "        texts=list(df['lemmatized_text']),\n",
    "        dictionary=Dictionary(list(df['lemmatized_text'])),\n",
    "        coherence='c_npmi',\n",
    "        topn=top_k_words)\n",
    "\n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "topic_coherence(None, topics_representations, top_k_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top2vec on tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ROOT_PATH = '/home/jhladnik/'\n",
    "\n",
    "NUM_SAMPLES = 50000\n",
    "df = pd.read_parquet(\n",
    "    f'{ROOT_PATH}/data/sl-tweets/df_tweets_lemmas_{NUM_SAMPLES}.parquet.gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_text_string\"] = df.lemmatized_text.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 20:31:49,856 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-01-24 20:31:51,284 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-01-24 20:38:10,869 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-01-24 20:38:44,661 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-01-24 20:38:48,398 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min 11s, sys: 4min 12s, total: 29min 23s\n",
      "Wall time: 6min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 3 min on remote  for 5k samples\n",
    "model = Top2Vec(documents=list(df[\"lemmatized_text_string\"]), speed=\"learn\", workers=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluation import get_top_words_for_topics_bertopic, topic_diversity\n",
    "\n",
    "topics_representations = topic_words\n",
    "#topics_representations = get_top_words_for_topics_bertopic(topic_model, 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate topic diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24805555555555556"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "topic_diversity(topics_representations, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calcualte topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.41373743560966486"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def topic_coherence(trained_model, topic_top_words, top_k_words=25):\n",
    "    if topic_top_words is None:\n",
    "        return -1\n",
    "    \n",
    "    if top_k_words > len(topic_top_words[0]):\n",
    "        print(\"top_k_words is larger than the number of words in the topic\")\n",
    "        return -1\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        topics=topic_top_words,\n",
    "        texts=list(df['lemmatized_text']),\n",
    "        dictionary=Dictionary(list(df['lemmatized_text'])),\n",
    "        coherence='c_npmi',\n",
    "        topn=top_k_words)\n",
    "\n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "topic_coherence(None, topics_representations, top_k_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b205b86b5cca952d03a16e638da7174a06c83fcc310bd34d2a8c2608236d81e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
