{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If cuda is not working first try reinstalling torch and restarting computer, if this does not work you are fucked yet again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from top2vec import Top2Vec\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top2vec on news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 124 ms, sys: 31.8 ms, total: 156 ms\n",
      "Wall time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "ROOT_PATH = '/home/jhladnik/'\n",
    "\n",
    "NUM_SAMPLES = 1000\n",
    "df = pd.read_parquet(\n",
    "    f'{ROOT_PATH}/data/eventregistry/df_news_lemmas_{NUM_SAMPLES}.parquet.gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "start = time.time()\n",
    "dictionary = Dictionary(list(df['lemmatized_text']))\n",
    "corpus = [dictionary.doc2bow(text) for text in list(df['lemmatized_text'])]\n",
    "\n",
    "def join_lemmas(list_lemmas):\n",
    "    return ' '.join(list_lemmas)\n",
    "df['lemmatized_body'] = df['lemmatized_text'].apply(join_lemmas)\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "results_dict = {\"model_name\": [], \"num_topics\": [],\n",
    "\"topic_diversity\": [], \"topic_coherence_npmi\": [], \"topic_coherence_umass\":[], \"seed\":[]}\n",
    "\n",
    "top_n_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_params = [5]#, 8, 10, 15, 20, 25, 30]\n",
    "seeds = [1]#, 2, 3]\n",
    "for num_topics in tqdm.tqdm(num_topics_params):\n",
    "    for seed in seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 14:09:46,859 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-01-29 14:09:47,300 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-01-29 14:10:00,125 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-01-29 14:10:13,890 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-01-29 14:10:13,914 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "# 3 min on remote  for 5k samples\n",
    "# 30s on remote for 1k samples\n",
    "trained_model = Top2Vec(documents=list(df[\"lemmatized_body\"]), speed=\"learn\", workers=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words_for_topics_top2vec(trained_model, top_n_words, num_topics=15):\n",
    "    topic_words, word_scores, topic_nums = model.get_topics()\n",
    "    num_identified_topics = len(topic_words)\n",
    "    if num_identified_topics < num_topics:\n",
    "        topic_representations = topic_words\n",
    "        print(f\"identified {num_identified_topics} topics, but {num_topics} were requested\")\n",
    "    else:\n",
    "        topic_mapping = trained_model.hierarchical_topic_reduction(num_topics=num_topics)\n",
    "        topic_representations = model.topic_words_reduced\n",
    "    \n",
    "    topic_representations = [topic_words[:top_n_words] for topic_words in topic_representations]\n",
    "    return topic_representations\n",
    "\n",
    "top_n_words = 10\n",
    "num_topics = 6\n",
    "trained_model = model\n",
    "topics_representations = get_top_words_for_topics_top2vec(model, top_n_words=top_n_words, num_topics=num_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import topic_diversity, topic_coherence\n",
    "results_dict[\"model_name\"].append(\"top2vec\")\n",
    "results_dict[\"num_topics\"].append(num_topics)\n",
    "results_dict[\"topic_diversity\"].append(topic_diversity(topics_representations, top_n_words))\n",
    "results_dict[\"topic_coherence_npmi\"].append(topic_coherence(df, trained_model, topics_representations, top_k_words=top_n_words, coherence_metric=\"c_npmi\"))\n",
    "results_dict[\"topic_coherence_umass\"].append(topic_coherence(df, trained_model, topics_representations, top_k_words=top_n_words, coherence_metric=\"u_mass\"))\n",
    "results_dict[\"seed\"].append(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': ['top2vec', 'top2vec', 'top2vec', 'top2vec'],\n",
       " 'num_topics': [6, 6, 6],\n",
       " 'topic_diversity': [1.0, 1.0, 1.0],\n",
       " 'topic_coherence_npmi': [-0.007363732517235069, -0.007363732517235069],\n",
       " 'topic_coherence_umass': [-1.824472197745601, -1.824472197745601],\n",
       " 'seed': []}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top2vec on tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ROOT_PATH = '/home/jhladnik/'\n",
    "\n",
    "NUM_SAMPLES = 50000\n",
    "df = pd.read_parquet(\n",
    "    f'{ROOT_PATH}/data/sl-tweets/df_tweets_lemmas_{NUM_SAMPLES}.parquet.gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_text_string\"] = df.lemmatized_text.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-24 20:35:20,369 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-01-24 20:35:21,929 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-01-24 20:41:09,343 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-01-24 20:41:55,396 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-01-24 20:41:58,815 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 12s, sys: 3min 28s, total: 27min 41s\n",
      "Wall time: 6min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 6 min on remote  for 50k samples\n",
    "model = Top2Vec(documents=list(df[\"lemmatized_text_string\"]), speed=\"learn\", workers=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluation import get_top_words_for_topics_bertopic, topic_diversity\n",
    "\n",
    "topics_representations = topic_words\n",
    "#topics_representations = get_top_words_for_topics_bertopic(topic_model, 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate topic diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21225"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "topic_diversity(topics_representations, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calcualte topic coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.41547616656131436"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "\n",
    "topic_coherence(None, topics_representations, top_k_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b205b86b5cca952d03a16e638da7174a06c83fcc310bd34d2a8c2608236d81e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
