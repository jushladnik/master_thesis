{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2a4f82a-9fb5-43ac-85c8-542d19f99b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import smart_open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292d8ce-9dcf-48a1-ae42-65164e807d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "    with smart_open.open(url, \"rb\") as file:\n",
    "        with tarfile.open(fileobj=file) as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', member.name):\n",
    "                    member_bytes = tar.extractfile(member).read()\n",
    "                    yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "docs = list(extract_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5040d8-8c1a-4ea9-ae8e-ede99e464e29",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['387 \\nNeural Net and Traditional Classifiers \\x7f \\nWilliam Y. Huang and Richard P. Lippmann \\nMIT Lincoln Laboratory \\nLexington, MA 02173, USA \\nAbstract\\nPrevious work on nets with continuous-valued inputs led to generative \\nprocedures to construct convex decision regions with two-layer percepttons (one hidden \\nlayer) and arbitrary decision regions with three-layer percepttons (two hidden layers). \\nHere we demonstrate that two-layer perceptton classifiers trained with back propagation \\ncan form both convex and disjoint decision regions. Such classifiers are robust, train \\nrapidly, and provide good performance with simple decision regions. When complex \\ndecision regions are required, however, convergence time can be excessively long and \\nperformance is often no better than that of k-nearest neighbor classifiers. Three neural \\nnet classifiers are presented that provide more rapid training under such situations. \\nTwo use fixed weights in the first one or two layers and are similar to classifiers that \\nestimate probability density functions using histograms. A third \"feature map classifier\" \\nuses both unsupervised and supervised training. It provides good performance with \\nlittle supervised training in situations such as speech recognition where much unlabeled \\ntraining data is available. The architecture of this classifier can be used to implement \\na neural net k-nearest neighbor classifier. \\n1. INTRODUCTION \\nNeural net architectures can be used to construct many different types of classi- \\ntiers [7]. In particular, multi-layer perceptron classifiers with continuous valued in- \\nputs trained with back propagation are robust, often train rapidly, and provide perfor- \\nmance similar to that provided by Gaussian classifiers when decision regions are convex \\n[12,7,5,8]. Generative procedures demonstrate that such classifiers can form convex deci- \\nsion regions with two-layer perceptrons (one hidden layer) and arbitrary decision regions \\nwith three-layer perceptrons (two hidden layers) [7,2,9]. More recent work has demon- \\nstrated that two-layer perceptrons can form non-convex and disjoint decision regions. \\nExamples of hand crafted two-layer networks which generate such decision regions are \\npresented in this paper along with Monte Carlo simulations where complex decision \\nregions were generated using back propagation training. These and previous simula- \\ntions [5,8] demonstrate that convergence time with back propagation can be excessive \\nwhen complex decision regions are desired and performance is often no better than that \\nobtained with k-nearest neighbor classifiers [4]. These results led us to explore other \\nneural net classifiers that might provide faster convergence. Three classifiers called, \\n\"fixed weight,\" \"hypercube,\" and \"feature map\" classifiers, were developed and eval- \\nuated. All classifiers were tested on illustrative problems with two continuous-valued \\ninputs and two classes (A and B). A more restricted set of classifiers was tested with \\nvowel formant data. \\n2. CAPABILITIES OF TWO LAYER PERCEPTRONS \\nMulti-layer perceptron classifiers with hard-limiting nonlinearities (node outputs \\nof 0 or 1) and continuous-valued inputs can form complex decision regions. Simple \\nconstructive proofs demonstrate that a three-layer perceptron (two hidden layers) can \\n\\x7f This work was sponsored by the Defense Advanced Research Projects Agency and the Department \\nof the Air Force. The views expressed are those of the authors and do not reflect the policy or position \\nof the U.S. Government. \\nAmerican Institute of Physics 1988 \\n388 \\nb\\x7f I b\\x7f2 \\nXl x 2 \\nDECISION REGION FOR CLASS A \\nx I bl b2 \\n\\x7f b6 ........ \\n-2 \\nI I \\nI \\n\\x7f I \\nI \\no I 2 \\n3 4 \\nXl \\nFIG. 1. A two-layer perceptton that forms disjoint decision re9ions for class A (shaded areas). Connec- \\ntion weights and node offsets are shown in the left. Hyperplanes formed by all hidden nodes are drawn \\nas dashed lines with node labels. Arrows on these lines point to the half plane where the hidden node \\noutput is \"high\" \\nform arbitrary decision regions and a two-layer perceptron (one hidden layer) can form \\nsingle convex decision regions [7,2,9]. Recently, however, it has been demonstrated that \\ntwo-layer perceptrons can form decision regions that are not simply convex [14]. Fig. 1, \\nfor example, shows how disjoint decision regions can be generated using a two-layer \\nperceptron. The two disjoint shaded areas in this Fig. represent the decision region \\nfor class A (output node has a \"high\" output, y = 1). The remaining area represents \\nthe decision region for class B (output node has a \"low\" output, y = 0). Nodes in \\nthis Fig. contain hard-limiting nonlinearities. Connection weights and node offsets are \\nindicated in the left diagram. Ten other complex decision regions formed using two-layer \\nperceptrons are presented in Fig. 2. \\nThe above examples suggest that two-layer perceptrons can form decision regions \\nwith arbitrary shapes. We, however, know of no general proof of this capability. A \\n1965 book by Nilson discusses this issue and contains a proof that two-layer nets can \\ndivide a finite number of points into two arbitrary sets ([10] page 89). This proof \\ninvolves separating M points using at most M - 1 parallel hyperplanes formed by first- \\nlayer nodes where no hyperplane intersects two or more points. Proving that a given \\ndecision region can be formed in a two-layer net involves testing to determine whether \\nthe Boolean representations at the output of the first layer for all points within the \\ndecision region for class A are linearly separable from the Boolean representations for \\nclass B. One test for linear separability was presented in 1962 [13]. \\nA problem with forming complex decision regions with two-layer percepttons is that \\nweights and offsets must be adjusted carefully because they interact extensively to form \\ndecision regions. Fig. 1 illustrates this sensitivity problem. Here it can be seen that \\nweights to one hidden node form a hyperplane which influences decision regions in \\nan entire halLplane. For example, small errors in first layer weights that results in a \\nchange in the slopes of hyperplanes b$ and b6 might only slightly extend the A\\x7f region \\nbut completely eliminate the A2 region. This interdependence can be eliminated in \\nthree layer perceptrons. \\nIt is possible to train two-layer percepttons to form complex decision regions using \\nback propagation and sigmoidal nonlinearities despite weight interactions. Fig. 3, for \\nexample, shows disjoint decision regions formed using back propagation for the problem \\nof Fig. 1. In this and all other simulations, inputs were presented alternately from \\nclasses A and B and selected from a uniform distribution covering the desired decision \\nregion. In addition, the back propagation rate of descent term, r/, was set equal to the \\nmomentum gain term, a and r/= a = .01. Small values for r/and a were necessary to \\nguarantee convergence for the difficult problems in Fig. 2. Other simulation details are \\n389 \\nI) ) s) ) I \\nFIG. 2. Ten complex decision regions formed by two-layer perceptrons. The numbers assigned to each \\ncase are the acase\"numbers used in the rest of this paper. \\nas in [5,8]. Also shown in Fig. 3 are hyperplanes formed by those first-layer nodes with \\nthe strongest connection weights to the output node. These hyperplanes and weights \\nare similar to those in the networks created by hand except for sign inversions, the \\noccurrence of multiple similar hyperplanes formed by two nodes, and the use of node \\noffsets with values near zero. \\n3. COMPARATIVE RESULTS OF TwO-LAYERS VS. THREE-LAYERS \\nPrevious results [5,8], as well as the weight interactions mentioned above, suggest \\nthat three-layer percepttons may be able to form complex decision regions faster with \\nback propagation than two-layer percepttons. This was explored using Monte Carlo \\nsimulations for the first nine cases of Fig. 2. All networks have 32 nodes in the first \\nhidden layer. The number of nodes in the second hidden layer was twice the number \\nof convex regions needed to form the decision region (2, 4, 6, 4, 6, 6, 8, 6 and 6 for \\nCases I through 9 respectively). Ten runs were typically averaged together to obtain \\na smooth curve of percentage error vs. time (number of training trials) and enough \\ntrials were run (to a limit of 250,000) until the curve appeared to fiatten out with little \\nimprovement over time. The error curve was then low-pass filtered to determine the \\nconvergence time. Convergence time was defined as the time when the curve crossed a \\nvalue 5 percentage points above the final percentage error. This definition provides a \\nframework for comparing the convergence time of the different classifiers. It, however, is \\nnot the time after which error rates do not improve. Fig. 4 summarizes results in terms \\nof convergence time and final percentage error. In those cases with disjoint decision \\nregions, back propagation sometimes failed to form separate regions after 250,000 trials. \\nFor example, the two disjoint regions required in Case 2 were never fully separated with \\n390 \\nFIG. 3. Decision regions formed using back propagation for Cases \\x7f of Fig. \\x7f. Thick solid lines represent \\ndecision boundaries. Dashed lines and arrows have the same meaning as in Fig. 1. Only hyperplanes \\nfor hidden nodes with large weights to the output node are shown. Over 300,000 training trials were \\nrequired to form separate regions. \\na two-layer perceptron but were separated with a three-layer perceptron. This is noted \\nby the use of filled symbols in Fig. 4. \\nFig. 4 shows that there is no significant performance difference between two and \\nthree layer perceptrons when forming complex decision regions using back propagation \\ntraining. Both types of classifiers take an excessively long time (> 100,000 trials) to \\nform complex decision regions. A minor difference is that in Cases 2 and 7 the two-layer \\nnetwork failed to separate disjoint regions after 250,000 trials whereas the three-layer \\nnetwork was able to do so. This, however, is not significant in terms of convergence time \\nand error rate. Problems that are difficult for the two-layer networks are also difficult \\nfor the three-layer networks, and vice versa. \\n4. ALTERNATIVE CLASSIFIERS \\nResults presented above and previous results [5,8] demonstrate that multi-layer per- \\nceptron classifiers can take very long to converge for complex decision regions. Three \\nalternative classifiers were studied to determine whether other types of neural net clas- \\nsifiers could provide faster convergence. \\n4.1. FIXED WEIGHT CLASSIFIERS \\nFixed weight classifiers attempt to reduce training time by adapting only weights \\nbetween upper layers of multi-layer perceptrons. Weights to the first layer are fixed \\nbefore training and remain unchanged. These weights form fixed hyperplanes which \\ncan be used by upper layers to form decision regions. Performance will be good if the \\nfixed hyperplanes are near the decision region boundaries that are required in a specific \\nproblem. Weights between upper layers are trained using back propagation as described \\nabove. Two methods were used to adjust weights to the first layer. Weights were \\nadjusted to place hyperplanes randomly or in a grid in the region (-1 < x\\x7f,x2 < 10). \\nAll decision regions in Fig. 2 fall within this region. Hyperplanes formed by first layer \\nnodes for \"fixed random\" and \"fixed grid\" classifiers for Case 2 of Fig. 2 are shown as \\ndashed lines in Fig. 5. Also shown in this Fig. are decision regions (shaded areas) formed \\n391 \\n% \\n12 \\n10 \\n8 \\n6 \\n4 \\n2 \\nO \\nI I I I i i I i \\n[2] 2- layers ERROR KATE \\nO 3- layers \\nI I I J I I I J \\n200000 I I I I I / I I I \\nCONVERGENCE TIME \\n1OOOOO \\n50000 \\n0 I I I I I I I \\n1 2 3 4 5 6 7 8 9 \\nCase Numbers (see Fig. 2) \\nFro. 4. Perce.taae error (top) a.d co-oer9e.ce time (bottom) for Cases I throu9h 9 of Fi9. \\x7f for \\nttvo-and three.layer perceptton classifiers trained usin 9 back propa9ation, Filled syrabois indicate that \\nseparate disjoint re9ions tvere not formed after \\x7f50,000 trials. \\nusing back propagation to train only the upper network layers. These regions illustrate \\nhow fixed hyperplanes are combined to form decision regions. It can be seen that decision \\nboundaries form along the available hyperplanes. A good solution is possible for the \\nfixed grid classifier where desired decision region boundaries are near hyperplanes. The \\nrandom grid classifier provides a poor solution because hyperplanes are not near desired \\ndecision boundaries. The performance of a fixed weight classifier depends both on the \\nplacement of hyperplanes and on the number of hyperplanes provided. \\n4.2. HYPERCUBE CLASSIFIER \\nMany traditional cidsifters estimate probability density functions of input variables \\nfor different classes using histogram techniques [4]. Hypercube cidsifters use this tech- \\nnique by fixing weights in the first two layers to break the input space into hypercubes \\n(squares in the case of two inputs). Hypercube classifiers are similar to fixed weight \\nclassifiers, except weights to the first two layers are fixed, and only weights to output \\nnodes are trained. Hypercube classifiers are also similar in structure to the CMAC \\nmodel described by Albus [1]. The output of a second layer node is \"high\" only if the \\ninput is in the hypercube corresponding to that node. This is illustrated in Fig. 6 for a \\nnetwork with two inputs. \\nThe top layer of a hypercube classifier can be trained using back propagation. A \\nmaximum likelihood approach, however, suggests a simpler training algorithm which \\nconsists of counting. The output of second layer node Hi is connected to the output \\nnode corresponding to that class with greatest frequency of occurrence of training inputs \\nin hypercube Hi. That is, if a sample falls in hypercube Hi, then it is classified as class \\nO* where \\nNi,o. > Ni,o for all 0 \\x7f 0\\'. (1) \\nIn this equation, Ni,o is the number of training tokens in hypercube Hi which belong to \\nclass O. This will be called maximum likelihood (ML) training. It can be implemented \\nby connection second-layer node Hi only to that output node corresponding to class O* \\nin Eq. (1). In all simulations hypercubes covered the area (-1 < x\\x7f, x2 < 10). \\n392 \\nRANDOM \\nk r_,\\x7f \\x7f,..W\\x7f - 4\\',,;-,- \\n/; /l.\\x7f C\\'( /it %. \\nJ ; / \\x7f \\x7f\\'\\x7f, b t :\\'\\x7f \\n,,,.,., ,,? . \\noP,\\',\"\\'\\x7f \\'\" \\x7f \\n0 I 2 3 \\nGRID \\n0 1 2 3 4 \\nFla. 5. Decision regions formed with \\'\\x7fxed random\" and \\'\\x7fxed grid\" classifiers for Case $ from Fig. \\n$ using back propagation training. Lines shown are h\\x7fIperplanes formed by the first layer nodes. Shaded \\nareas represent the decision region for class A. \\nA \\nI B -1 H2\\'\\x7f 1.6 H \\nTRAINED \\nLAYER \\nFIXED \\nLAYERS \\nFOUR BINS CREATED \\nBY FIXED LAYERS \\nx 2 \\n3 \\n2 \\n1 \\nb3 \\n\\x7f \\x7fjb6 \\nm \\x7fj bE \\n2 3 \\nxl x2 \\nINPUT \\nFIG. 6. A h\\x7flpercube classifier (left) is a three-lager perceptton with fixed weights to the first two layers, \\nand trainable weights to output nodes. Weights are initialized such that outputs of nodes Hi through H4 \\n(left) are \"high\" oni\\x7fl when the input is in the corresponding h\\x7fIpercube (right). \\n393 \\nSELECT \\nCLASS \\nWITH MAJORITY \\nIN TOP k \\nSELECT TOP \\nk EXEMPLARS \\nCALCULATE \\nCORRELATION \\nTO STORED \\nEXEMPLARS \\nOUTPUT (Only One High) \\nYl \\nINPUT \\nSUPERVISED \\nASSOCIATIVE \\nLEARNING \\nUNSUPERVISED \\nKOHONEN \\nFEATURE MAP \\nLEARNING \\nx N \\nFIG. 7. Feature map classifier. \\n4.3. FEATURE MAP CLASSIFIER \\nIn many speech and image classification problems a large quantity of unlabeled \\ntraining data can be obtained, but little labeled data is available. In such situations \\nunsupervised training with unlabeled training data can substantially reduce the amount \\nof supervised training required [3]. The feature map classifier shown in Fig. 7 uses com- \\nbined supervised/unsupervised training, and is designed for such problems. It is similar \\nto histogram classifiers used in discrete observation hidden Markov models [11] and the \\nclassifier used in [6]. The first layer of this classifier forms a feature map using a self \\norganizing clustering algorithm described by Kohonen [6]. In all simulations reported in \\nthis paper 10,000 trials of unsupervised training were used. After unsupervised train- \\ning, first-layer feature nodes sample the input space with node density proportional to \\nthe combined probability density of all classes. First layer feature map nodes perform a \\nfunction similar to that of second layer hypercube nodes except each node has maximum \\noutput for input regions that are more general than hypercubes and only the output of \\nthe node with a maximum output is fed to the output nodes. Weights to output nodes \\nare trained with supervision after the first layer has been trained. Back propag.\\x7ftion, or \\nmaximum likelihood training can be used. Maximum likelihood training reqmres Ni,e \\n(Eq. 1) to be the number of times first layer node i has maximum output for inputs \\nfrom class 0. In addition, during classification, the outputs of nodes with Ni,e = 0 for \\nall 0 (untrained nodes) are not considered when the first-layer node with the maximum \\noutput is selected. The network architecture of a feature map classifier can be used \\nto implement a k-nearest neighbor classifier. In this case, the feedback connections in \\nFig. 7 (large circular summing nodes and triangular integrators) used to select those \\nk nodes with the maximum outputs must be slightly modified. K is I for a feature \\nmap classifier and must be adjusted to the desired value of k for a k-nearest neighbor \\nclassifier. \\n5. COMPARISON BETWEEN CLASSIFIERS \\nThe results of Monte Carlo simulations using all classifiers for Case 2 are shown in \\nFig. 8. Error rates and convergence times were determined as in Section 3. All alter- \\n394 \\n12 \\n0 \\nTrials \\n25OO \\n2O00 \\n1500 \\n1000 \\n5OO \\n0 \\nConventional \\nI \\nGAUSS \\nKNN \\nBack-Prop \\n3- lay \\nPercent Correct \\nFlxed Welght \\nI \\nI \\nHypercube \\n! ! \\nFeature Map \\nI I \\n! I \\nI \\n2-lay \\ni I I \\nKNN GAUSS 32 \\nConvergence Time \\n\\' ndom\\x7f \\n64K \\n3-lay ra \\ngrid \\nI I I I \\n36 40 120 440 1680 \\nNumber of Hidden Nodes \\nI I \\nI I I \\nB/4.51 \\n100 1600 \\nFIG. 8. Comparative performance of classifiers for Case �. Trainin 9 time of the feature map classifiers \\ndoes not include the 10,000 unsupervised trainin9 trials. \\nnative classifiers had shorter convergence times than multi-layer perceptron classifiers \\ntrained with back propagation. The feature map classifier provided best performance. \\nWith 1,600 nodes, its error rate was similar to that of the k-nearest neighbor classifiers \\nbut it required fewer than 100 supervised training tokens. The larger fixed weight and \\nhypercube classifiers performed well but required more supervised training than the \\nfeature map classifiers. These classifiers will work well when the combined probability \\ndensity function of all classes varies smoothly and the domain where this function is \\nnon-zero is known. In this case weights and offsets can be set such that hyperplanes and \\nhypercubes cover the domain and provide good performance. The feature map classifier \\nautomatically covers the domain. Fixed weight \"random\" classifiers performed substan- \\ntially worse than fixed weight \"grid\" classifiers. Back propagation training (BP) was \\ngenerally much slower than maximum likelihood training (ML). \\n6. VOWEL CLASSIFICATION \\nMulti layer perceptron, feature map, and traditional classifiers were tested with \\nvowel formant data from Peterson and Barney [11]. These data had been obtained \\nby spectrographic analysis of vowels in /hVd/ context spoken by 67 men, women and \\nchildren. First and second formant data of ten vowels was split into two sets, resulting \\nin a total of 338 training tokens and 333 testing tokens. Fig. 9 shows the test data \\nand the decision regions formed by a two-layer perceptton classifier trained with back \\npropagation. The performance of classifiers is presented in Table I. All classifiers had \\nsimilar error rates. The feature map classifier with only 100 nodes required less than 50 \\nsupervised training tokens (5 samples per vowel class) for convergence. The perceptton \\nclassifier trained with back propagation required more than 50,000 training tokens. The \\nfirst stage of the feature map classifier and the multi-layer perceptton classifier were \\ntrained by ra\\x7fndomly selecting entries from the 338 training tokens after labels had been \\nremoved and using tokens repetitively. \\n395 \\n4OOO \\n2000 \\nF\\x7f (\\x7f) \\nlOOC \\n5oo \\no head \\n� hid \\n� hod \\n\\x7f had \\no hawed \\n� heard \\no heed \\n\\x7f hud \\n) who\\'d \\n^ hood \\nFIG. 9. Decision regions formed by a two-layer network usin9 BP after �00,000 trainin9 tokens from \\nPetersoh\\'s steady state vowel data [Peterson, 195�]. Also shown are samples of the testin9 set. Le9end \\nshow example of the pronunciation of the 10 vowels and the error within each vowel. \\nALGORITHM TRAINING TOKENS-\\'[ % ERROR \\nKNN :338 18.0 \\nGaussian \" 338 \" 20.4 \\n2-L.ayer Pe..r\\x7f\\'eptron ,,, 50,000 19.8 \\nFeature. Map < 50 ..... 22.8 \\nTABLE I \\nPerformance of classifiers on steady state vowel data. \\n396 \\n7. CONCLUSIONS \\nNeural net architectures form a flexible framework that can be used to construct \\nmany different types of classifiers. These include Gaussian, k-nearest neighbor, and \\nmulti-layer perceptton classifiers as well as classifiers such as the feature map classifier \\nwhich use unsupervised training. Here we first demonstrated that two-layer percepttons \\n(one hidden layer) can form non-convex and disjoint decision regions. Back propagation \\ntraining, however, can be extremely slow when forming complex decision regions with \\nmulti-layer perceptrons. Alternative classifiers were thus developed and tested. All \\nprovided faster training and many provided improved performance. Two were similar to \\ntraditional classifiers. One (hypercube classifier) can be used to implement a histogram \\nclassifier, a\\x7fnd another (feature map classifier) can be used to implement a k-nearest \\nneighbor classifier. The feature map classifier provided best overall performance. It \\nused combined supervised/unsupervised training and attained the same error rate as a \\nk-nearest neighbor classifier, but with fewer supervised training tokens. Furthermore, \\nit required fewer nodes then a k-nearest neighbor classifier. \\nREFERENCES \\n[1] J. \\n[2] D. \\n[3] v. \\nS. Albus, Brains, Behavior, and Robotics. McGraw-Hill, Petersborough, N.H., 1981. \\nJ. Burr, \"A neural network digit recognizer,\" in Proceedings of the International Conference \\non Systems, Man, and Cybernetics, IEEE, 1986. \\nB. Cooper and J. H. Freeman, \"On the asymptotic improvement in the outcome of supervised \\nlearning provided by aAditional nonsupervised learning,\" IEEE Transactions on Computers, \\nvol. C-19, pp. 1055-63, November 1970. \\n[4] R. O. Duda and P. E. Hart, Pattern Classification and Scene Analysis. John-Wiley &\\x7f Sons, New \\nYork, 1973. \\n[5] W.Y. Huang and R. P. Lippmann, \"Comparisons between conventional and neural net classifiers,\" \\nin 1st International Conference on Neural Network, IEEE, June 1987. \\n[6] T. \\nKohonen, K. Makisara, and T. Saramaki, \"Phonotopic maps -- insightful representation of \\nphonological features for speech recognition,\" in Proceedings of the 7th International Confer- \\nence on Pattern Recognition, IEEE, August 1984. \\n[7] R. P. Lippmann, \"An introduction to computing with neural nets,\" IEEE ASSP Magazine, vol. 4, \\npp. 4-22, April 1987. \\n[8] R. P. Lippmann and B. Gold, \"Neural classifiers useful for speech recognition,\" in 1st International \\nConference on Neural Network, IEEE, June 1987. \\n[g] I.D. Longstaff and J. F. Cross, \"A pattern recognition approach to understanding the multi-layer \\nperceptton,\" Mem. 3936, Royal Signals and Radar Establishment, July 1986. \\n[10] \\n[11] \\n[12] r. \\n[131 R. \\n[14] A. \\nN.J. Nilsson, Learning Machines. McGraw Hill, N.Y., 1965. \\nT. Paxsons, Voice and Speech Processing. McGraw-Hill, New York, 1986. \\nRosenblurt, Percepttons and the Theory of Brain Mechanisms. Spartan Books, 1962. \\nC. Singleton, \"A test for linear separability as applied to self-organizing machines,\" in Self- \\nOrganization Systems, 196�, (M. C. Yovits, G. T. Jacobi, and G. D. Goldstein, eds.), pp. 503- \\n524, Spartan Books, Washington, 1962. \\nWieland and R. Leighton, \"Geometric analysis of neural network capabilities,\" in Ist Interna- \\ntional Conference on Neural Networks, IEEE, June 1987. \\n',\n",
       " '1 \\nCONNECTIVITY VERSUS ENTROPY \\nYaser S. Abu-Mostafa \\nCalifornia Institute of Technology \\nPasadena, CA 91125 \\nABSTRACT \\nHow does the connectivity of a neural network (number of synapses per \\nneuron) relate to the complexity of the problems it can handle (measured by \\nthe entropy)? Switching theory would suggest no relation at all, since all Boolean \\nfunctions can be implemented using a circuit with very low connectivity (e.g., \\nusing two-input NAND gates). However, for a network that learns a problem \\nfrom examples using a local learning rule, we prove that the entropy of the \\nproblem becomes a lower bound for the connectivity of the network. \\nINTRODUCTION \\nThe most distinguishing feature of neural networks is their ability to spon- \\ntaneously learn the desired function from \\'training\\' samples, i.e., their ability \\nto program themselves. Clearly, a given neural network cannot just learn any \\nfunction, there must be some restrictions on which networks can learn which \\nfunctions. One obvious restriction, which is independent of the learning aspect, \\nis that the network must be big enough to accommodate the circuit complex- \\nity of the function it will eventually simulate. Are there restrictions that arise \\nmerely from the fact that the network is expected to learn the function, rather \\nthan being purposely designed for the function? This paper reports a restriction \\nof this kind. \\nThe result imposes a lower bound on the connectivity of the network (num- \\nber of synapses per neuron). This lower bound can only be a consequence of \\nthe learning aspect, since switching theory provides purposely designed circuits \\nof low connectivity (e.g., using only two-input NAND gates) capable of imple- \\nmenting any Boolean function [1,2]. It also follows that the learning mechanism \\nmust be restricted for this lower bound to hold; a powerful mechanism can be \\n@ American Institute of Physics 1988 \\n2 \\ndesigned that will find one of the low-connectivity circuits (perhaps by exhaus- \\ntive search), and hence the lower bound on connectivity cannot hold in general. \\nIndeed, we restrict the learning mechanism to be local; when a training sample \\nis loaded into the network, each neuron has access only to those bits carried by \\nitself and the neurons it is directly connected to. This is a strong assumption \\nthat excludes sophisticated learning mechanisms used in neural-network models, \\nbut may be more plausible from a biological point of view. \\nThe lower bound on the connectivity of the network is given in terms of \\nthe entropy of the environment that provides the training samples. Entropy is a \\nquantitative measure of the disorder or randomness in an environment or, equiv- \\nalently, the amount of information needed to specify the environment. There \\nare many different ways to define entropy, and many technical variations of this \\nconcept [3]. In the next section, we shall introduce the formal definitions and \\nresults, but we start here with an informal exposition of the ideas involved. \\nThe environment in our model produces patterns represented by N bits \\nx = x\\x7f... xN (pixels in the picture of a visual scene if you will). Only h different \\npatterns can be generated by a given environment, where h < 2 \\x7fv (the entropy \\nis essentially log 2 h). No knowledge is assumed about which patterns the en- \\nvironment is likely to generate, only that there are h of them. In the learning \\nprocess, a huge number of sample patterns are generated at random from the \\nenvironment and input to the network, one bit per neuron. The network uses \\nthis information to set its internal parameters and gradually tune itself to this \\nparticular environment. Because of the network architecture, each neuron knows \\nonly its own bit and (at best) the bits of the neurons it is directly connected to \\nby a synapse. Hence, the learning rules are local: a neuron does not have the \\nbenefit of the entire global pattern that is being learned. \\nAfter the learning process has taken place, each neuron is ready to perform \\na function derned by \\x7fvhat it has l\\x7farned. The collective interaction of the \\nfunctions of the neurons is what defines the overall function of the network. The \\nmain result of this paper is that (roughly speaking) if the connectivity of the \\nnetwork is less than the entropy of the environment, the network cannot learn \\nabout the environment. The idea of the proof is to show that if the connectivity \\nis small, the final function of each neuron is independent of the environment, \\nand hence to conclude that the overall network has accumulated no information \\nabout the environment it is supposed to learn about. \\nFORMAL RESULT \\nA neural network is an undirected graph (the vertices are the neurons and the \\nedges are the synapses). Label the neurons 1,-.-, N and define K, _c \\x7f1,.. -, N\\x7f \\nto be the set of neurons connected by a synapse to neuron n, together with \\nneuron n itself. An environment is a subset e __C C0, 1\\x7f \\x7fv (each x \\x7f e is a sample \\n3 \\nfrom the environment). During learning, Zl,-\", z\\x7fv (the bits of x) are loaded \\ninto the neurons 1,...,N, respectively. Consider an arbitrary neuron n and \\ntelabel everything to make K\\x7f become {1,...,K}. Thus the neuron sees the \\nfirst K coordinates of each x. \\nSince our result is asymptotic in N, we will specify K as a function of N; \\nK = aN where a = a(N) satifies limN-.,o a(N) = ao (0 < ao < 1). Since the \\nresult is also statistical, we will consider the ensemble of environments � \\ne = {, c I I,I = \\nwhere h = 2 oN and/\\x7f =/\\x7f(N) satifies limN_.oo/\\x7f(N) = /\\x7fo (0 < /\\x7fo < 1). The \\nprobability distribution on \\x7f is uniform; any environmen\\x7f e G \\x7f \\x7f \\x7f likely \\noccur \\x7f any o\\x7fher. \\nThe neuron sees only \\x7fhe firs\\x7f K coordinates of each x generated by \\x7fhe \\nenvironment e. For each e, we define the function \\x7f: {0,1} \\x7f \\x7f {0,1,2,--.} \\nwhere \\nn(a\\x7f...a\\x7f) =l{x6e [ z,=a, fork=l,--.,K}l \\nand the normalized version \\nThe function v describes the relative frequency of occurrence for each of the 2 \\x7fr \\nbinary vectors a:l -\" z\\x7fr as x = zl \\'\" z Jr runs through all h vectors in e. In other \\nwords, \\x7f/specifies the projection of e as seen by the neuron. Clearly, v(a) _> 0 \\nfor all a \\x7f {0, 1} \\x7fr and Z&E{O,1}K v(a) = 1. \\nCorresponding to two environments el and es, we will have two functions vl \\nand bt 2. If//1 is not distinguishable from t/z, the neuron cannot tell the difference \\nbetween ea and es. The distinguishability between btl and t/: can be measured \\nby \\n1 \\nd(l/l\\'l/2) -- \\x7f Z \\nThe range of d(t/1,\\x7f) is 0 <_ d(t/1,\\x7f) <_ 1, where \\'0\\' corresponds to complete \\nindistinguishability while \\'1\\' corresponds to maximum distinguishability. We \\naxe now in a position to state the main result. \\nLet e\\x7f and es be independently selected environments from � according to the \\nuniform probability distribution. d(vl, v\\x7f) is now a random variable, and we are \\ninterested in the expected value E(d(vl,v2)). The case where E(d(vl,v2)) -- 0 \\ncorresponds to the neuron getting no information about the environment, while \\nthe case where E(d(Vl,V2)) = I corresponds to the neuron getting maximum \\ninformation. The theorem predicts, in the limit, one of these extremes depending \\non how the connectivity (ao) compares to the entropy (/\\x7fo). \\n4 \\nTheorem. \\n1. If ao > f/o, then lim\\x7fv-.\\x7fo E (d(Vl, v2)) = 1. \\n2. If c\\x7fo < \\x7fo, then limN._.ooS(d(\\x7f,,,v2)) =0. \\nThe proof is given in the appendix, but the idea is easy to illustrate infor- \\nmally. Suppose h = 2 K+\\x7f� (corresponding to part 2 of the theorem). For most \\nenvironments e 6 �, the first K bits of x 6 e go through all 2 K possible val- \\nues approximately 2 \\x7f� times each as x goes through all h possible values once. \\nTherefore, the patterns seen by the neuron are drawn from the fixed ensemble of \\nall binary vectors of length K with essentially uniform probability distribution, \\ni.e., v is the same for most environments. This means that, statistically, the \\nneuron will end up doing the same function regardless of the environment at \\nhand. \\nWhat about the opposite case, where h = 2 K-\\x7f� (corresponding to part 1 of \\nthe theorem)? Now, with only 2 \\x7f-\\x7f� patterns available from the environment, \\nthe first K bits of x can assume at most 2 K-\\x7f� values out of the possible 2 g \\nvalues a binary vector of length K can assume in principle. Furthermore, which \\nvalues can be assumed depends on the particular environment at hand, i.e., \\n, does depend on the environment. Therefore, although the neuron still does \\nnot have the global picture, the information it has says something about the \\nenvironment. \\nACKNOWLEDGEMENT \\nThis work was supported by the Air Force Office of Scientific Research under \\nGrant AFOSR-86-0296. \\nAPPENDIX \\nIn this appendix we prove the main theorem. We start by discussing some \\nbasic properties about the ensemble of environments �. Since the probability \\ndistribution on � is uniform and since [�1-- (2h\\x7f), we have \\nwhich is equivalent to generating e by choosing h elements x 6 {0, 1} \\x7fv with \\nuniform probability (without replacement). It follows that \\nh \\nPr(x6e)= 2\\x7f v \\n5 \\nwhile for x 1 \\x7f x2, \\nh h-1 \\nPr(xl\\x7fe, xi\\x7fe) = 2\\x7f v x 2\\x7f v_l \\nand so on. \\nThe functions \\x7f and \\x7f are defined on K-bit vectors. \\n(a random variable for fixed a) is independent of a \\nThe statistics of r\\x7f(a) \\nPr(r\\x7f(sx) = m) = Pr(r\\x7f(s\\x7f) = m) \\nwhich follows from the symmetry with respect to each bit of a. The same holds \\nfor the statistics of \\x7f(a). The expected value E(r\\x7f(a)) = h2 -K (h objects going \\ninto 2 K cells), hence E(\\x7f(a)) = 2 -\\x7f. We now restate and prove the theorem. \\nTheorem. \\n1. If c\\x7fo > \\x7fo, then lim\\x7fr-.\\x7fo E (d(\\x7fl, \\x7f2)) = 1. \\n2. If ao < \\x7fo, then limN-\\x7fo E (d(\\x7fl, \\x7f2)) =0. \\nProof. \\nWe expand E (d(\\x7fl, \\x7f2)) as follows \\nwhere n\\x7f and n2 denote nl(0.--0) and n2(0..-0), respectively, and the last step \\nfollows from the fact that the statistics of nl(a) and n2(a) is independent of a. \\nTherefore, to prove the theorem, we evaluate E(Irh- r\\x7f21) for large N. \\n1. Assume ao > f\\x7fo. Let n denote n(0...0), and consider Pr(n - 0). For r\\x7f to \\nbe zero, all 2 N-K strings x of N bits starting with K O\\'s must not be in the \\nenvironment e. Hence \\nPr(r\\x7f = O) = (1- -- \\nh \\n2)(1 \\nh h \\n2 \\x7fr - 1 )\\'\" (1 - 2\\x7f r _ 2\\x7fr_\\x7f: + 1 ) \\nwhere the first term is the probability that 0... 00 \\x7f e, the second term is the \\n6 \\nprobability that 0.--O1 \\x7f e given that 0-.. O0 \\x7f e, and so on. \\n: (1 - h2-m\\'(1 -- 2-x) -\\') \\n>_ (1 - \\n> 1 - 2h2-\\x7fr2 \\x7fv-K \\n= 1 - 2h2 -K \\n2N--K \\nHence, Pr(n, = 0): Pr(n2: 0): Pr(n: 0) _> i - 2h2 -K. However, E(n,) = \\nE(n2) = h2 -\\x7f. Therefore, \\nh h \\nZ Z Pr(rtl: i, rt2 \\'-- j)li -- Jl \\ni=0 j=O \\nh h \\n= Z Y] Pr(nl = i)Pr(n2: j)l i - Jl \\ni=0 j=0 \\nh \\n_\\x7f \\x7f Pt(hi = 0)Pr(n2 = j)j \\nh \\n+ Z Pt(hi =/)Pr(n2 = 0)i \\ni=0 \\nwhich follows by throwing away all the terms where neither i nor j is zero (the \\nterm where both i an j are zero appears twice for convenience, but this term is \\nzero anyway). \\n= Pr(nl = 0)E(n2) + Pr(n2 = 0)E(rh) \\n> 2(1- 2h2-\\x7f)h2 -\\x7f \\nSubstituting this estimate in the expression for E(d(t\\x7fl, t\\x7f2)), we get \\n2 K \\nE(d(//1,//2)) = 2hE(lnl -- \\n\\x7f_ \\x7f-\\x7f x 2(1- 2h2-K)h2 -\\x7ft \\n= 1 - 2h2 -K \\n= 1 - 2 x 2 (f\\x7f-\")\\x7fv \\nSince ao >/\\x7fo by assumption, this lower bound goes to 1 as N goes to infinity. \\nSince 1 is also an upper bound for d(\\x7fl, \\x7f2) (and hence an upper bound for the \\nexpected value E(d(/�l,/�2))), lim\\x7fv-.oo E(d(/�\\x7f,/\\x7f2)) must be 1. \\n7 \\n2. Assume ao < \\x7fo. Consider \\nTo evaluate E([n - h2-K[), we estimate the variance of n and use the fact \\nthat E([n- h2-KI) <_ va,\\x7f\\x7f (recall that h2 -\\x7f = E(n)). Since vat(n) = \\nE(n 2) - (E(n)) 2, we need an estimate for E(n2). We write n = Eaei0.1)N-\\x7f \\x7fa, \\nwhere \\n1, if 0.-.0a 6 e; \\n5\\x7f = 0, otherwise. \\nIn this notation, E(n 2) can be written as \\n: >- \\n&�{0,1} N-I� be{0,1} \\nFor the \\'diagonal\\' terms (a = b), \\n= h2 -/v \\nThere are 2 N-K such diagonal terms, hence a total contribution of 2 N-K X \\nh2 -\\x7fr = h2 -K to the sum. For the \\'off-diagonal\\' terms (a \\x7f b), \\nE(5. Sb) = Pr(5. = 1, Sb = 1) \\n= Pr(5. = 1)Pr(Sb: l[\\x7fa = 1) \\nh h-1 \\n-- -- X \\n2 r\\x7f 2 \\x7fr - I \\nThere are 2\\x7fr-\\x7f(2 \\x7fv-K - 1) such off-diagonal terms, hence a total contribution of \\n2\\x7fV-\\x7f(2N-K \\x7f� h(h-i) < {h,\\x7f_K\\x7f2 2 \\x7fv \\nk --\\'\\x7f1 ^ 2N(2N--1) -- \\x7f, \\x7f ] \\x7f--i to the sum. Putting the contributions \\n8 \\nfrom the diagonal and off-diagonal terms together, we get \\nwr(n) = E(n 2) - (E(n)) 2 \\n< h2-K + (h2-K)a2\\x7fv - 1 \\n1 \\n= h2-K + (h2-\\x7f)a 2 N - 1 \\nh2_\\x7f \\x7f \\n= h2 -K 1 + \\n< 2h2 -\\x7f \\nThe last step follows since h2 -K is much smaller than 2 \\x7fr - 1. Therefore, E(In - \\n1 \\nh2-\\x7f]) < v\\x7f < (2h2-\\x7f) \\x7f Substituting this estimate in the expression for \\nwe get \\nSince Cto < f/o by assumption, this upper bound goes to 0 as N goes to infinity. \\nSince 0 is also a lower bound for d(yx,ya) (and hence a lower bound for the \\nexpected value E(d(\\x7f,x,\\x7f,a))), lim\\x7fo E(d(\\x7f,\\x7f,\\x7f,a)) must be 0. I \\nREFERENCES \\n[1] Y. Abu-Mostafa, \"Neural networks for computing?,\"AIP Conference Pro- \\nceedings \\x7f 151, Neural Networks for Computing, J. Denker (ed.), pp. 1-6, 1986. \\n[2] Z. Kohavi, Switching and Finite Automata Theory, McGraw-Hill, 1978. \\n[3] Y. Abu-Mostafa, \"The complexity of information extraction,\"IEEE Trans. \\non Information Theory, vol. IT-32, pp. 513-525, July 1986. \\n[4] Y. Abu-Mostafa, \"Complexity in neural systems,\"in Analog VLSiand Neural \\nSystems by C. Mead, Addison-Wesley, 1988. \\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9597e5e-a92a-4ec9-aadf-78663c9e2c28",
   "metadata": {},
   "source": [
    "## Pre-process and vectorize the documents\n",
    "\n",
    "As part of preprocessing, we will:\n",
    "\n",
    "Tokenize (split the documents into tokens).\n",
    "\n",
    "Lemmatize the tokens.\n",
    "\n",
    "Compute bigrams.\n",
    "\n",
    "Compute a bag-of-words representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a81dc1-e29f-439b-a02b-3cca58324be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bddd8bc0-5b68-4547-ac36-2f67c397bf7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hladn\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the documents.\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "193767b2-f2aa-4085-a228-daad7d87c00b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neural',\n",
       " 'net',\n",
       " 'and',\n",
       " 'traditional',\n",
       " 'classifier',\n",
       " 'william',\n",
       " 'huang',\n",
       " 'and',\n",
       " 'richard',\n",
       " 'lippmann',\n",
       " 'mit',\n",
       " 'lincoln',\n",
       " 'laboratory',\n",
       " 'lexington',\n",
       " 'ma',\n",
       " 'usa',\n",
       " 'abstract',\n",
       " 'previous',\n",
       " 'work',\n",
       " 'on',\n",
       " 'net',\n",
       " 'with',\n",
       " 'continuous',\n",
       " 'valued',\n",
       " 'input',\n",
       " 'led',\n",
       " 'to',\n",
       " 'generative',\n",
       " 'procedure',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'convex',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'with',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'percepttons',\n",
       " 'one',\n",
       " 'hidden',\n",
       " 'layer',\n",
       " 'and',\n",
       " 'arbitrary',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'with',\n",
       " 'three',\n",
       " 'layer',\n",
       " 'percepttons',\n",
       " 'two',\n",
       " 'hidden',\n",
       " 'layer',\n",
       " 'here',\n",
       " 'we',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'perceptton',\n",
       " 'classifier',\n",
       " 'trained',\n",
       " 'with',\n",
       " 'back',\n",
       " 'propagation',\n",
       " 'can',\n",
       " 'form',\n",
       " 'both',\n",
       " 'convex',\n",
       " 'and',\n",
       " 'disjoint',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'such',\n",
       " 'classifier',\n",
       " 'are',\n",
       " 'robust',\n",
       " 'train',\n",
       " 'rapidly',\n",
       " 'and',\n",
       " 'provide',\n",
       " 'good',\n",
       " 'performance',\n",
       " 'with',\n",
       " 'simple',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'when',\n",
       " 'complex',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'are',\n",
       " 'required',\n",
       " 'however',\n",
       " 'convergence',\n",
       " 'time',\n",
       " 'can',\n",
       " 'be',\n",
       " 'excessively',\n",
       " 'long',\n",
       " 'and',\n",
       " 'performance',\n",
       " 'is',\n",
       " 'often',\n",
       " 'no',\n",
       " 'better',\n",
       " 'than',\n",
       " 'that',\n",
       " 'of',\n",
       " 'nearest',\n",
       " 'neighbor',\n",
       " 'classifier',\n",
       " 'three',\n",
       " 'neural',\n",
       " 'net',\n",
       " 'classifier',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'that',\n",
       " 'provide',\n",
       " 'more',\n",
       " 'rapid',\n",
       " 'training',\n",
       " 'under',\n",
       " 'such',\n",
       " 'situation',\n",
       " 'two',\n",
       " 'use',\n",
       " 'fixed',\n",
       " 'weight',\n",
       " 'in',\n",
       " 'the',\n",
       " 'first',\n",
       " 'one',\n",
       " 'or',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'and',\n",
       " 'are',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'classifier',\n",
       " 'that',\n",
       " 'estimate',\n",
       " 'probability',\n",
       " 'density',\n",
       " 'function',\n",
       " 'using',\n",
       " 'histogram',\n",
       " 'third',\n",
       " 'feature',\n",
       " 'map',\n",
       " 'classifier',\n",
       " 'us',\n",
       " 'both',\n",
       " 'unsupervised',\n",
       " 'and',\n",
       " 'supervised',\n",
       " 'training',\n",
       " 'it',\n",
       " 'provides',\n",
       " 'good',\n",
       " 'performance',\n",
       " 'with',\n",
       " 'little',\n",
       " 'supervised',\n",
       " 'training',\n",
       " 'in',\n",
       " 'situation',\n",
       " 'such',\n",
       " 'a',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'where',\n",
       " 'much',\n",
       " 'unlabeled',\n",
       " 'training',\n",
       " 'data',\n",
       " 'is',\n",
       " 'available',\n",
       " 'the',\n",
       " 'architecture',\n",
       " 'of',\n",
       " 'this',\n",
       " 'classifier',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'implement',\n",
       " 'neural',\n",
       " 'net',\n",
       " 'nearest',\n",
       " 'neighbor',\n",
       " 'classifier',\n",
       " 'introduction',\n",
       " 'neural',\n",
       " 'net',\n",
       " 'architecture',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'many',\n",
       " 'different',\n",
       " 'type',\n",
       " 'of',\n",
       " 'classi',\n",
       " 'tier',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'multi',\n",
       " 'layer',\n",
       " 'perceptron',\n",
       " 'classifier',\n",
       " 'with',\n",
       " 'continuous',\n",
       " 'valued',\n",
       " 'in',\n",
       " 'put',\n",
       " 'trained',\n",
       " 'with',\n",
       " 'back',\n",
       " 'propagation',\n",
       " 'are',\n",
       " 'robust',\n",
       " 'often',\n",
       " 'train',\n",
       " 'rapidly',\n",
       " 'and',\n",
       " 'provide',\n",
       " 'perfor',\n",
       " 'mance',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'that',\n",
       " 'provided',\n",
       " 'by',\n",
       " 'gaussian',\n",
       " 'classifier',\n",
       " 'when',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'are',\n",
       " 'convex',\n",
       " 'generative',\n",
       " 'procedure',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'such',\n",
       " 'classifier',\n",
       " 'can',\n",
       " 'form',\n",
       " 'convex',\n",
       " 'deci',\n",
       " 'sion',\n",
       " 'region',\n",
       " 'with',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'perceptrons',\n",
       " 'one',\n",
       " 'hidden',\n",
       " 'layer',\n",
       " 'and',\n",
       " 'arbitrary',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'with',\n",
       " 'three',\n",
       " 'layer',\n",
       " 'perceptrons',\n",
       " 'two',\n",
       " 'hidden',\n",
       " 'layer',\n",
       " 'more',\n",
       " 'recent',\n",
       " 'work',\n",
       " 'ha',\n",
       " 'demon',\n",
       " 'strated',\n",
       " 'that',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'perceptrons',\n",
       " 'can',\n",
       " 'form',\n",
       " 'non',\n",
       " 'convex',\n",
       " 'and',\n",
       " 'disjoint',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'example',\n",
       " 'of',\n",
       " 'hand',\n",
       " 'crafted',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'network',\n",
       " 'which',\n",
       " 'generate',\n",
       " 'such',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'in',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'along',\n",
       " 'with',\n",
       " 'monte',\n",
       " 'carlo',\n",
       " 'simulation',\n",
       " 'where',\n",
       " 'complex',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'were',\n",
       " 'generated',\n",
       " 'using',\n",
       " 'back',\n",
       " 'propagation',\n",
       " 'training',\n",
       " 'these',\n",
       " 'and',\n",
       " 'previous',\n",
       " 'simula',\n",
       " 'tions',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'convergence',\n",
       " 'time',\n",
       " 'with',\n",
       " 'back',\n",
       " 'propagation',\n",
       " 'can',\n",
       " 'be',\n",
       " 'excessive',\n",
       " 'when',\n",
       " 'complex',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'are',\n",
       " 'desired',\n",
       " 'and',\n",
       " 'performance',\n",
       " 'is',\n",
       " 'often',\n",
       " 'no',\n",
       " 'better',\n",
       " 'than',\n",
       " 'that',\n",
       " 'obtained',\n",
       " 'with',\n",
       " 'nearest',\n",
       " 'neighbor',\n",
       " 'classifier',\n",
       " 'these',\n",
       " 'result',\n",
       " 'led',\n",
       " 'u',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'other',\n",
       " 'neural',\n",
       " 'net',\n",
       " 'classifier',\n",
       " 'that',\n",
       " 'might',\n",
       " 'provide',\n",
       " 'faster',\n",
       " 'convergence',\n",
       " 'three',\n",
       " 'classifier',\n",
       " 'called',\n",
       " 'fixed',\n",
       " 'weight',\n",
       " 'hypercube',\n",
       " 'and',\n",
       " 'feature',\n",
       " 'map',\n",
       " 'classifier',\n",
       " 'were',\n",
       " 'developed',\n",
       " 'and',\n",
       " 'eval',\n",
       " 'uated',\n",
       " 'all',\n",
       " 'classifier',\n",
       " 'were',\n",
       " 'tested',\n",
       " 'on',\n",
       " 'illustrative',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'two',\n",
       " 'continuous',\n",
       " 'valued',\n",
       " 'input',\n",
       " 'and',\n",
       " 'two',\n",
       " 'class',\n",
       " 'and',\n",
       " 'more',\n",
       " 'restricted',\n",
       " 'set',\n",
       " 'of',\n",
       " 'classifier',\n",
       " 'wa',\n",
       " 'tested',\n",
       " 'with',\n",
       " 'vowel',\n",
       " 'formant',\n",
       " 'data',\n",
       " 'capability',\n",
       " 'of',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'perceptrons',\n",
       " 'multi',\n",
       " 'layer',\n",
       " 'perceptron',\n",
       " 'classifier',\n",
       " 'with',\n",
       " 'hard',\n",
       " 'limiting',\n",
       " 'nonlinearities',\n",
       " 'node',\n",
       " 'output',\n",
       " 'of',\n",
       " 'or',\n",
       " 'and',\n",
       " 'continuous',\n",
       " 'valued',\n",
       " 'input',\n",
       " 'can',\n",
       " 'form',\n",
       " 'complex',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'simple',\n",
       " 'constructive',\n",
       " 'proof',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'three',\n",
       " 'layer',\n",
       " 'perceptron',\n",
       " 'two',\n",
       " 'hidden',\n",
       " 'layer',\n",
       " 'can',\n",
       " 'this',\n",
       " 'work',\n",
       " 'wa',\n",
       " 'sponsored',\n",
       " 'by',\n",
       " 'the',\n",
       " 'defense',\n",
       " 'advanced',\n",
       " 'research',\n",
       " 'project',\n",
       " 'agency',\n",
       " 'and',\n",
       " 'the',\n",
       " 'department',\n",
       " 'of',\n",
       " 'the',\n",
       " 'air',\n",
       " 'force',\n",
       " 'the',\n",
       " 'view',\n",
       " 'expressed',\n",
       " 'are',\n",
       " 'those',\n",
       " 'of',\n",
       " 'the',\n",
       " 'author',\n",
       " 'and',\n",
       " 'do',\n",
       " 'not',\n",
       " 'reflect',\n",
       " 'the',\n",
       " 'policy',\n",
       " 'or',\n",
       " 'position',\n",
       " 'of',\n",
       " 'the',\n",
       " 'government',\n",
       " 'american',\n",
       " 'institute',\n",
       " 'of',\n",
       " 'physic',\n",
       " 'xl',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'for',\n",
       " 'class',\n",
       " 'bl',\n",
       " 'b2',\n",
       " 'b6',\n",
       " 'xl',\n",
       " 'fig',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'perceptton',\n",
       " 'that',\n",
       " 'form',\n",
       " 'disjoint',\n",
       " 'decision',\n",
       " 're9ions',\n",
       " 'for',\n",
       " 'class',\n",
       " 'shaded',\n",
       " 'area',\n",
       " 'connec',\n",
       " 'tion',\n",
       " 'weight',\n",
       " 'and',\n",
       " 'node',\n",
       " 'offset',\n",
       " 'are',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'the',\n",
       " 'left',\n",
       " 'hyperplanes',\n",
       " 'formed',\n",
       " 'by',\n",
       " 'all',\n",
       " 'hidden',\n",
       " 'node',\n",
       " 'are',\n",
       " 'drawn',\n",
       " 'a',\n",
       " 'dashed',\n",
       " 'line',\n",
       " 'with',\n",
       " 'node',\n",
       " 'label',\n",
       " 'arrow',\n",
       " 'on',\n",
       " 'these',\n",
       " 'line',\n",
       " 'point',\n",
       " 'to',\n",
       " 'the',\n",
       " 'half',\n",
       " 'plane',\n",
       " 'where',\n",
       " 'the',\n",
       " 'hidden',\n",
       " 'node',\n",
       " 'output',\n",
       " 'is',\n",
       " 'high',\n",
       " 'form',\n",
       " 'arbitrary',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'and',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'perceptron',\n",
       " 'one',\n",
       " 'hidden',\n",
       " 'layer',\n",
       " 'can',\n",
       " 'form',\n",
       " 'single',\n",
       " 'convex',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'recently',\n",
       " 'however',\n",
       " 'it',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'demonstrated',\n",
       " 'that',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'perceptrons',\n",
       " 'can',\n",
       " 'form',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'that',\n",
       " 'are',\n",
       " 'not',\n",
       " 'simply',\n",
       " 'convex',\n",
       " 'fig',\n",
       " 'for',\n",
       " 'example',\n",
       " 'show',\n",
       " 'how',\n",
       " 'disjoint',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'can',\n",
       " 'be',\n",
       " 'generated',\n",
       " 'using',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'perceptron',\n",
       " 'the',\n",
       " 'two',\n",
       " 'disjoint',\n",
       " 'shaded',\n",
       " 'area',\n",
       " 'in',\n",
       " 'this',\n",
       " 'fig',\n",
       " 'represent',\n",
       " 'the',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'for',\n",
       " 'class',\n",
       " 'output',\n",
       " 'node',\n",
       " 'ha',\n",
       " 'high',\n",
       " 'output',\n",
       " 'the',\n",
       " 'remaining',\n",
       " 'area',\n",
       " 'represents',\n",
       " 'the',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'for',\n",
       " 'class',\n",
       " 'output',\n",
       " 'node',\n",
       " 'ha',\n",
       " 'low',\n",
       " 'output',\n",
       " 'node',\n",
       " 'in',\n",
       " 'this',\n",
       " 'fig',\n",
       " 'contain',\n",
       " 'hard',\n",
       " 'limiting',\n",
       " 'nonlinearities',\n",
       " 'connection',\n",
       " 'weight',\n",
       " 'and',\n",
       " 'node',\n",
       " 'offset',\n",
       " 'are',\n",
       " 'indicated',\n",
       " 'in',\n",
       " 'the',\n",
       " 'left',\n",
       " 'diagram',\n",
       " 'ten',\n",
       " 'other',\n",
       " 'complex',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'formed',\n",
       " 'using',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'perceptrons',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'in',\n",
       " 'fig',\n",
       " 'the',\n",
       " 'above',\n",
       " 'example',\n",
       " 'suggest',\n",
       " 'that',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'perceptrons',\n",
       " 'can',\n",
       " 'form',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'with',\n",
       " 'arbitrary',\n",
       " 'shape',\n",
       " 'we',\n",
       " 'however',\n",
       " 'know',\n",
       " 'of',\n",
       " 'no',\n",
       " 'general',\n",
       " 'proof',\n",
       " 'of',\n",
       " 'this',\n",
       " 'capability',\n",
       " 'book',\n",
       " 'by',\n",
       " 'nilson',\n",
       " 'discus',\n",
       " 'this',\n",
       " 'issue',\n",
       " 'and',\n",
       " 'contains',\n",
       " 'proof',\n",
       " 'that',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'net',\n",
       " 'can',\n",
       " 'divide',\n",
       " 'finite',\n",
       " 'number',\n",
       " 'of',\n",
       " 'point',\n",
       " 'into',\n",
       " 'two',\n",
       " 'arbitrary',\n",
       " 'set',\n",
       " 'page',\n",
       " 'this',\n",
       " 'proof',\n",
       " 'involves',\n",
       " 'separating',\n",
       " 'point',\n",
       " 'using',\n",
       " 'at',\n",
       " 'most',\n",
       " 'parallel',\n",
       " 'hyperplanes',\n",
       " 'formed',\n",
       " 'by',\n",
       " 'first',\n",
       " 'layer',\n",
       " 'node',\n",
       " 'where',\n",
       " 'no',\n",
       " 'hyperplane',\n",
       " 'intersects',\n",
       " 'two',\n",
       " 'or',\n",
       " 'more',\n",
       " 'point',\n",
       " 'proving',\n",
       " 'that',\n",
       " 'given',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'can',\n",
       " 'be',\n",
       " 'formed',\n",
       " 'in',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'net',\n",
       " 'involves',\n",
       " 'testing',\n",
       " 'to',\n",
       " 'determine',\n",
       " 'whether',\n",
       " 'the',\n",
       " 'boolean',\n",
       " 'representation',\n",
       " 'at',\n",
       " 'the',\n",
       " 'output',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'layer',\n",
       " 'for',\n",
       " 'all',\n",
       " 'point',\n",
       " 'within',\n",
       " 'the',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'for',\n",
       " 'class',\n",
       " 'are',\n",
       " 'linearly',\n",
       " 'separable',\n",
       " 'from',\n",
       " 'the',\n",
       " 'boolean',\n",
       " 'representation',\n",
       " 'for',\n",
       " 'class',\n",
       " 'one',\n",
       " 'test',\n",
       " 'for',\n",
       " 'linear',\n",
       " 'separability',\n",
       " 'wa',\n",
       " 'presented',\n",
       " 'in',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'forming',\n",
       " 'complex',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'with',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'percepttons',\n",
       " 'is',\n",
       " 'that',\n",
       " 'weight',\n",
       " 'and',\n",
       " 'offset',\n",
       " 'must',\n",
       " 'be',\n",
       " 'adjusted',\n",
       " 'carefully',\n",
       " 'because',\n",
       " 'they',\n",
       " 'interact',\n",
       " 'extensively',\n",
       " 'to',\n",
       " 'form',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'fig',\n",
       " 'illustrates',\n",
       " 'this',\n",
       " 'sensitivity',\n",
       " 'problem',\n",
       " 'here',\n",
       " 'it',\n",
       " 'can',\n",
       " 'be',\n",
       " 'seen',\n",
       " 'that',\n",
       " 'weight',\n",
       " 'to',\n",
       " 'one',\n",
       " 'hidden',\n",
       " 'node',\n",
       " 'form',\n",
       " 'hyperplane',\n",
       " 'which',\n",
       " 'influence',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'in',\n",
       " 'an',\n",
       " 'entire',\n",
       " 'hallplane',\n",
       " 'for',\n",
       " 'example',\n",
       " 'small',\n",
       " 'error',\n",
       " 'in',\n",
       " 'first',\n",
       " 'layer',\n",
       " 'weight',\n",
       " 'that',\n",
       " 'result',\n",
       " 'in',\n",
       " 'change',\n",
       " 'in',\n",
       " 'the',\n",
       " 'slope',\n",
       " 'of',\n",
       " 'hyperplanes',\n",
       " 'and',\n",
       " 'b6',\n",
       " 'might',\n",
       " 'only',\n",
       " 'slightly',\n",
       " 'extend',\n",
       " 'the',\n",
       " 'region',\n",
       " 'but',\n",
       " 'completely',\n",
       " 'eliminate',\n",
       " 'the',\n",
       " 'a2',\n",
       " 'region',\n",
       " 'this',\n",
       " 'interdependence',\n",
       " 'can',\n",
       " 'be',\n",
       " 'eliminated',\n",
       " 'in',\n",
       " 'three',\n",
       " 'layer',\n",
       " 'perceptrons',\n",
       " 'it',\n",
       " 'is',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'train',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'percepttons',\n",
       " 'to',\n",
       " 'form',\n",
       " 'complex',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'using',\n",
       " 'back',\n",
       " 'propagation',\n",
       " 'and',\n",
       " 'sigmoidal',\n",
       " 'nonlinearities',\n",
       " 'despite',\n",
       " 'weight',\n",
       " 'interaction',\n",
       " 'fig',\n",
       " 'for',\n",
       " 'example',\n",
       " 'show',\n",
       " 'disjoint',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'formed',\n",
       " 'using',\n",
       " 'back',\n",
       " 'propagation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'fig',\n",
       " 'in',\n",
       " 'this',\n",
       " 'and',\n",
       " 'all',\n",
       " 'other',\n",
       " 'simulation',\n",
       " 'input',\n",
       " 'were',\n",
       " 'presented',\n",
       " 'alternately',\n",
       " 'from',\n",
       " 'class',\n",
       " 'and',\n",
       " 'and',\n",
       " 'selected',\n",
       " 'from',\n",
       " 'uniform',\n",
       " 'distribution',\n",
       " 'covering',\n",
       " 'the',\n",
       " 'desired',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'the',\n",
       " 'back',\n",
       " 'propagation',\n",
       " 'rate',\n",
       " 'of',\n",
       " 'descent',\n",
       " 'term',\n",
       " 'wa',\n",
       " 'set',\n",
       " 'equal',\n",
       " 'to',\n",
       " 'the',\n",
       " 'momentum',\n",
       " 'gain',\n",
       " 'term',\n",
       " 'and',\n",
       " 'small',\n",
       " 'value',\n",
       " 'for',\n",
       " 'and',\n",
       " 'were',\n",
       " 'necessary',\n",
       " 'to',\n",
       " 'guarantee',\n",
       " 'convergence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'difficult',\n",
       " 'problem',\n",
       " 'in',\n",
       " 'fig',\n",
       " 'other',\n",
       " 'simulation',\n",
       " 'detail',\n",
       " 'are',\n",
       " 'fig',\n",
       " 'ten',\n",
       " 'complex',\n",
       " 'decision',\n",
       " 'region',\n",
       " 'formed',\n",
       " 'by',\n",
       " 'two',\n",
       " 'layer',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017366d3-85e9-4ae6-a9ce-3c16e66c7388",
   "metadata": {},
   "source": [
    "We find bigrams in the documents. Bigrams are sets of two adjacent words. Using bigrams we can get phrases like “machine_learning” in our output (spaces are replaced with underscores); without bigrams we would only get “machine” and “learning”.\n",
    "\n",
    "Note that in the code below, we find bigrams and then add them to the original data, because we would like to keep the words “machine” and “learning” as well as the bigram “machine_learning”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6831e002-a518-476f-8c8f-2019992bebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da442436-cd06-47f6-944e-d2fff0ac6433",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neural_net',\n",
       " 'and',\n",
       " 'traditional',\n",
       " 'classifier',\n",
       " 'william',\n",
       " 'huang',\n",
       " 'and',\n",
       " 'richard_lippmann',\n",
       " 'mit',\n",
       " 'lincoln_laboratory',\n",
       " 'lexington',\n",
       " 'ma',\n",
       " 'usa_abstract',\n",
       " 'previous_work',\n",
       " 'on',\n",
       " 'net',\n",
       " 'with',\n",
       " 'continuous_valued',\n",
       " 'input',\n",
       " 'led',\n",
       " 'to',\n",
       " 'generative',\n",
       " 'procedure',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'convex',\n",
       " 'decision_region',\n",
       " 'with',\n",
       " 'two',\n",
       " 'layer_percepttons',\n",
       " 'one',\n",
       " 'hidden_layer',\n",
       " 'and',\n",
       " 'arbitrary',\n",
       " 'decision_region',\n",
       " 'with',\n",
       " 'three',\n",
       " 'layer_percepttons',\n",
       " 'two',\n",
       " 'hidden_layer',\n",
       " 'here',\n",
       " 'we',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'perceptton',\n",
       " 'classifier',\n",
       " 'trained',\n",
       " 'with',\n",
       " 'back_propagation',\n",
       " 'can',\n",
       " 'form',\n",
       " 'both',\n",
       " 'convex',\n",
       " 'and',\n",
       " 'disjoint',\n",
       " 'decision_region',\n",
       " 'such',\n",
       " 'classifier',\n",
       " 'are',\n",
       " 'robust',\n",
       " 'train',\n",
       " 'rapidly',\n",
       " 'and',\n",
       " 'provide',\n",
       " 'good',\n",
       " 'performance',\n",
       " 'with',\n",
       " 'simple',\n",
       " 'decision_region',\n",
       " 'when',\n",
       " 'complex',\n",
       " 'decision_region',\n",
       " 'are',\n",
       " 'required',\n",
       " 'however',\n",
       " 'convergence',\n",
       " 'time',\n",
       " 'can_be',\n",
       " 'excessively',\n",
       " 'long',\n",
       " 'and',\n",
       " 'performance',\n",
       " 'is',\n",
       " 'often',\n",
       " 'no',\n",
       " 'better_than',\n",
       " 'that',\n",
       " 'of',\n",
       " 'nearest_neighbor',\n",
       " 'classifier',\n",
       " 'three',\n",
       " 'neural_net',\n",
       " 'classifier',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'that',\n",
       " 'provide',\n",
       " 'more',\n",
       " 'rapid',\n",
       " 'training',\n",
       " 'under',\n",
       " 'such',\n",
       " 'situation',\n",
       " 'two',\n",
       " 'use',\n",
       " 'fixed',\n",
       " 'weight',\n",
       " 'in',\n",
       " 'the',\n",
       " 'first',\n",
       " 'one',\n",
       " 'or',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'and',\n",
       " 'are',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'classifier',\n",
       " 'that',\n",
       " 'estimate',\n",
       " 'probability_density',\n",
       " 'function',\n",
       " 'using',\n",
       " 'histogram',\n",
       " 'third',\n",
       " 'feature_map',\n",
       " 'classifier',\n",
       " 'us',\n",
       " 'both',\n",
       " 'unsupervised',\n",
       " 'and',\n",
       " 'supervised',\n",
       " 'training',\n",
       " 'it',\n",
       " 'provides',\n",
       " 'good',\n",
       " 'performance',\n",
       " 'with',\n",
       " 'little',\n",
       " 'supervised',\n",
       " 'training',\n",
       " 'in',\n",
       " 'situation',\n",
       " 'such_a',\n",
       " 'speech_recognition',\n",
       " 'where',\n",
       " 'much',\n",
       " 'unlabeled',\n",
       " 'training',\n",
       " 'data',\n",
       " 'is',\n",
       " 'available',\n",
       " 'the',\n",
       " 'architecture',\n",
       " 'of',\n",
       " 'this',\n",
       " 'classifier',\n",
       " 'can_be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'implement',\n",
       " 'neural_net',\n",
       " 'nearest_neighbor',\n",
       " 'classifier',\n",
       " 'introduction',\n",
       " 'neural_net',\n",
       " 'architecture',\n",
       " 'can_be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'many',\n",
       " 'different',\n",
       " 'type',\n",
       " 'of',\n",
       " 'classi',\n",
       " 'tier',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'multi_layer',\n",
       " 'perceptron',\n",
       " 'classifier',\n",
       " 'with',\n",
       " 'continuous_valued',\n",
       " 'in',\n",
       " 'put',\n",
       " 'trained',\n",
       " 'with',\n",
       " 'back_propagation',\n",
       " 'are',\n",
       " 'robust',\n",
       " 'often',\n",
       " 'train',\n",
       " 'rapidly',\n",
       " 'and',\n",
       " 'provide',\n",
       " 'perfor_mance',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'that',\n",
       " 'provided',\n",
       " 'by',\n",
       " 'gaussian',\n",
       " 'classifier',\n",
       " 'when',\n",
       " 'decision_region',\n",
       " 'are',\n",
       " 'convex',\n",
       " 'generative',\n",
       " 'procedure',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'such',\n",
       " 'classifier',\n",
       " 'can',\n",
       " 'form',\n",
       " 'convex',\n",
       " 'deci_sion',\n",
       " 'region',\n",
       " 'with',\n",
       " 'two',\n",
       " 'layer_perceptrons',\n",
       " 'one',\n",
       " 'hidden_layer',\n",
       " 'and',\n",
       " 'arbitrary',\n",
       " 'decision_region',\n",
       " 'with',\n",
       " 'three',\n",
       " 'layer_perceptrons',\n",
       " 'two',\n",
       " 'hidden_layer',\n",
       " 'more',\n",
       " 'recent_work',\n",
       " 'ha',\n",
       " 'demon_strated',\n",
       " 'that',\n",
       " 'two',\n",
       " 'layer_perceptrons',\n",
       " 'can',\n",
       " 'form',\n",
       " 'non',\n",
       " 'convex',\n",
       " 'and',\n",
       " 'disjoint',\n",
       " 'decision_region',\n",
       " 'example',\n",
       " 'of',\n",
       " 'hand_crafted',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'network',\n",
       " 'which',\n",
       " 'generate',\n",
       " 'such',\n",
       " 'decision_region',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'in',\n",
       " 'this_paper',\n",
       " 'along',\n",
       " 'with',\n",
       " 'monte_carlo',\n",
       " 'simulation',\n",
       " 'where',\n",
       " 'complex',\n",
       " 'decision_region',\n",
       " 'were_generated',\n",
       " 'using',\n",
       " 'back_propagation',\n",
       " 'training',\n",
       " 'these',\n",
       " 'and',\n",
       " 'previous',\n",
       " 'simula',\n",
       " 'tions',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'convergence',\n",
       " 'time',\n",
       " 'with',\n",
       " 'back_propagation',\n",
       " 'can_be',\n",
       " 'excessive',\n",
       " 'when',\n",
       " 'complex',\n",
       " 'decision_region',\n",
       " 'are',\n",
       " 'desired',\n",
       " 'and',\n",
       " 'performance',\n",
       " 'is',\n",
       " 'often',\n",
       " 'no',\n",
       " 'better_than',\n",
       " 'that',\n",
       " 'obtained',\n",
       " 'with',\n",
       " 'nearest_neighbor',\n",
       " 'classifier',\n",
       " 'these',\n",
       " 'result',\n",
       " 'led',\n",
       " 'u',\n",
       " 'to',\n",
       " 'explore',\n",
       " 'other',\n",
       " 'neural_net',\n",
       " 'classifier',\n",
       " 'that',\n",
       " 'might',\n",
       " 'provide',\n",
       " 'faster',\n",
       " 'convergence',\n",
       " 'three',\n",
       " 'classifier',\n",
       " 'called',\n",
       " 'fixed',\n",
       " 'weight',\n",
       " 'hypercube',\n",
       " 'and',\n",
       " 'feature_map',\n",
       " 'classifier',\n",
       " 'were',\n",
       " 'developed',\n",
       " 'and',\n",
       " 'eval',\n",
       " 'uated',\n",
       " 'all',\n",
       " 'classifier',\n",
       " 'were',\n",
       " 'tested',\n",
       " 'on',\n",
       " 'illustrative',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'two',\n",
       " 'continuous_valued',\n",
       " 'input',\n",
       " 'and',\n",
       " 'two',\n",
       " 'class',\n",
       " 'and',\n",
       " 'more',\n",
       " 'restricted',\n",
       " 'set',\n",
       " 'of',\n",
       " 'classifier',\n",
       " 'wa_tested',\n",
       " 'with',\n",
       " 'vowel',\n",
       " 'formant',\n",
       " 'data',\n",
       " 'capability',\n",
       " 'of',\n",
       " 'two',\n",
       " 'layer_perceptrons',\n",
       " 'multi_layer',\n",
       " 'perceptron',\n",
       " 'classifier',\n",
       " 'with',\n",
       " 'hard',\n",
       " 'limiting',\n",
       " 'nonlinearities',\n",
       " 'node',\n",
       " 'output',\n",
       " 'of',\n",
       " 'or',\n",
       " 'and',\n",
       " 'continuous_valued',\n",
       " 'input',\n",
       " 'can',\n",
       " 'form',\n",
       " 'complex',\n",
       " 'decision_region',\n",
       " 'simple',\n",
       " 'constructive',\n",
       " 'proof',\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'three',\n",
       " 'layer_perceptron',\n",
       " 'two',\n",
       " 'hidden_layer',\n",
       " 'can',\n",
       " 'this',\n",
       " 'work_wa',\n",
       " 'sponsored_by',\n",
       " 'the',\n",
       " 'defense_advanced',\n",
       " 'research_project',\n",
       " 'agency',\n",
       " 'and',\n",
       " 'the',\n",
       " 'department',\n",
       " 'of',\n",
       " 'the',\n",
       " 'air_force',\n",
       " 'the',\n",
       " 'view',\n",
       " 'expressed',\n",
       " 'are',\n",
       " 'those',\n",
       " 'of',\n",
       " 'the',\n",
       " 'author',\n",
       " 'and',\n",
       " 'do_not',\n",
       " 'reflect',\n",
       " 'the',\n",
       " 'policy',\n",
       " 'or',\n",
       " 'position',\n",
       " 'of',\n",
       " 'the',\n",
       " 'government',\n",
       " 'american_institute',\n",
       " 'of',\n",
       " 'physic',\n",
       " 'xl',\n",
       " 'decision_region',\n",
       " 'for',\n",
       " 'class',\n",
       " 'bl',\n",
       " 'b2',\n",
       " 'b6',\n",
       " 'xl',\n",
       " 'fig',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'perceptton',\n",
       " 'that',\n",
       " 'form',\n",
       " 'disjoint',\n",
       " 'decision',\n",
       " 're9ions',\n",
       " 'for',\n",
       " 'class',\n",
       " 'shaded_area',\n",
       " 'connec',\n",
       " 'tion',\n",
       " 'weight',\n",
       " 'and',\n",
       " 'node',\n",
       " 'offset',\n",
       " 'are',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'the',\n",
       " 'left',\n",
       " 'hyperplanes',\n",
       " 'formed',\n",
       " 'by',\n",
       " 'all',\n",
       " 'hidden',\n",
       " 'node',\n",
       " 'are',\n",
       " 'drawn',\n",
       " 'a',\n",
       " 'dashed_line',\n",
       " 'with',\n",
       " 'node',\n",
       " 'label',\n",
       " 'arrow',\n",
       " 'on',\n",
       " 'these',\n",
       " 'line',\n",
       " 'point',\n",
       " 'to',\n",
       " 'the',\n",
       " 'half_plane',\n",
       " 'where',\n",
       " 'the',\n",
       " 'hidden',\n",
       " 'node',\n",
       " 'output',\n",
       " 'is',\n",
       " 'high',\n",
       " 'form',\n",
       " 'arbitrary',\n",
       " 'decision_region',\n",
       " 'and',\n",
       " 'two',\n",
       " 'layer_perceptron',\n",
       " 'one',\n",
       " 'hidden_layer',\n",
       " 'can',\n",
       " 'form',\n",
       " 'single',\n",
       " 'convex',\n",
       " 'decision_region',\n",
       " 'recently',\n",
       " 'however',\n",
       " 'it',\n",
       " 'ha_been',\n",
       " 'demonstrated',\n",
       " 'that',\n",
       " 'two',\n",
       " 'layer_perceptrons',\n",
       " 'can',\n",
       " 'form',\n",
       " 'decision_region',\n",
       " 'that',\n",
       " 'are',\n",
       " 'not',\n",
       " 'simply',\n",
       " 'convex',\n",
       " 'fig',\n",
       " 'for',\n",
       " 'example',\n",
       " 'show_how',\n",
       " 'disjoint',\n",
       " 'decision_region',\n",
       " 'can_be',\n",
       " 'generated',\n",
       " 'using',\n",
       " 'two',\n",
       " 'layer_perceptron',\n",
       " 'the',\n",
       " 'two',\n",
       " 'disjoint',\n",
       " 'shaded_area',\n",
       " 'in',\n",
       " 'this',\n",
       " 'fig',\n",
       " 'represent',\n",
       " 'the',\n",
       " 'decision_region',\n",
       " 'for',\n",
       " 'class',\n",
       " 'output',\n",
       " 'node',\n",
       " 'ha',\n",
       " 'high',\n",
       " 'output',\n",
       " 'the',\n",
       " 'remaining',\n",
       " 'area',\n",
       " 'represents',\n",
       " 'the',\n",
       " 'decision_region',\n",
       " 'for',\n",
       " 'class',\n",
       " 'output',\n",
       " 'node',\n",
       " 'ha',\n",
       " 'low',\n",
       " 'output',\n",
       " 'node',\n",
       " 'in',\n",
       " 'this',\n",
       " 'fig',\n",
       " 'contain',\n",
       " 'hard',\n",
       " 'limiting',\n",
       " 'nonlinearities',\n",
       " 'connection',\n",
       " 'weight',\n",
       " 'and',\n",
       " 'node',\n",
       " 'offset',\n",
       " 'are',\n",
       " 'indicated',\n",
       " 'in',\n",
       " 'the',\n",
       " 'left',\n",
       " 'diagram',\n",
       " 'ten',\n",
       " 'other',\n",
       " 'complex',\n",
       " 'decision_region',\n",
       " 'formed',\n",
       " 'using',\n",
       " 'two',\n",
       " 'layer_perceptrons',\n",
       " 'are',\n",
       " 'presented',\n",
       " 'in',\n",
       " 'fig',\n",
       " 'the',\n",
       " 'above',\n",
       " 'example',\n",
       " 'suggest_that',\n",
       " 'two',\n",
       " 'layer_perceptrons',\n",
       " 'can',\n",
       " 'form',\n",
       " 'decision_region',\n",
       " 'with',\n",
       " 'arbitrary',\n",
       " 'shape',\n",
       " 'we',\n",
       " 'however',\n",
       " 'know',\n",
       " 'of',\n",
       " 'no',\n",
       " 'general',\n",
       " 'proof',\n",
       " 'of',\n",
       " 'this',\n",
       " 'capability',\n",
       " 'book',\n",
       " 'by',\n",
       " 'nilson',\n",
       " 'discus',\n",
       " 'this',\n",
       " 'issue',\n",
       " 'and',\n",
       " 'contains',\n",
       " 'proof',\n",
       " 'that',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'net',\n",
       " 'can',\n",
       " 'divide',\n",
       " 'finite',\n",
       " 'number',\n",
       " 'of',\n",
       " 'point',\n",
       " 'into',\n",
       " 'two',\n",
       " 'arbitrary',\n",
       " 'set',\n",
       " 'page',\n",
       " 'this',\n",
       " 'proof',\n",
       " 'involves',\n",
       " 'separating',\n",
       " 'point',\n",
       " 'using',\n",
       " 'at',\n",
       " 'most',\n",
       " 'parallel',\n",
       " 'hyperplanes',\n",
       " 'formed',\n",
       " 'by',\n",
       " 'first',\n",
       " 'layer',\n",
       " 'node',\n",
       " 'where',\n",
       " 'no',\n",
       " 'hyperplane',\n",
       " 'intersects',\n",
       " 'two',\n",
       " 'or',\n",
       " 'more',\n",
       " 'point',\n",
       " 'proving',\n",
       " 'that',\n",
       " 'given',\n",
       " 'decision_region',\n",
       " 'can_be',\n",
       " 'formed',\n",
       " 'in',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'net',\n",
       " 'involves',\n",
       " 'testing',\n",
       " 'to',\n",
       " 'determine_whether',\n",
       " 'the',\n",
       " 'boolean',\n",
       " 'representation',\n",
       " 'at',\n",
       " 'the',\n",
       " 'output',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'layer',\n",
       " 'for',\n",
       " 'all',\n",
       " 'point',\n",
       " 'within',\n",
       " 'the',\n",
       " 'decision_region',\n",
       " 'for',\n",
       " 'class',\n",
       " 'are',\n",
       " 'linearly_separable',\n",
       " 'from',\n",
       " 'the',\n",
       " 'boolean',\n",
       " 'representation',\n",
       " 'for',\n",
       " 'class',\n",
       " 'one',\n",
       " 'test',\n",
       " 'for',\n",
       " 'linear',\n",
       " 'separability',\n",
       " 'wa',\n",
       " 'presented',\n",
       " 'in',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'forming',\n",
       " 'complex',\n",
       " 'decision_region',\n",
       " 'with',\n",
       " 'two',\n",
       " 'layer_percepttons',\n",
       " 'is',\n",
       " 'that',\n",
       " 'weight',\n",
       " 'and',\n",
       " 'offset',\n",
       " 'must_be',\n",
       " 'adjusted',\n",
       " 'carefully',\n",
       " 'because_they',\n",
       " 'interact',\n",
       " 'extensively',\n",
       " 'to',\n",
       " 'form',\n",
       " 'decision_region',\n",
       " 'fig',\n",
       " 'illustrates',\n",
       " 'this',\n",
       " 'sensitivity',\n",
       " 'problem',\n",
       " 'here',\n",
       " 'it',\n",
       " 'can_be',\n",
       " 'seen',\n",
       " 'that',\n",
       " 'weight',\n",
       " 'to',\n",
       " 'one',\n",
       " 'hidden',\n",
       " 'node',\n",
       " 'form',\n",
       " 'hyperplane',\n",
       " 'which',\n",
       " 'influence',\n",
       " 'decision_region',\n",
       " 'in',\n",
       " 'an',\n",
       " 'entire',\n",
       " 'hallplane',\n",
       " 'for',\n",
       " 'example',\n",
       " 'small',\n",
       " 'error',\n",
       " 'in',\n",
       " 'first',\n",
       " 'layer',\n",
       " 'weight',\n",
       " 'that',\n",
       " 'result',\n",
       " 'in',\n",
       " 'change',\n",
       " 'in',\n",
       " 'the',\n",
       " 'slope',\n",
       " 'of',\n",
       " 'hyperplanes',\n",
       " 'and',\n",
       " 'b6',\n",
       " 'might',\n",
       " 'only',\n",
       " 'slightly',\n",
       " 'extend',\n",
       " 'the',\n",
       " 'region',\n",
       " 'but',\n",
       " 'completely',\n",
       " 'eliminate',\n",
       " 'the',\n",
       " 'a2',\n",
       " 'region',\n",
       " 'this',\n",
       " 'interdependence',\n",
       " 'can_be',\n",
       " 'eliminated',\n",
       " 'in',\n",
       " 'three',\n",
       " 'layer_perceptrons',\n",
       " 'it',\n",
       " 'is',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'train',\n",
       " 'two',\n",
       " 'layer_percepttons',\n",
       " 'to',\n",
       " 'form',\n",
       " 'complex',\n",
       " 'decision_region',\n",
       " 'using',\n",
       " 'back_propagation',\n",
       " 'and',\n",
       " 'sigmoidal',\n",
       " 'nonlinearities',\n",
       " 'despite',\n",
       " 'weight',\n",
       " 'interaction',\n",
       " 'fig',\n",
       " 'for',\n",
       " 'example',\n",
       " 'show',\n",
       " 'disjoint',\n",
       " 'decision_region',\n",
       " 'formed',\n",
       " 'using',\n",
       " 'back_propagation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'fig',\n",
       " 'in',\n",
       " 'this',\n",
       " 'and',\n",
       " 'all',\n",
       " 'other',\n",
       " 'simulation',\n",
       " 'input',\n",
       " 'were',\n",
       " 'presented',\n",
       " 'alternately',\n",
       " 'from',\n",
       " 'class',\n",
       " 'and',\n",
       " 'and',\n",
       " 'selected',\n",
       " 'from',\n",
       " 'uniform_distribution',\n",
       " 'covering',\n",
       " 'the',\n",
       " 'desired',\n",
       " 'decision_region',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'the',\n",
       " 'back_propagation',\n",
       " 'rate',\n",
       " 'of',\n",
       " 'descent',\n",
       " 'term',\n",
       " 'wa',\n",
       " 'set',\n",
       " 'equal',\n",
       " 'to',\n",
       " 'the',\n",
       " 'momentum',\n",
       " 'gain',\n",
       " 'term',\n",
       " 'and',\n",
       " 'small',\n",
       " 'value',\n",
       " 'for',\n",
       " 'and',\n",
       " 'were',\n",
       " 'necessary',\n",
       " 'to',\n",
       " 'guarantee',\n",
       " 'convergence',\n",
       " 'for',\n",
       " 'the',\n",
       " 'difficult',\n",
       " 'problem',\n",
       " 'in',\n",
       " 'fig',\n",
       " 'other',\n",
       " 'simulation',\n",
       " 'detail',\n",
       " 'are',\n",
       " 'fig',\n",
       " 'ten',\n",
       " 'complex',\n",
       " 'decision_region',\n",
       " 'formed',\n",
       " 'by',\n",
       " 'two',\n",
       " 'layer_perceptrons',\n",
       " 'the',\n",
       " 'number',\n",
       " 'assigned',\n",
       " 'to',\n",
       " 'each',\n",
       " 'case',\n",
       " 'are',\n",
       " 'the',\n",
       " 'acase',\n",
       " 'number',\n",
       " 'used',\n",
       " 'in',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'this_paper',\n",
       " 'a',\n",
       " 'in',\n",
       " 'also',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'fig',\n",
       " 'are',\n",
       " 'hyperplanes',\n",
       " 'formed',\n",
       " 'by',\n",
       " 'those',\n",
       " 'first',\n",
       " 'layer',\n",
       " 'node',\n",
       " 'with',\n",
       " 'the',\n",
       " 'strongest',\n",
       " 'connection',\n",
       " 'weight',\n",
       " 'to',\n",
       " 'the',\n",
       " 'output',\n",
       " 'node',\n",
       " 'these',\n",
       " 'hyperplanes',\n",
       " 'and',\n",
       " 'weight',\n",
       " 'are',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'those',\n",
       " 'in',\n",
       " 'the',\n",
       " 'network',\n",
       " 'created',\n",
       " 'by',\n",
       " 'hand',\n",
       " 'except',\n",
       " 'for',\n",
       " 'sign',\n",
       " 'inversion',\n",
       " 'the',\n",
       " 'occurrence',\n",
       " 'of',\n",
       " 'multiple',\n",
       " 'similar',\n",
       " 'hyperplanes',\n",
       " 'formed',\n",
       " 'by',\n",
       " 'two',\n",
       " 'node',\n",
       " 'and',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'node',\n",
       " 'offset',\n",
       " 'with',\n",
       " 'value',\n",
       " 'near_zero',\n",
       " 'comparative',\n",
       " 'result',\n",
       " 'of',\n",
       " 'two',\n",
       " 'layer',\n",
       " 'v',\n",
       " 'three',\n",
       " 'layer',\n",
       " 'previous',\n",
       " 'result',\n",
       " 'a_well',\n",
       " 'a',\n",
       " 'the',\n",
       " 'weight',\n",
       " 'interaction',\n",
       " 'mentioned_above',\n",
       " 'suggest_that',\n",
       " 'three',\n",
       " 'layer_percepttons',\n",
       " 'may_be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'form',\n",
       " 'complex',\n",
       " 'decision_region',\n",
       " 'faster',\n",
       " 'with',\n",
       " 'back_propagation',\n",
       " 'than',\n",
       " 'two',\n",
       " 'layer_percepttons',\n",
       " 'this',\n",
       " 'wa',\n",
       " 'explored',\n",
       " 'using',\n",
       " 'monte_carlo',\n",
       " 'simulation',\n",
       " 'for',\n",
       " 'the',\n",
       " 'first',\n",
       " 'nine',\n",
       " 'case',\n",
       " 'of',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram[docs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e095741f-8cee-4976-9f76-44d9b88acf3e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_map',\n",
       " 'must_be',\n",
       " 'nearest_neighbor',\n",
       " 'monte_carlo',\n",
       " 'error_rate',\n",
       " 'back_prop',\n",
       " 'percent_correct',\n",
       " 'feature_map',\n",
       " 'feature_map',\n",
       " 'doe_not',\n",
       " 'multi_layer',\n",
       " 'back_propagation',\n",
       " 'feature_map',\n",
       " 'error_rate',\n",
       " 'nearest_neighbor',\n",
       " 'fewer_than',\n",
       " 'feature_map',\n",
       " 'probability_density',\n",
       " 'non_zero',\n",
       " 'can_be',\n",
       " 'feature_map',\n",
       " 'worse_than',\n",
       " 'back_propagation',\n",
       " 'slower_than',\n",
       " 'maximum_likelihood',\n",
       " 'multi_layer',\n",
       " 'feature_map',\n",
       " 'had_been',\n",
       " 'split_into',\n",
       " 'fig_show',\n",
       " 'decision_region',\n",
       " 'back_propagation',\n",
       " 'error_rate',\n",
       " 'feature_map',\n",
       " 'le_than',\n",
       " 'back_propagation',\n",
       " 'more_than',\n",
       " 'feature_map',\n",
       " 'multi_layer',\n",
       " 'had_been',\n",
       " 'decision_region',\n",
       " 'steady_state',\n",
       " 'feature_map',\n",
       " 'steady_state',\n",
       " 'neural_net',\n",
       " 'can_be',\n",
       " 'nearest_neighbor',\n",
       " 'multi_layer',\n",
       " 'a_well',\n",
       " 'such_a',\n",
       " 'feature_map',\n",
       " 'layer_percepttons',\n",
       " 'hidden_layer',\n",
       " 'decision_region',\n",
       " 'back_propagation',\n",
       " 'can_be',\n",
       " 'decision_region',\n",
       " 'multi_layer',\n",
       " 'can_be',\n",
       " 'feature_map',\n",
       " 'can_be',\n",
       " 'nearest_neighbor',\n",
       " 'feature_map',\n",
       " 'error_rate',\n",
       " 'nearest_neighbor',\n",
       " 'nearest_neighbor',\n",
       " 'mcgraw_hill',\n",
       " 'neural_network',\n",
       " 'international_conference',\n",
       " 'system_man',\n",
       " 'supervised_learning',\n",
       " 'ieee_transaction',\n",
       " 'vol_pp',\n",
       " 'hart_pattern',\n",
       " 'scene_analysis',\n",
       " 'john_wiley',\n",
       " 'son_new',\n",
       " 'neural_net',\n",
       " 'international_conference',\n",
       " 'neural_network',\n",
       " 'speech_recognition',\n",
       " 'confer_ence',\n",
       " 'pattern_recognition',\n",
       " 'neural_net',\n",
       " 'ieee_assp',\n",
       " 'vol_pp',\n",
       " 'speech_recognition',\n",
       " 'international_conference',\n",
       " 'neural_network',\n",
       " 'pattern_recognition',\n",
       " 'multi_layer',\n",
       " 'mcgraw_hill',\n",
       " 'mcgraw_hill',\n",
       " 'new_york',\n",
       " 'self_organizing',\n",
       " 'self_organization',\n",
       " 'neural_network',\n",
       " 'interna_tional',\n",
       " 'conference_on',\n",
       " 'neural_network']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0][-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9689a0-54a3-4f4a-b7af-7df48ba9593d",
   "metadata": {},
   "source": [
    "We remove rare words and common words based on their document frequency. Below we remove words that appear in less than 20 documents or in more than 50% of the documents. Consider trying to remove words only based on their frequency, or maybe combining that with this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30f25d75-b668-4cd7-bbe4-ced28e374143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "# Dictionary – a mapping between words and their integer ids.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15280ff5-c9d5-43e3-8344-970aa1010bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comparative'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9096219c-38a0-43f7-b3cb-c1c3c7000d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (6, 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [['human', 'interface', 'computer']]\n",
    "dct = Dictionary(texts)  # initialize a Dictionary\n",
    "dct.add_documents([[\"cat\", \"say\", \"meow\", \"human\"], [\"dog\"]])  # add more document (extend the vocabulary)\n",
    "dct.doc2bow([\"dog\", \"human\", \"human\", \"non_existent_word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da1dac07-aa75-438f-8d45-6004c9d229ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "# doc2bow: Convert document into the bag-of-words (BoW) format = list of (token_id, token_count) tuples.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35e32e26-5ec1-4457-a203-e7c3c0b1ecf4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 2),\n",
       " (5, 2),\n",
       " (6, 1),\n",
       " (7, 2),\n",
       " (8, 1),\n",
       " (9, 3),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 2),\n",
       " (15, 2),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 3),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 5),\n",
       " (26, 4),\n",
       " (27, 6),\n",
       " (28, 2),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 3),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 23),\n",
       " (42, 1),\n",
       " (43, 20),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 2),\n",
       " (49, 2),\n",
       " (50, 2),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 3),\n",
       " (54, 2),\n",
       " (55, 1),\n",
       " (56, 5),\n",
       " (57, 2),\n",
       " (58, 2),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 2),\n",
       " (63, 3),\n",
       " (64, 1),\n",
       " (65, 3),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 19),\n",
       " (71, 1),\n",
       " (72, 4),\n",
       " (73, 1),\n",
       " (74, 86),\n",
       " (75, 1),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 4),\n",
       " (80, 2),\n",
       " (81, 1),\n",
       " (82, 2),\n",
       " (83, 1),\n",
       " (84, 13),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 4),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 4),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 3),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 1),\n",
       " (99, 1),\n",
       " (100, 4),\n",
       " (101, 4),\n",
       " (102, 2),\n",
       " (103, 1),\n",
       " (104, 15),\n",
       " (105, 9),\n",
       " (106, 1),\n",
       " (107, 1),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 2),\n",
       " (111, 1),\n",
       " (112, 1),\n",
       " (113, 2),\n",
       " (114, 1),\n",
       " (115, 4),\n",
       " (116, 1),\n",
       " (117, 3),\n",
       " (118, 3),\n",
       " (119, 1),\n",
       " (120, 53),\n",
       " (121, 3),\n",
       " (122, 49),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 1),\n",
       " (126, 1),\n",
       " (127, 1),\n",
       " (128, 5),\n",
       " (129, 2),\n",
       " (130, 5),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 5),\n",
       " (137, 1),\n",
       " (138, 1),\n",
       " (139, 3),\n",
       " (140, 2),\n",
       " (141, 1),\n",
       " (142, 2),\n",
       " (143, 1),\n",
       " (144, 1),\n",
       " (145, 3),\n",
       " (146, 1),\n",
       " (147, 1),\n",
       " (148, 1),\n",
       " (149, 11),\n",
       " (150, 1),\n",
       " (151, 2),\n",
       " (152, 3),\n",
       " (153, 1),\n",
       " (154, 1),\n",
       " (155, 1),\n",
       " (156, 1),\n",
       " (157, 1),\n",
       " (158, 1),\n",
       " (159, 1),\n",
       " (160, 1),\n",
       " (161, 1),\n",
       " (162, 2),\n",
       " (163, 1),\n",
       " (164, 6),\n",
       " (165, 1),\n",
       " (166, 2),\n",
       " (167, 1),\n",
       " (168, 3),\n",
       " (169, 1),\n",
       " (170, 2),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 1),\n",
       " (174, 1),\n",
       " (175, 1),\n",
       " (176, 1),\n",
       " (177, 2),\n",
       " (178, 2),\n",
       " (179, 1),\n",
       " (180, 4),\n",
       " (181, 22),\n",
       " (182, 1),\n",
       " (183, 1),\n",
       " (184, 3),\n",
       " (185, 1),\n",
       " (186, 33),\n",
       " (187, 2),\n",
       " (188, 2),\n",
       " (189, 1),\n",
       " (190, 2),\n",
       " (191, 1),\n",
       " (192, 1),\n",
       " (193, 1),\n",
       " (194, 1),\n",
       " (195, 3),\n",
       " (196, 16),\n",
       " (197, 3),\n",
       " (198, 1),\n",
       " (199, 2),\n",
       " (200, 1),\n",
       " (201, 1),\n",
       " (202, 1),\n",
       " (203, 1),\n",
       " (204, 1),\n",
       " (205, 2),\n",
       " (206, 3),\n",
       " (207, 1),\n",
       " (208, 1),\n",
       " (209, 2),\n",
       " (210, 2),\n",
       " (211, 1),\n",
       " (212, 1),\n",
       " (213, 1),\n",
       " (214, 5),\n",
       " (215, 1),\n",
       " (216, 1),\n",
       " (217, 8),\n",
       " (218, 1),\n",
       " (219, 1),\n",
       " (220, 5),\n",
       " (221, 2),\n",
       " (222, 1),\n",
       " (223, 2),\n",
       " (224, 2),\n",
       " (225, 1),\n",
       " (226, 1),\n",
       " (227, 1),\n",
       " (228, 6),\n",
       " (229, 1),\n",
       " (230, 15),\n",
       " (231, 9),\n",
       " (232, 1),\n",
       " (233, 3),\n",
       " (234, 4),\n",
       " (235, 1),\n",
       " (236, 2),\n",
       " (237, 14),\n",
       " (238, 2),\n",
       " (239, 18),\n",
       " (240, 7),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 1),\n",
       " (244, 1),\n",
       " (245, 1),\n",
       " (246, 1),\n",
       " (247, 1),\n",
       " (248, 4),\n",
       " (249, 1),\n",
       " (250, 1),\n",
       " (251, 1),\n",
       " (252, 2),\n",
       " (253, 1),\n",
       " (254, 2),\n",
       " (255, 1),\n",
       " (256, 1),\n",
       " (257, 1),\n",
       " (258, 1),\n",
       " (259, 1),\n",
       " (260, 1),\n",
       " (261, 1),\n",
       " (262, 1),\n",
       " (263, 1),\n",
       " (264, 2),\n",
       " (265, 1),\n",
       " (266, 1),\n",
       " (267, 4),\n",
       " (268, 3),\n",
       " (269, 1),\n",
       " (270, 2),\n",
       " (271, 1),\n",
       " (272, 1),\n",
       " (273, 1),\n",
       " (274, 1),\n",
       " (275, 1),\n",
       " (276, 3),\n",
       " (277, 1),\n",
       " (278, 3),\n",
       " (279, 2),\n",
       " (280, 1),\n",
       " (281, 1),\n",
       " (282, 1),\n",
       " (283, 3),\n",
       " (284, 87),\n",
       " (285, 5),\n",
       " (286, 1),\n",
       " (287, 2),\n",
       " (288, 4),\n",
       " (289, 5),\n",
       " (290, 1),\n",
       " (291, 2),\n",
       " (292, 1),\n",
       " (293, 1),\n",
       " (294, 1),\n",
       " (295, 1),\n",
       " (296, 4),\n",
       " (297, 3),\n",
       " (298, 3),\n",
       " (299, 2),\n",
       " (300, 1),\n",
       " (301, 1),\n",
       " (302, 2),\n",
       " (303, 1),\n",
       " (304, 1),\n",
       " (305, 1),\n",
       " (306, 1),\n",
       " (307, 23),\n",
       " (308, 1),\n",
       " (309, 10),\n",
       " (310, 5),\n",
       " (311, 3),\n",
       " (312, 3),\n",
       " (313, 1),\n",
       " (314, 1),\n",
       " (315, 1),\n",
       " (316, 1),\n",
       " (317, 1),\n",
       " (318, 1),\n",
       " (319, 2),\n",
       " (320, 1),\n",
       " (321, 1),\n",
       " (322, 2),\n",
       " (323, 1),\n",
       " (324, 1),\n",
       " (325, 3),\n",
       " (326, 3),\n",
       " (327, 1),\n",
       " (328, 1),\n",
       " (329, 10),\n",
       " (330, 10),\n",
       " (331, 1),\n",
       " (332, 3),\n",
       " (333, 1),\n",
       " (334, 1),\n",
       " (335, 4),\n",
       " (336, 1),\n",
       " (337, 10),\n",
       " (338, 10),\n",
       " (339, 1),\n",
       " (340, 1),\n",
       " (341, 10),\n",
       " (342, 12),\n",
       " (343, 9),\n",
       " (344, 1),\n",
       " (345, 1),\n",
       " (346, 5),\n",
       " (347, 1),\n",
       " (348, 1),\n",
       " (349, 48),\n",
       " (350, 1),\n",
       " (351, 3),\n",
       " (352, 1),\n",
       " (353, 1),\n",
       " (354, 1),\n",
       " (355, 1),\n",
       " (356, 2),\n",
       " (357, 5),\n",
       " (358, 3),\n",
       " (359, 1),\n",
       " (360, 1),\n",
       " (361, 2),\n",
       " (362, 1),\n",
       " (363, 1),\n",
       " (364, 1),\n",
       " (365, 1),\n",
       " (366, 1),\n",
       " (367, 2),\n",
       " (368, 1),\n",
       " (369, 2),\n",
       " (370, 1),\n",
       " (371, 4),\n",
       " (372, 9),\n",
       " (373, 12),\n",
       " (374, 9),\n",
       " (375, 8),\n",
       " (376, 1),\n",
       " (377, 1),\n",
       " (378, 1),\n",
       " (379, 2),\n",
       " (380, 2),\n",
       " (381, 1),\n",
       " (382, 1),\n",
       " (383, 1),\n",
       " (384, 1),\n",
       " (385, 1),\n",
       " (386, 1),\n",
       " (387, 1),\n",
       " (388, 3),\n",
       " (389, 4),\n",
       " (390, 1),\n",
       " (391, 4),\n",
       " (392, 2),\n",
       " (393, 2),\n",
       " (394, 1),\n",
       " (395, 4),\n",
       " (396, 1),\n",
       " (397, 20),\n",
       " (398, 1),\n",
       " (399, 6),\n",
       " (400, 7),\n",
       " (401, 3),\n",
       " (402, 1),\n",
       " (403, 1),\n",
       " (404, 1),\n",
       " (405, 2),\n",
       " (406, 1),\n",
       " (407, 1),\n",
       " (408, 1),\n",
       " (409, 2),\n",
       " (410, 1),\n",
       " (411, 1),\n",
       " (412, 1),\n",
       " (413, 5),\n",
       " (414, 1),\n",
       " (415, 2),\n",
       " (416, 1),\n",
       " (417, 61),\n",
       " (418, 1),\n",
       " (419, 1),\n",
       " (420, 1),\n",
       " (421, 1),\n",
       " (422, 3),\n",
       " (423, 1),\n",
       " (424, 10),\n",
       " (425, 1),\n",
       " (426, 1),\n",
       " (427, 1),\n",
       " (428, 1),\n",
       " (429, 1),\n",
       " (430, 1),\n",
       " (431, 1),\n",
       " (432, 1),\n",
       " (433, 1),\n",
       " (434, 2),\n",
       " (435, 1),\n",
       " (436, 2),\n",
       " (437, 4),\n",
       " (438, 1),\n",
       " (439, 1),\n",
       " (440, 1),\n",
       " (441, 2),\n",
       " (442, 3),\n",
       " (443, 2),\n",
       " (444, 1),\n",
       " (445, 3),\n",
       " (446, 1),\n",
       " (447, 2),\n",
       " (448, 1),\n",
       " (449, 2),\n",
       " (450, 1),\n",
       " (451, 4),\n",
       " (452, 2),\n",
       " (453, 1),\n",
       " (454, 4),\n",
       " (455, 1),\n",
       " (456, 1),\n",
       " (457, 1),\n",
       " (458, 1),\n",
       " (459, 1),\n",
       " (460, 1),\n",
       " (461, 2),\n",
       " (462, 1),\n",
       " (463, 1),\n",
       " (464, 1),\n",
       " (465, 7),\n",
       " (466, 1),\n",
       " (467, 3),\n",
       " (468, 2),\n",
       " (469, 1),\n",
       " (470, 1),\n",
       " (471, 1),\n",
       " (472, 1),\n",
       " (473, 1),\n",
       " (474, 1),\n",
       " (475, 1),\n",
       " (476, 1),\n",
       " (477, 2),\n",
       " (478, 1),\n",
       " (479, 1),\n",
       " (480, 1),\n",
       " (481, 1),\n",
       " (482, 5),\n",
       " (483, 3),\n",
       " (484, 1),\n",
       " (485, 1),\n",
       " (486, 1),\n",
       " (487, 1),\n",
       " (488, 1),\n",
       " (489, 1),\n",
       " (490, 1),\n",
       " (491, 2),\n",
       " (492, 2),\n",
       " (493, 1),\n",
       " (494, 1),\n",
       " (495, 1),\n",
       " (496, 1),\n",
       " (497, 1),\n",
       " (498, 2),\n",
       " (499, 2),\n",
       " (500, 1),\n",
       " (501, 1),\n",
       " (502, 1),\n",
       " (503, 11),\n",
       " (504, 1),\n",
       " (505, 1),\n",
       " (506, 1),\n",
       " (507, 1),\n",
       " (508, 2),\n",
       " (509, 1),\n",
       " (510, 1),\n",
       " (511, 4),\n",
       " (512, 4),\n",
       " (513, 2),\n",
       " (514, 1),\n",
       " (515, 1),\n",
       " (516, 1),\n",
       " (517, 1),\n",
       " (518, 1),\n",
       " (519, 1),\n",
       " (520, 11),\n",
       " (521, 4),\n",
       " (522, 1),\n",
       " (523, 4),\n",
       " (524, 5),\n",
       " (525, 1),\n",
       " (526, 1),\n",
       " (527, 13),\n",
       " (528, 1),\n",
       " (529, 10),\n",
       " (530, 1),\n",
       " (531, 1),\n",
       " (532, 1),\n",
       " (533, 1),\n",
       " (534, 1),\n",
       " (535, 1),\n",
       " (536, 1),\n",
       " (537, 1),\n",
       " (538, 3),\n",
       " (539, 9),\n",
       " (540, 1),\n",
       " (541, 1),\n",
       " (542, 4),\n",
       " (543, 2),\n",
       " (544, 1),\n",
       " (545, 1),\n",
       " (546, 1),\n",
       " (547, 2),\n",
       " (548, 4),\n",
       " (549, 1),\n",
       " (550, 1),\n",
       " (551, 1),\n",
       " (552, 1),\n",
       " (553, 1),\n",
       " (554, 1),\n",
       " (555, 2),\n",
       " (556, 2),\n",
       " (557, 10),\n",
       " (558, 1),\n",
       " (559, 1),\n",
       " (560, 1),\n",
       " (561, 1),\n",
       " (562, 2),\n",
       " (563, 1),\n",
       " (564, 1),\n",
       " (565, 1),\n",
       " (566, 1),\n",
       " (567, 1),\n",
       " (568, 1),\n",
       " (569, 3),\n",
       " (570, 3),\n",
       " (571, 1),\n",
       " (572, 1),\n",
       " (573, 2)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a1a2bee-17e9-4833-8b58-4a0e04030b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 8644\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f46f8162-271e-4c52-90ca-842d6a990ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000 #chunksize controls how many documents are processed at a time in the training algorithm\n",
    "passes = 20 #passes controls how often we train the model on the entire corpus (like epochs)\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29a36740-b0d6-4e0d-b140-76b89b6655e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1st',\n",
       " 1: '5oo',\n",
       " 2: '7th',\n",
       " 3: 'a2',\n",
       " 4: 'a_well',\n",
       " 5: 'able',\n",
       " 6: 'adapting',\n",
       " 7: 'addition',\n",
       " 8: 'adjust',\n",
       " 9: 'adjusted',\n",
       " 10: 'advanced',\n",
       " 11: 'agency',\n",
       " 12: 'air',\n",
       " 13: 'air_force',\n",
       " 14: 'albus',\n",
       " 15: 'along',\n",
       " 16: 'alter',\n",
       " 17: 'alternately',\n",
       " 18: 'alternative',\n",
       " 19: 'american',\n",
       " 20: 'american_institute',\n",
       " 21: 'amount',\n",
       " 22: 'another',\n",
       " 23: 'appeared',\n",
       " 24: 'april',\n",
       " 25: 'arbitrary',\n",
       " 26: 'architecture',\n",
       " 27: 'area',\n",
       " 28: 'arrow',\n",
       " 29: 'assigned',\n",
       " 30: 'associative',\n",
       " 31: 'assp',\n",
       " 32: 'asymptotic',\n",
       " 33: 'attained',\n",
       " 34: 'attempt',\n",
       " 35: 'august',\n",
       " 36: 'author',\n",
       " 37: 'automatically',\n",
       " 38: 'available',\n",
       " 39: 'averaged',\n",
       " 40: 'b2',\n",
       " 41: 'back',\n",
       " 42: 'back_prop',\n",
       " 43: 'back_propagation',\n",
       " 44: 'because_they',\n",
       " 45: 'before',\n",
       " 46: 'behavior',\n",
       " 47: 'belong',\n",
       " 48: 'best',\n",
       " 49: 'better',\n",
       " 50: 'better_than',\n",
       " 51: 'bin',\n",
       " 52: 'bl',\n",
       " 53: 'book',\n",
       " 54: 'boolean',\n",
       " 55: 'bottom',\n",
       " 56: 'boundary',\n",
       " 57: 'bp',\n",
       " 58: 'brain',\n",
       " 59: 'break',\n",
       " 60: 'burr',\n",
       " 61: 'calculate',\n",
       " 62: 'called',\n",
       " 63: 'capability',\n",
       " 64: 'carefully',\n",
       " 65: 'carlo',\n",
       " 66: 'ce',\n",
       " 67: 'child',\n",
       " 68: 'circular',\n",
       " 69: 'clas',\n",
       " 70: 'class',\n",
       " 71: 'classi',\n",
       " 72: 'classification',\n",
       " 73: 'classified',\n",
       " 74: 'classifier',\n",
       " 75: 'clustering',\n",
       " 76: 'cmac',\n",
       " 77: 'co',\n",
       " 78: 'com',\n",
       " 79: 'combined',\n",
       " 80: 'comparative',\n",
       " 81: 'comparing',\n",
       " 82: 'comparison',\n",
       " 83: 'completely',\n",
       " 84: 'complex',\n",
       " 85: 'computing',\n",
       " 86: 'confer',\n",
       " 87: 'confer_ence',\n",
       " 88: 'conference',\n",
       " 89: 'conference_on',\n",
       " 90: 'connec',\n",
       " 91: 'connected',\n",
       " 92: 'connection',\n",
       " 93: 'considered',\n",
       " 94: 'consists',\n",
       " 95: 'construct',\n",
       " 96: 'constructive',\n",
       " 97: 'contain',\n",
       " 98: 'contains',\n",
       " 99: 'context',\n",
       " 100: 'continuous',\n",
       " 101: 'continuous_valued',\n",
       " 102: 'conventional',\n",
       " 103: 'converge',\n",
       " 104: 'convergence',\n",
       " 105: 'convex',\n",
       " 106: 'cooper',\n",
       " 107: 'correct',\n",
       " 108: 'correlation',\n",
       " 109: 'counting',\n",
       " 110: 'cover',\n",
       " 111: 'covered',\n",
       " 112: 'covering',\n",
       " 113: 'created',\n",
       " 114: 'cross',\n",
       " 115: 'curve',\n",
       " 116: 'cybernetics',\n",
       " 117: 'dashed',\n",
       " 118: 'dashed_line',\n",
       " 119: 'deci',\n",
       " 120: 'decision',\n",
       " 121: 'decision_boundary',\n",
       " 122: 'decision_region',\n",
       " 123: 'defense',\n",
       " 124: 'defense_advanced',\n",
       " 125: 'definition',\n",
       " 126: 'demon',\n",
       " 127: 'demon_strated',\n",
       " 128: 'demonstrate',\n",
       " 129: 'demonstrated',\n",
       " 130: 'density',\n",
       " 131: 'department',\n",
       " 132: 'depends',\n",
       " 133: 'descent',\n",
       " 134: 'described_above',\n",
       " 135: 'designed',\n",
       " 136: 'desired',\n",
       " 137: 'despite',\n",
       " 138: 'detail',\n",
       " 139: 'determine',\n",
       " 140: 'determine_whether',\n",
       " 141: 'determined',\n",
       " 142: 'developed',\n",
       " 143: 'diagram',\n",
       " 144: 'difference_between',\n",
       " 145: 'difficult',\n",
       " 146: 'digit',\n",
       " 147: 'discrete',\n",
       " 148: 'discus',\n",
       " 149: 'disjoint',\n",
       " 150: 'divide',\n",
       " 151: 'do_not',\n",
       " 152: 'domain',\n",
       " 153: 'drawn',\n",
       " 154: 'duda',\n",
       " 155: 'ed',\n",
       " 156: 'eliminate',\n",
       " 157: 'eliminated',\n",
       " 158: 'ence',\n",
       " 159: 'enough',\n",
       " 160: 'entire',\n",
       " 161: 'entry',\n",
       " 162: 'eq',\n",
       " 163: 'equal',\n",
       " 164: 'error_rate',\n",
       " 165: 'establishment',\n",
       " 166: 'estimate',\n",
       " 167: 'eval',\n",
       " 168: 'except',\n",
       " 169: 'excessive',\n",
       " 170: 'exemplar',\n",
       " 171: 'explore',\n",
       " 172: 'explored',\n",
       " 173: 'expressed',\n",
       " 174: 'extend',\n",
       " 175: 'extensively',\n",
       " 176: 'extremely',\n",
       " 177: 'failed',\n",
       " 178: 'fall',\n",
       " 179: 'fall_within',\n",
       " 180: 'faster',\n",
       " 181: 'feature_map',\n",
       " 182: 'fed',\n",
       " 183: 'feedback',\n",
       " 184: 'fewer',\n",
       " 185: 'fewer_than',\n",
       " 186: 'fig',\n",
       " 187: 'fig_show',\n",
       " 188: 'filled',\n",
       " 189: 'filtered',\n",
       " 190: 'final',\n",
       " 191: 'finite',\n",
       " 192: 'fixing',\n",
       " 193: 'flexible',\n",
       " 194: 'force',\n",
       " 195: 'formant',\n",
       " 196: 'formed',\n",
       " 197: 'forming',\n",
       " 198: 'four',\n",
       " 199: 'framework',\n",
       " 200: 'freeman',\n",
       " 201: 'frequency',\n",
       " 202: 'fully',\n",
       " 203: 'furthermore',\n",
       " 204: 'gain',\n",
       " 205: 'gauss',\n",
       " 206: 'gaussian',\n",
       " 207: 'generally',\n",
       " 208: 'generate',\n",
       " 209: 'generated',\n",
       " 210: 'generative',\n",
       " 211: 'geometric',\n",
       " 212: 'gold',\n",
       " 213: 'goldstein',\n",
       " 214: 'good',\n",
       " 215: 'government',\n",
       " 216: 'greatest',\n",
       " 217: 'grid',\n",
       " 218: 'guarantee',\n",
       " 219: 'h2',\n",
       " 220: 'had',\n",
       " 221: 'had_been',\n",
       " 222: 'half',\n",
       " 223: 'hand',\n",
       " 224: 'hard',\n",
       " 225: 'hart',\n",
       " 226: 'hart_pattern',\n",
       " 227: 'head',\n",
       " 228: 'hi',\n",
       " 229: 'hid',\n",
       " 230: 'hidden',\n",
       " 231: 'hidden_layer',\n",
       " 232: 'hidden_markov',\n",
       " 233: 'hill',\n",
       " 234: 'histogram',\n",
       " 235: 'hood',\n",
       " 236: 'huang',\n",
       " 237: 'hypercube',\n",
       " 238: 'hyperplane',\n",
       " 239: 'hyperplanes',\n",
       " 240: 'ieee',\n",
       " 241: 'ieee_assp',\n",
       " 242: 'ieee_transaction',\n",
       " 243: 'illustrate',\n",
       " 244: 'illustrated',\n",
       " 245: 'illustrates',\n",
       " 246: 'illustrative',\n",
       " 247: 'image',\n",
       " 248: 'implement',\n",
       " 249: 'implemented',\n",
       " 250: 'improve',\n",
       " 251: 'improved',\n",
       " 252: 'improvement',\n",
       " 253: 'improvement_over',\n",
       " 254: 'include',\n",
       " 255: 'indicate',\n",
       " 256: 'indicate_that',\n",
       " 257: 'indicated',\n",
       " 258: 'influence',\n",
       " 259: 'ing',\n",
       " 260: 'initialized',\n",
       " 261: 'institute',\n",
       " 262: 'integrator',\n",
       " 263: 'interact',\n",
       " 264: 'interaction',\n",
       " 265: 'interna',\n",
       " 266: 'interna_tional',\n",
       " 267: 'international',\n",
       " 268: 'international_conference',\n",
       " 269: 'inversion',\n",
       " 270: 'involves',\n",
       " 271: 'issue',\n",
       " 272: 'ist',\n",
       " 273: 'john',\n",
       " 274: 'john_wiley',\n",
       " 275: 'july',\n",
       " 276: 'june',\n",
       " 277: 'know',\n",
       " 278: 'kohonen',\n",
       " 279: 'label',\n",
       " 280: 'labeled',\n",
       " 281: 'laboratory',\n",
       " 282: 'larger',\n",
       " 283: 'lay',\n",
       " 284: 'layer',\n",
       " 285: 'layer_perceptron',\n",
       " 286: 'le_than',\n",
       " 287: 'led',\n",
       " 288: 'left',\n",
       " 289: 'likelihood',\n",
       " 290: 'limit',\n",
       " 291: 'limiting',\n",
       " 292: 'lincoln',\n",
       " 293: 'lincoln_laboratory',\n",
       " 294: 'linearly',\n",
       " 295: 'linearly_separable',\n",
       " 296: 'lippmann',\n",
       " 297: 'little',\n",
       " 298: 'long',\n",
       " 299: 'low',\n",
       " 300: 'low_pas',\n",
       " 301: 'ma',\n",
       " 302: 'machine',\n",
       " 303: 'magazine',\n",
       " 304: 'majority',\n",
       " 305: 'man',\n",
       " 306: 'mance',\n",
       " 307: 'map',\n",
       " 308: 'markov',\n",
       " 309: 'maximum',\n",
       " 310: 'maximum_likelihood',\n",
       " 311: 'mcgraw',\n",
       " 312: 'mcgraw_hill',\n",
       " 313: 'meaning',\n",
       " 314: 'mechanism',\n",
       " 315: 'mem',\n",
       " 316: 'men',\n",
       " 317: 'mentioned',\n",
       " 318: 'mentioned_above',\n",
       " 319: 'might',\n",
       " 320: 'minor',\n",
       " 321: 'mit',\n",
       " 322: 'ml',\n",
       " 323: 'modified',\n",
       " 324: 'momentum',\n",
       " 325: 'monte',\n",
       " 326: 'monte_carlo',\n",
       " 327: 'more_general',\n",
       " 328: 'more_than',\n",
       " 329: 'multi',\n",
       " 330: 'multi_layer',\n",
       " 331: 'multiple',\n",
       " 332: 'must_be',\n",
       " 333: 'native',\n",
       " 334: 'nd',\n",
       " 335: 'near',\n",
       " 336: 'near_zero',\n",
       " 337: 'nearest',\n",
       " 338: 'nearest_neighbor',\n",
       " 339: 'necessary',\n",
       " 340: 'needed',\n",
       " 341: 'neighbor',\n",
       " 342: 'net',\n",
       " 343: 'neural_net',\n",
       " 344: 'never',\n",
       " 345: 'new_york',\n",
       " 346: 'ni',\n",
       " 347: 'nine',\n",
       " 348: 'nique',\n",
       " 349: 'node',\n",
       " 350: 'non_zero',\n",
       " 351: 'nonlinearities',\n",
       " 352: 'noted',\n",
       " 353: 'november',\n",
       " 354: 'observation',\n",
       " 355: 'obtain',\n",
       " 356: 'occurrence',\n",
       " 357: 'offset',\n",
       " 358: 'often',\n",
       " 359: 'op',\n",
       " 360: 'organization',\n",
       " 361: 'organizing',\n",
       " 362: 'outcome',\n",
       " 363: 'overall',\n",
       " 364: 'page',\n",
       " 365: 'parallel',\n",
       " 366: 'pas',\n",
       " 367: 'pattern_recognition',\n",
       " 368: 'pe',\n",
       " 369: 'per',\n",
       " 370: 'percent',\n",
       " 371: 'percentage',\n",
       " 372: 'perceptron',\n",
       " 373: 'perceptrons',\n",
       " 374: 'perceptton',\n",
       " 375: 'percepttons',\n",
       " 376: 'perfor',\n",
       " 377: 'perfor_mance',\n",
       " 378: 'perform',\n",
       " 379: 'performed',\n",
       " 380: 'peterson',\n",
       " 381: 'physic',\n",
       " 382: 'place',\n",
       " 383: 'placement',\n",
       " 384: 'plane',\n",
       " 385: 'policy',\n",
       " 386: 'poor',\n",
       " 387: 'position',\n",
       " 388: 'pp',\n",
       " 389: 'previous',\n",
       " 390: 'previous_work',\n",
       " 391: 'probability_density',\n",
       " 392: 'procedure',\n",
       " 393: 'proceeding',\n",
       " 394: 'project',\n",
       " 395: 'proof',\n",
       " 396: 'prop',\n",
       " 397: 'propagation',\n",
       " 398: 'proportional',\n",
       " 399: 'provide',\n",
       " 400: 'provided',\n",
       " 401: 'provides',\n",
       " 402: 'proving',\n",
       " 403: 'put',\n",
       " 404: 'quantity',\n",
       " 405: 'ra',\n",
       " 406: 'radar',\n",
       " 407: 'randomly',\n",
       " 408: 'rapid',\n",
       " 409: 'rapidly',\n",
       " 410: 'recent',\n",
       " 411: 'recent_work',\n",
       " 412: 'recently',\n",
       " 413: 'recognition',\n",
       " 414: 'recognizer',\n",
       " 415: 'reduce',\n",
       " 416: 'reflect',\n",
       " 417: 'region',\n",
       " 418: 'remain',\n",
       " 419: 'remaining',\n",
       " 420: 'removed',\n",
       " 421: 'reported',\n",
       " 422: 'represent',\n",
       " 423: 'represents',\n",
       " 424: 'required',\n",
       " 425: 'research_project',\n",
       " 426: 'respectively',\n",
       " 427: 'rest',\n",
       " 428: 'restricted',\n",
       " 429: 'resulting',\n",
       " 430: 'richard',\n",
       " 431: 'richard_lippmann',\n",
       " 432: 'right',\n",
       " 433: 'robotics',\n",
       " 434: 'robust',\n",
       " 435: 'royal',\n",
       " 436: 'run',\n",
       " 437: 'sample',\n",
       " 438: 'scene',\n",
       " 439: 'scene_analysis',\n",
       " 440: 'see_fig',\n",
       " 441: 'seen',\n",
       " 442: 'select',\n",
       " 443: 'selected',\n",
       " 444: 'selecting',\n",
       " 445: 'self',\n",
       " 446: 'self_organization',\n",
       " 447: 'self_organizing',\n",
       " 448: 'sensitivity',\n",
       " 449: 'separability',\n",
       " 450: 'separable',\n",
       " 451: 'separate',\n",
       " 452: 'separated',\n",
       " 453: 'separating',\n",
       " 454: 'shaded',\n",
       " 455: 'shape',\n",
       " 456: 'shorter',\n",
       " 457: 'show_how',\n",
       " 458: 'sigmoidal',\n",
       " 459: 'sign',\n",
       " 460: 'signal',\n",
       " 461: 'significant',\n",
       " 462: 'simpler',\n",
       " 463: 'simply',\n",
       " 464: 'simula',\n",
       " 465: 'simulation',\n",
       " 466: 'sion',\n",
       " 467: 'situation',\n",
       " 468: 'slightly',\n",
       " 469: 'slope',\n",
       " 470: 'slow',\n",
       " 471: 'slower',\n",
       " 472: 'slower_than',\n",
       " 473: 'smooth',\n",
       " 474: 'smoothly',\n",
       " 475: 'solid',\n",
       " 476: 'solid_line',\n",
       " 477: 'solution',\n",
       " 478: 'sometimes',\n",
       " 479: 'son',\n",
       " 480: 'son_new',\n",
       " 481: 'specific',\n",
       " 482: 'speech',\n",
       " 483: 'speech_recognition',\n",
       " 484: 'split',\n",
       " 485: 'split_into',\n",
       " 486: 'spoken',\n",
       " 487: 'sponsored',\n",
       " 488: 'sponsored_by',\n",
       " 489: 'square',\n",
       " 490: 'stage',\n",
       " 491: 'steady',\n",
       " 492: 'steady_state',\n",
       " 493: 'stored',\n",
       " 494: 'strated',\n",
       " 495: 'strongest',\n",
       " 496: 'studied',\n",
       " 497: 'substantially',\n",
       " 498: 'suggest',\n",
       " 499: 'suggest_that',\n",
       " 500: 'suggests',\n",
       " 501: 'summarizes',\n",
       " 502: 'summing',\n",
       " 503: 'supervised',\n",
       " 504: 'supervised_learning',\n",
       " 505: 'supervision',\n",
       " 506: 'symbol',\n",
       " 507: 'system_man',\n",
       " 508: 'table',\n",
       " 509: 'tech',\n",
       " 510: 'tech_nique',\n",
       " 511: 'ten',\n",
       " 512: 'tested',\n",
       " 513: 'testing',\n",
       " 514: 'thick',\n",
       " 515: 'third',\n",
       " 516: 'tially',\n",
       " 517: 'tional',\n",
       " 518: 'tions',\n",
       " 519: 'together',\n",
       " 520: 'token',\n",
       " 521: 'top',\n",
       " 522: 'total',\n",
       " 523: 'traditional',\n",
       " 524: 'train',\n",
       " 525: 'train_ing',\n",
       " 526: 'trainable',\n",
       " 527: 'trained',\n",
       " 528: 'transaction',\n",
       " 529: 'trial',\n",
       " 530: 'triangular',\n",
       " 531: 'twice',\n",
       " 532: 'typically',\n",
       " 533: 'u',\n",
       " 534: 'unchanged',\n",
       " 535: 'understanding',\n",
       " 536: 'uniform',\n",
       " 537: 'uniform_distribution',\n",
       " 538: 'unlabeled',\n",
       " 539: 'unsupervised',\n",
       " 540: 'until',\n",
       " 541: 'untrained',\n",
       " 542: 'upper',\n",
       " 543: 'us',\n",
       " 544: 'usa',\n",
       " 545: 'usa_abstract',\n",
       " 546: 'useful',\n",
       " 547: 'v',\n",
       " 548: 'valued',\n",
       " 549: 'varies',\n",
       " 550: 'versa',\n",
       " 551: 'vice',\n",
       " 552: 'vice_versa',\n",
       " 553: 'view',\n",
       " 554: 'voice',\n",
       " 555: 'vol',\n",
       " 556: 'vol_pp',\n",
       " 557: 'vowel',\n",
       " 558: 'wa_tested',\n",
       " 559: 'washington',\n",
       " 560: 'were_generated',\n",
       " 561: 'whereas',\n",
       " 562: 'whether',\n",
       " 563: 'who',\n",
       " 564: 'wiley',\n",
       " 565: 'william',\n",
       " 566: 'work_wa',\n",
       " 567: 'worse',\n",
       " 568: 'worse_than',\n",
       " 569: 'x2',\n",
       " 570: 'xl',\n",
       " 571: 'xl_x2',\n",
       " 572: 'yl',\n",
       " 573: 'york',\n",
       " 574: '2n',\n",
       " 575: '_c',\n",
       " 576: 'a_follows',\n",
       " 577: 'ability',\n",
       " 578: 'abu',\n",
       " 579: 'abu_mostafa',\n",
       " 580: 'access',\n",
       " 581: 'accommodate',\n",
       " 582: 'according',\n",
       " 583: 'according_to',\n",
       " 584: 'accumulated',\n",
       " 585: 'acknowledgement',\n",
       " 586: 'acknowledgement_this',\n",
       " 587: 'addison',\n",
       " 588: 'addison_wesley',\n",
       " 589: 'afosr',\n",
       " 590: 'aip',\n",
       " 591: 'all_possible',\n",
       " 592: 'although',\n",
       " 593: 'an_arbitrary',\n",
       " 594: 'analog',\n",
       " 595: 'anyway',\n",
       " 596: 'ao',\n",
       " 597: 'appears',\n",
       " 598: 'appendix',\n",
       " 599: 'approximately',\n",
       " 600: 'are_interested',\n",
       " 601: 'arise',\n",
       " 602: 'aspect',\n",
       " 603: 'assume',\n",
       " 604: 'assumed',\n",
       " 605: 'assumption',\n",
       " 606: 'automaton',\n",
       " 607: 'away',\n",
       " 608: 'axe',\n",
       " 609: 'basic',\n",
       " 610: 'become',\n",
       " 611: 'becomes',\n",
       " 612: 'benefit',\n",
       " 613: 'ber',\n",
       " 614: 'big',\n",
       " 615: 'binary',\n",
       " 616: 'biological',\n",
       " 617: 'bit',\n",
       " 618: 'bit_per',\n",
       " 619: 'boolean_function',\n",
       " 620: 'bound',\n",
       " 621: 'bt',\n",
       " 622: 'ca',\n",
       " 623: 'ca_abstract',\n",
       " 624: 'california',\n",
       " 625: 'california_institute',\n",
       " 626: 'cannot',\n",
       " 627: 'capable',\n",
       " 628: 'carried',\n",
       " 629: 'ceedings',\n",
       " 630: 'cell',\n",
       " 631: 'choosing',\n",
       " 632: 'circuit',\n",
       " 633: 'clearly',\n",
       " 634: 'collective',\n",
       " 635: 'compare',\n",
       " 636: 'complete',\n",
       " 637: 'complexity',\n",
       " 638: 'concept',\n",
       " 639: 'conclude',\n",
       " 640: 'conclude_that',\n",
       " 641: 'connectivity',\n",
       " 642: 'consequence',\n",
       " 643: 'contribution',\n",
       " 644: 'convenience',\n",
       " 645: 'coordinate',\n",
       " 646: 'corresponds',\n",
       " 647: 'corresponds_to',\n",
       " 648: 'define',\n",
       " 649: 'defines',\n",
       " 650: 'denker',\n",
       " 651: 'denote',\n",
       " 652: 'depend',\n",
       " 653: 'depend_on',\n",
       " 654: 'depending',\n",
       " 655: 'depending_on',\n",
       " 656: 'depends_on',\n",
       " 657: 'describes',\n",
       " 658: 'diagonal',\n",
       " 659: 'directly',\n",
       " 660: 'discussing',\n",
       " 661: 'disorder',\n",
       " 662: 'distinguishing',\n",
       " 663: 'doing',\n",
       " 664: 'drawn_from',\n",
       " 665: 'e',\n",
       " 666: 'ea',\n",
       " 667: 'easy',\n",
       " 668: 'edge',\n",
       " 669: 'el',\n",
       " 670: 'element',\n",
       " 671: 'en',\n",
       " 672: 'end',\n",
       " 673: 'ensemble',\n",
       " 674: 'entropy',\n",
       " 675: 'environment',\n",
       " 676: 'equiv',\n",
       " 677: 'equivalent',\n",
       " 678: 'essentially',\n",
       " 679: 'evaluate',\n",
       " 680: 'eventually',\n",
       " 681: 'everything',\n",
       " 682: 'expand',\n",
       " 683: 'expected',\n",
       " 684: 'expected_value',\n",
       " 685: 'exposition',\n",
       " 686: 'expression',\n",
       " 687: 'extraction',\n",
       " 688: 'extreme',\n",
       " 689: 'fact',\n",
       " 690: 'fact_that',\n",
       " 691: 'finite_automaton',\n",
       " 692: 'fir',\n",
       " 693: 'follows',\n",
       " 694: 'formal',\n",
       " 695: 'gate',\n",
       " 696: 'generated_by',\n",
       " 697: 'generating',\n",
       " 698: 'get',\n",
       " 699: 'getting',\n",
       " 700: 'global',\n",
       " 701: 'go',\n",
       " 702: 'go_through',\n",
       " 703: 'going',\n",
       " 704: 'gradually',\n",
       " 705: 'grant',\n",
       " 706: 'graph',\n",
       " 707: 'handle',\n",
       " 708: 'he',\n",
       " 709: 'hence',\n",
       " 710: 'her',\n",
       " 711: 'hold',\n",
       " 712: 'huge',\n",
       " 713: 'idea',\n",
       " 714: 'ieee_trans',\n",
       " 715: 'imple',\n",
       " 716: 'imposes',\n",
       " 717: 'indeed',\n",
       " 718: 'independent',\n",
       " 719: 'independently',\n",
       " 720: 'infinity',\n",
       " 721: 'infor',\n",
       " 722: 'information_about',\n",
       " 723: 'interested',\n",
       " 724: 'internal',\n",
       " 725: 'introduce',\n",
       " 726: 'involved',\n",
       " 727: 'it_own',\n",
       " 728: 'itself',\n",
       " 729: 'ity',\n",
       " 730: 'jl',\n",
       " 731: 'jr',\n",
       " 732: 'just',\n",
       " 733: 'ki',\n",
       " 734: 'kind',\n",
       " 735: 'knowledge',\n",
       " 736: 'last',\n",
       " 737: 'learn',\n",
       " 738: 'learned',\n",
       " 739: 'learning_rule',\n",
       " 740: 'learns',\n",
       " 741: 'length',\n",
       " 742: 'let',\n",
       " 743: 'let_denote',\n",
       " 744: 'li',\n",
       " 745: 'likely',\n",
       " 746: 'lim',\n",
       " 747: 'loaded',\n",
       " 748: 'log',\n",
       " 749: 'lower',\n",
       " 750: 'lower_bound',\n",
       " 751: 'main',\n",
       " 752: 'main_result',\n",
       " 753: 'mead',\n",
       " 754: 'measure',\n",
       " 755: 'measured',\n",
       " 756: 'merely',\n",
       " 757: 'mostafa',\n",
       " 758: 'much_smaller',\n",
       " 759: 'n2',\n",
       " 760: 'need',\n",
       " 761: 'neither',\n",
       " 762: 'neuron',\n",
       " 763: 'next',\n",
       " 764: 'next_section',\n",
       " 765: 'nl',\n",
       " 766: 'nor',\n",
       " 767: 'normalized',\n",
       " 768: 'notation',\n",
       " 769: 'num',\n",
       " 770: 'num_ber',\n",
       " 771: 'o0',\n",
       " 772: 'o1',\n",
       " 773: 'object',\n",
       " 774: 'obvious',\n",
       " 775: 'occur',\n",
       " 776: 'off',\n",
       " 777: 'off_diagonal',\n",
       " 778: 'office',\n",
       " 779: 'once',\n",
       " 780: 'oo',\n",
       " 781: 'opposite',\n",
       " 782: 'other_word',\n",
       " 783: 'otherwise',\n",
       " 784: 'own',\n",
       " 785: 'pasadena',\n",
       " 786: 'perhaps',\n",
       " 787: 'picture',\n",
       " 788: 'pixel',\n",
       " 789: 'plausible',\n",
       " 790: 'powerful',\n",
       " 791: 'pr',\n",
       " 792: 'predicts',\n",
       " 793: 'principle',\n",
       " 794: 'pro',\n",
       " 795: 'pro_ceedings',\n",
       " 796: 'probability_distribution',\n",
       " 797: 'produce',\n",
       " 798: 'program',\n",
       " 799: 'projection',\n",
       " 800: 'prove',\n",
       " 801: 'pt',\n",
       " 802: 'putting',\n",
       " 803: 'quantitative',\n",
       " 804: 'r2',\n",
       " 805: 'random_variable',\n",
       " 806: 'randomness',\n",
       " 807: 'range',\n",
       " 808: 'rather',\n",
       " 809: 'rather_than',\n",
       " 810: 'ready',\n",
       " 811: 'recall',\n",
       " 812: 'regardless',\n",
       " 813: 'relate',\n",
       " 814: 'relation',\n",
       " 815: 'relative',\n",
       " 816: 'relative_frequency',\n",
       " 817: 'replacement',\n",
       " 818: 'report',\n",
       " 819: 'represented',\n",
       " 820: 'represented_by',\n",
       " 821: 'respect',\n",
       " 822: 'restrict',\n",
       " 823: 'restriction',\n",
       " 824: 'rh',\n",
       " 825: 'roughly',\n",
       " 826: 'roughly_speaking',\n",
       " 827: 'rule',\n",
       " 828: 'say',\n",
       " 829: 'scientific',\n",
       " 830: 'scientific_research',\n",
       " 831: 'search',\n",
       " 832: 'shall',\n",
       " 833: 'simulate',\n",
       " 834: 'smaller',\n",
       " 835: 'something',\n",
       " 836: 'sophisticated',\n",
       " 837: 'speaking',\n",
       " 838: 'specifies',\n",
       " 839: 'specify',\n",
       " 840: 'start',\n",
       " 841: 'starting',\n",
       " 842: 'statistic',\n",
       " 843: 'statistical',\n",
       " 844: 'statistically',\n",
       " 845: 'still',\n",
       " 846: 'string',\n",
       " 847: 'strong',\n",
       " 848: 'subset',\n",
       " 849: 'substituting',\n",
       " 850: 'sum',\n",
       " 851: 'supported',\n",
       " 852: 'supported_by',\n",
       " 853: 'suppose',\n",
       " 854: 'supposed',\n",
       " 855: 'switching',\n",
       " 856: 'sx',\n",
       " 857: 'symmetry',\n",
       " 858: 'synapse',\n",
       " 859: 'synapsis',\n",
       " 860: 'taken',\n",
       " 861: 'technical',\n",
       " 862: 'technology',\n",
       " 863: 'technology_pasadena',\n",
       " 864: 'tell',\n",
       " 865: 'them',\n",
       " 866: 'themselves',\n",
       " 867: 'theorem',\n",
       " 868: 'tive',\n",
       " 869: 'together_with',\n",
       " 870: 'trans',\n",
       " 871: 'tune',\n",
       " 872: 'ues',\n",
       " 873: 'under_grant',\n",
       " 874: 'undirected',\n",
       " 875: 'upper_bound',\n",
       " 876: 'v2',\n",
       " 877: 'va',\n",
       " 878: 'val',\n",
       " 879: 'val_ues',\n",
       " 880: 'variance',\n",
       " 881: 'variation',\n",
       " 882: 'version',\n",
       " 883: 'versus',\n",
       " 884: 'vertex',\n",
       " 885: 'very_low',\n",
       " 886: 'visual',\n",
       " 887: 'visual_scene',\n",
       " 888: 'vl',\n",
       " 889: 'we_define',\n",
       " 890: 'we_shall',\n",
       " 891: 'wesley',\n",
       " 892: 'what',\n",
       " 893: 'with_respect',\n",
       " 894: 'word',\n",
       " 895: 'wr',\n",
       " 896: 'write',\n",
       " 897: 'written',\n",
       " 898: 'written_a',\n",
       " 899: 'xi',\n",
       " 900: 'xn',\n",
       " 901: 'ya',\n",
       " 902: 'you',\n",
       " 903: 'yx',\n",
       " 904: 'zl',\n",
       " 905: '2e',\n",
       " 906: '2k',\n",
       " 907: 'absence',\n",
       " 908: 'absolute',\n",
       " 909: 'acad',\n",
       " 910: 'acad_sci',\n",
       " 911: 'achieve',\n",
       " 912: 'ackley',\n",
       " 913: 'ackley_hinton',\n",
       " 914: 'acknowledgment',\n",
       " 915: 'across',\n",
       " 916: 'activation',\n",
       " 917: 'activation_function',\n",
       " 918: 'adaptive',\n",
       " 919: 'add',\n",
       " 920: 'added',\n",
       " 921: 'adding',\n",
       " 922: 'address',\n",
       " 923: 'adjacent',\n",
       " 924: 'adjusting',\n",
       " 925: 'adjustment',\n",
       " 926: 'adopted',\n",
       " 927: 'advance',\n",
       " 928: 'advanced_research',\n",
       " 929: 'allen',\n",
       " 930: 'allows',\n",
       " 931: 'almost',\n",
       " 932: 'alspector',\n",
       " 933: 'always',\n",
       " 934: 'am',\n",
       " 935: 'amherst',\n",
       " 936: 'amplifier',\n",
       " 937: 'an_important',\n",
       " 938: 'analog_circuit',\n",
       " 939: 'analog_cmos',\n",
       " 940: 'anderson',\n",
       " 941: 'annealing',\n",
       " 942: 'annealing_schedule',\n",
       " 943: 'answer',\n",
       " 944: 'approximation',\n",
       " 945: 'arranged',\n",
       " 946: 'arrangement',\n",
       " 947: 'array',\n",
       " 948: 'artificial',\n",
       " 949: 'artificial_neural',\n",
       " 950: 'aside',\n",
       " 951: 'assignment',\n",
       " 952: 'associative_memory',\n",
       " 953: 'assure',\n",
       " 954: 'asynchronous',\n",
       " 955: 'asynchronously',\n",
       " 956: 'ation',\n",
       " 957: 'attached',\n",
       " 958: 'attack',\n",
       " 959: 'average_over',\n",
       " 960: 'averaged_over',\n",
       " 961: 'avoid',\n",
       " 962: 'avoiding',\n",
       " 963: 'ay',\n",
       " 964: 'bandwidth',\n",
       " 965: 'barto',\n",
       " 966: 'barto_sutton',\n",
       " 967: 'basis',\n",
       " 968: 'believe',\n",
       " 969: 'bell',\n",
       " 970: 'below',\n",
       " 971: 'berkeley',\n",
       " 972: 'berkeley_ca',\n",
       " 973: 'bi',\n",
       " 974: 'biologically',\n",
       " 975: 'bipolar',\n",
       " 976: 'block',\n",
       " 977: 'block_diagram',\n",
       " 978: 'boltzmann',\n",
       " 979: 'boltzmann_machine',\n",
       " 980: 'branch',\n",
       " 981: 'cambridge',\n",
       " 982: 'cambridge_ma',\n",
       " 983: 'capacitance',\n",
       " 984: 'capacitor',\n",
       " 985: 'ccd',\n",
       " 986: 'ch',\n",
       " 987: 'chance',\n",
       " 988: 'channel',\n",
       " 989: 'characteristic',\n",
       " 990: 'characterize',\n",
       " 991: 'characterized',\n",
       " 992: 'charge',\n",
       " 993: 'chip',\n",
       " 994: 'choice',\n",
       " 995: 'chose',\n",
       " 996: 'chosen',\n",
       " 997: 'cij',\n",
       " 998: 'clamping',\n",
       " 999: 'close',\n",
       " ...}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8a2892f-ca70-4130-843f-993f949ba7e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.1230.\n",
      "[([(0.012668772, 'hidden'),\n",
      "   (0.008279954, 'hidden_unit'),\n",
      "   (0.008079696, 'layer'),\n",
      "   (0.005334097, 'rule'),\n",
      "   (0.005044605, 'net'),\n",
      "   (0.004333512, 'code'),\n",
      "   (0.0041409824, 'propagation'),\n",
      "   (0.003919791, 'gradient'),\n",
      "   (0.0038634592, 'solution'),\n",
      "   (0.0036693944, 'back'),\n",
      "   (0.0034518326, 'prediction'),\n",
      "   (0.003405478, 'cost'),\n",
      "   (0.0033953947, 'training_set'),\n",
      "   (0.0031170342, 'minimum'),\n",
      "   (0.0029447128, 'back_propagation'),\n",
      "   (0.0028661392, 'trained'),\n",
      "   (0.0027028725, 'table'),\n",
      "   (0.0026447475, 'connection'),\n",
      "   (0.0026163682, 'region'),\n",
      "   (0.0025777011, 'procedure')],\n",
      "  -0.891225547427413),\n",
      " ([(0.017644314, 'cell'),\n",
      "   (0.016823415, 'neuron'),\n",
      "   (0.008924995, 'response'),\n",
      "   (0.008490273, 'stimulus'),\n",
      "   (0.0070334687, 'spike'),\n",
      "   (0.0067016305, 'activity'),\n",
      "   (0.0064425184, 'visual'),\n",
      "   (0.005569585, 'synaptic'),\n",
      "   (0.00526994, 'firing'),\n",
      "   (0.0051779062, 'frequency'),\n",
      "   (0.0051573347, 'signal'),\n",
      "   (0.0048824996, 'cortex'),\n",
      "   (0.004798088, 'field'),\n",
      "   (0.004660823, 'motion'),\n",
      "   (0.0044160923, 'direction'),\n",
      "   (0.0040770127, 'connection'),\n",
      "   (0.0039126663, 'cortical'),\n",
      "   (0.0038584166, 'layer'),\n",
      "   (0.0037213406, 'orientation'),\n",
      "   (0.003643883, 'temporal')],\n",
      "  -0.9207250026999012),\n",
      " ([(0.0065361816, 'bound'),\n",
      "   (0.006332611, 'generalization'),\n",
      "   (0.0060881707, 'let'),\n",
      "   (0.0054215016, 'theorem'),\n",
      "   (0.0046123564, 'class'),\n",
      "   (0.004120238, 'approximation'),\n",
      "   (0.00404782, 'dimension'),\n",
      "   (0.0038903889, 'optimal'),\n",
      "   (0.003798379, 'matrix'),\n",
      "   (0.0036908982, 'threshold'),\n",
      "   (0.003590504, 'net'),\n",
      "   (0.0034835842, 'layer'),\n",
      "   (0.0031905794, 'proof'),\n",
      "   (0.003132586, 'noise'),\n",
      "   (0.0030428974, 'hidden'),\n",
      "   (0.002995076, 'polynomial'),\n",
      "   (0.0029552383, 'sample'),\n",
      "   (0.0029363758, 'complexity'),\n",
      "   (0.0028455267, 'xi'),\n",
      "   (0.0025662764, 'convergence')],\n",
      "  -1.0176189964435114),\n",
      " ([(0.007666773, 'gaussian'),\n",
      "   (0.0063503874, 'density'),\n",
      "   (0.006263258, 'mixture'),\n",
      "   (0.0058921888, 'estimate'),\n",
      "   (0.0057350965, 'matrix'),\n",
      "   (0.0056005726, 'likelihood'),\n",
      "   (0.005271599, 'prior'),\n",
      "   (0.0046650143, 'bayesian'),\n",
      "   (0.0046078013, 'component'),\n",
      "   (0.0045593265, 'sample'),\n",
      "   (0.0045157615, 'log'),\n",
      "   (0.0044996585, 'approximation'),\n",
      "   (0.004164531, 'kernel'),\n",
      "   (0.0041531445, 'noise'),\n",
      "   (0.0040843613, 'em'),\n",
      "   (0.004048968, 'estimation'),\n",
      "   (0.0038895295, 'variance'),\n",
      "   (0.0038441373, 'posterior'),\n",
      "   (0.003646367, 'regression'),\n",
      "   (0.003138778, 'xi')],\n",
      "  -1.0535908174186477),\n",
      " ([(0.031664707, 'image'),\n",
      "   (0.013056401, 'object'),\n",
      "   (0.009240887, 'map'),\n",
      "   (0.0076773735, 'recognition'),\n",
      "   (0.006265426, 'pixel'),\n",
      "   (0.00605521, 'visual'),\n",
      "   (0.0060098316, 'face'),\n",
      "   (0.0048135933, 'position'),\n",
      "   (0.004566354, 'view'),\n",
      "   (0.004350304, 'region'),\n",
      "   (0.0040144427, 'field'),\n",
      "   (0.003979997, 'distance'),\n",
      "   (0.003977936, 'location'),\n",
      "   (0.0034440013, 'layer'),\n",
      "   (0.0034189075, 'scale'),\n",
      "   (0.003378447, 'vision'),\n",
      "   (0.0032364982, 'human'),\n",
      "   (0.0031373333, 'scene'),\n",
      "   (0.0030469848, 'digit'),\n",
      "   (0.002861236, 'hand')],\n",
      "  -1.09605251977257),\n",
      " ([(0.013432095, 'circuit'),\n",
      "   (0.012387006, 'neuron'),\n",
      "   (0.010637215, 'chip'),\n",
      "   (0.010258474, 'analog'),\n",
      "   (0.00861632, 'memory'),\n",
      "   (0.006334821, 'voltage'),\n",
      "   (0.0051399935, 'bit'),\n",
      "   (0.00496942, 'vlsi'),\n",
      "   (0.004881687, 'implementation'),\n",
      "   (0.004755902, 'signal'),\n",
      "   (0.0038902469, 'processor'),\n",
      "   (0.003674586, 'connection'),\n",
      "   (0.0035915517, 'parallel'),\n",
      "   (0.0034495047, 'element'),\n",
      "   (0.0033580728, 'hopfield'),\n",
      "   (0.0033493054, 'operation'),\n",
      "   (0.00328342, 'design'),\n",
      "   (0.0032391453, 'block'),\n",
      "   (0.0032001857, 'gate'),\n",
      "   (0.0031531982, 'digital')],\n",
      "  -1.1457382978956148),\n",
      " ([(0.01184556, 'action'),\n",
      "   (0.009226922, 'policy'),\n",
      "   (0.007796992, 'reinforcement'),\n",
      "   (0.0066340882, 'optimal'),\n",
      "   (0.0064885463, 'control'),\n",
      "   (0.0056123664, 'dynamic'),\n",
      "   (0.0052467706, 'reinforcement_learning'),\n",
      "   (0.0045332564, 'gradient'),\n",
      "   (0.004375876, 'stochastic'),\n",
      "   (0.0042217826, 'reward'),\n",
      "   (0.0037786684, 'convergence'),\n",
      "   (0.0037103791, 'environment'),\n",
      "   (0.0035609172, 'goal'),\n",
      "   (0.0035395825, 'robot'),\n",
      "   (0.0035163364, 'transition'),\n",
      "   (0.0032913089, 'iteration'),\n",
      "   (0.003289984, 'decision'),\n",
      "   (0.0032657962, 'cost'),\n",
      "   (0.003142782, 'update'),\n",
      "   (0.00308174, 'td')],\n",
      "  -1.1664658094386342),\n",
      " ([(0.014357646, 'control'),\n",
      "   (0.010382426, 'dynamic'),\n",
      "   (0.008903001, 'signal'),\n",
      "   (0.0085054245, 'motor'),\n",
      "   (0.0069029154, 'movement'),\n",
      "   (0.006779539, 'trajectory'),\n",
      "   (0.0058801137, 'attractor'),\n",
      "   (0.0052021146, 'controller'),\n",
      "   (0.0048489734, 'delay'),\n",
      "   (0.004719018, 'feedback'),\n",
      "   (0.0044410345, 'forward'),\n",
      "   (0.0042787604, 'module'),\n",
      "   (0.004267863, 'behavior'),\n",
      "   (0.004199431, 'phase'),\n",
      "   (0.0041796416, 'target'),\n",
      "   (0.0037498646, 'component'),\n",
      "   (0.0036731292, 'subject'),\n",
      "   (0.0036688978, 'position'),\n",
      "   (0.0036582877, 'memory'),\n",
      "   (0.003626427, 'source')],\n",
      "  -1.245817517943417),\n",
      " ([(0.013394904, 'speech'),\n",
      "   (0.012913454, 'word'),\n",
      "   (0.012855022, 'recognition'),\n",
      "   (0.0073551754, 'sequence'),\n",
      "   (0.0072521763, 'hidden'),\n",
      "   (0.006559059, 'layer'),\n",
      "   (0.006378361, 'context'),\n",
      "   (0.006339411, 'net'),\n",
      "   (0.005594281, 'trained'),\n",
      "   (0.0052703214, 'architecture'),\n",
      "   (0.0052181673, 'character'),\n",
      "   (0.0050931056, 'class'),\n",
      "   (0.0048503634, 'speaker'),\n",
      "   (0.0046005584, 'hmm'),\n",
      "   (0.0043448936, 'rule'),\n",
      "   (0.0038672823, 'signal'),\n",
      "   (0.0037955923, 'language'),\n",
      "   (0.003705389, 'classification'),\n",
      "   (0.0034476027, 'phoneme'),\n",
      "   (0.003435844, 'letter')],\n",
      "  -1.2572255393488478),\n",
      " ([(0.018447142, 'node'),\n",
      "   (0.013101394, 'tree'),\n",
      "   (0.008646178, 'classifier'),\n",
      "   (0.008258885, 'class'),\n",
      "   (0.006400867, 'classification'),\n",
      "   (0.005877317, 'rule'),\n",
      "   (0.005616282, 'decision'),\n",
      "   (0.003969245, 'memory'),\n",
      "   (0.0038669268, 'instance'),\n",
      "   (0.0033376415, 'concept'),\n",
      "   (0.0031992649, 'graph'),\n",
      "   (0.003126729, 'target'),\n",
      "   (0.002980934, 'search'),\n",
      "   (0.002912792, 'activation'),\n",
      "   (0.0026930242, 'hypothesis'),\n",
      "   (0.00264749, 'neighbor'),\n",
      "   (0.0026182411, 'connectionist'),\n",
      "   (0.0025648773, 'similarity'),\n",
      "   (0.002506681, 'role'),\n",
      "   (0.0024281044, 'nearest')],\n",
      "  -1.4351544643101617)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996b728-3db7-4fdf-b3de-0f0ff3085319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d3bff45-3c09-4b0b-b879-854aeca835d9",
   "metadata": {},
   "source": [
    "# news data LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60d7c36c-3a69-4140-9431-6ed7607d8d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/clarinsi/classla-resources/main/resources_1.0.2.json: 10.5kB [00:00, 1.05MB/s]\n",
      "2022-10-02 16:54:32 INFO: Downloading these customized packages for language: sl (Slovenian)...\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | standard |\n",
      "| pos       | standard |\n",
      "| lemma     | standard |\n",
      "| depparse  | standard |\n",
      "| ner       | standard |\n",
      "| pretrain  | standard |\n",
      "========================\n",
      "\n",
      "2022-10-02 16:54:34 INFO: File exists: C:\\Users\\hladn\\classla_resources\\sl\\pos\\standard.pt.\n",
      "2022-10-02 16:54:34 INFO: File exists: C:\\Users\\hladn\\classla_resources\\sl\\lemma\\standard.pt.\n",
      "2022-10-02 16:54:35 INFO: File exists: C:\\Users\\hladn\\classla_resources\\sl\\depparse\\standard.pt.\n",
      "2022-10-02 16:54:35 INFO: File exists: C:\\Users\\hladn\\classla_resources\\sl\\ner\\standard.pt.\n",
      "2022-10-02 16:54:35 INFO: File exists: C:\\Users\\hladn\\classla_resources\\sl\\pretrain\\standard.pt.\n",
      "2022-10-02 16:54:35 INFO: Finished downloading models and saved to C:\\Users\\hladn\\classla_resources.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hladn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hladn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hladn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "##from sklearn.manifold import TSNE\n",
    "#from sklearn.manifold import MDS\n",
    "#from sklearn.decomposition import PCA\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import classla\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "classla.download('sl')        # download non-standard models for Slovenian, use hr for Croatian and sr for Serbian\n",
    "#classla.download('sl', type='nonstandard')        # download non-standard models for Slovenian, use hr for Croatian and sr for Serbian\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef7c96-08cf-4c12-b0dc-492288ba1c29",
   "metadata": {},
   "source": [
    "### select data that we will work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc8b1a96-5347-4f0a-9447-9db9339ca09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_parquet('C:/Users/hladn/FAKS/Magistrsko delo/data/eventregistry/df_news_2020.parquet.gzip')\n",
    "\n",
    "df = df_all[df_all['media']=='MMC RTV Slovenija']\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "daca638c-1d52-4f83-a9a6-80b18e4f8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4ef6b-5e04-47e0-b28e-a58d82cd669f",
   "metadata": {},
   "source": [
    "### set stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f90faa86-c3b0-44f9-8518-fff696f17a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2044"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'C:/Users/hladn/FAKS/Magistrsko delo/data/stopwords.txt'\n",
    "with open(filepath, 'r') as f:\n",
    "    additional_stopwords = f.read().splitlines()\n",
    "\n",
    "stop_words=stopwords.words('slovene') + list(string.punctuation) + additional_stopwords\n",
    "new_sw = [\"rt\",\"href\", \"http\", \"https\", \"quot\", \"nbsp\", \"mailto\", \"mail\", \"getty\", \"foto\", \"images\", \"urbanec\", \"sportid\"]\n",
    "stop_words.extend(new_sw)\n",
    "len(set(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9a71d339-7178-487f-a292-3dab91a8d641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'treba' in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0b7beb56-94a7-404e-bbdb-dc47a1e25aca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'ali',\n",
       " 'april',\n",
       " 'avgust',\n",
       " 'b',\n",
       " 'bi',\n",
       " 'bil',\n",
       " 'bila',\n",
       " 'bile',\n",
       " 'bili',\n",
       " 'bilo',\n",
       " 'biti',\n",
       " 'blizu',\n",
       " 'bo',\n",
       " 'bodo',\n",
       " 'bojo',\n",
       " 'bolj',\n",
       " 'bom',\n",
       " 'bomo',\n",
       " 'boste',\n",
       " 'bova',\n",
       " 'boĹˇ',\n",
       " 'brez',\n",
       " 'c',\n",
       " 'cel',\n",
       " 'cela',\n",
       " 'celi',\n",
       " 'celo',\n",
       " 'd',\n",
       " 'da',\n",
       " 'daleÄŤ',\n",
       " 'dan',\n",
       " 'danes',\n",
       " 'datum',\n",
       " 'december',\n",
       " 'deset',\n",
       " 'deseta',\n",
       " 'deseti',\n",
       " 'deseto',\n",
       " 'devet',\n",
       " 'deveta',\n",
       " 'deveti',\n",
       " 'deveto',\n",
       " 'do',\n",
       " 'dober',\n",
       " 'dobra',\n",
       " 'dobri',\n",
       " 'dobro',\n",
       " 'dokler',\n",
       " 'dol',\n",
       " 'dolg',\n",
       " 'dolga',\n",
       " 'dolgi',\n",
       " 'dovolj',\n",
       " 'drug',\n",
       " 'druga',\n",
       " 'drugi',\n",
       " 'drugo',\n",
       " 'dva',\n",
       " 'dve',\n",
       " 'e',\n",
       " 'eden',\n",
       " 'en',\n",
       " 'ena',\n",
       " 'ene',\n",
       " 'eni',\n",
       " 'enkrat',\n",
       " 'eno',\n",
       " 'etc.',\n",
       " 'f',\n",
       " 'februar',\n",
       " 'g',\n",
       " 'g.',\n",
       " 'ga',\n",
       " 'ga.',\n",
       " 'gor',\n",
       " 'gospa',\n",
       " 'gospod',\n",
       " 'h',\n",
       " 'halo',\n",
       " 'i',\n",
       " 'idr.',\n",
       " 'ii',\n",
       " 'iii',\n",
       " 'in',\n",
       " 'iv',\n",
       " 'ix',\n",
       " 'iz',\n",
       " 'j',\n",
       " 'januar',\n",
       " 'jaz',\n",
       " 'je',\n",
       " 'ji',\n",
       " 'jih',\n",
       " 'jim',\n",
       " 'jo',\n",
       " 'julij',\n",
       " 'junij',\n",
       " 'jutri',\n",
       " 'k',\n",
       " 'kadarkoli',\n",
       " 'kaj',\n",
       " 'kajti',\n",
       " 'kako',\n",
       " 'kakor',\n",
       " 'kamor',\n",
       " 'kamorkoli',\n",
       " 'kar',\n",
       " 'karkoli',\n",
       " 'katerikoli',\n",
       " 'kdaj',\n",
       " 'kdo',\n",
       " 'kdorkoli',\n",
       " 'ker',\n",
       " 'ki',\n",
       " 'kje',\n",
       " 'kjer',\n",
       " 'kjerkoli',\n",
       " 'ko',\n",
       " 'koder',\n",
       " 'koderkoli',\n",
       " 'koga',\n",
       " 'komu',\n",
       " 'kot',\n",
       " 'kratek',\n",
       " 'kratka',\n",
       " 'kratke',\n",
       " 'kratki',\n",
       " 'l',\n",
       " 'lahka',\n",
       " 'lahke',\n",
       " 'lahki',\n",
       " 'lahko',\n",
       " 'le',\n",
       " 'lep',\n",
       " 'lepa',\n",
       " 'lepe',\n",
       " 'lepi',\n",
       " 'lepo',\n",
       " 'leto',\n",
       " 'm',\n",
       " 'maj',\n",
       " 'majhen',\n",
       " 'majhna',\n",
       " 'majhni',\n",
       " 'malce',\n",
       " 'malo',\n",
       " 'manj',\n",
       " 'marec',\n",
       " 'me',\n",
       " 'med',\n",
       " 'medtem',\n",
       " 'mene',\n",
       " 'mesec',\n",
       " 'mi',\n",
       " 'midva',\n",
       " 'midve',\n",
       " 'mnogo',\n",
       " 'moj',\n",
       " 'moja',\n",
       " 'moje',\n",
       " 'mora',\n",
       " 'morajo',\n",
       " 'moram',\n",
       " 'moramo',\n",
       " 'morate',\n",
       " 'moraĹˇ',\n",
       " 'morem',\n",
       " 'mu',\n",
       " 'n',\n",
       " 'na',\n",
       " 'nad',\n",
       " 'naj',\n",
       " 'najina',\n",
       " 'najino',\n",
       " 'najmanj',\n",
       " 'naju',\n",
       " 'najveÄŤ',\n",
       " 'nam',\n",
       " 'narobe',\n",
       " 'nas',\n",
       " 'nato',\n",
       " 'nazaj',\n",
       " 'naĹˇ',\n",
       " 'naĹˇa',\n",
       " 'naĹˇe',\n",
       " 'ne',\n",
       " 'nedavno',\n",
       " 'nedelja',\n",
       " 'nek',\n",
       " 'neka',\n",
       " 'nekaj',\n",
       " 'nekatere',\n",
       " 'nekateri',\n",
       " 'nekatero',\n",
       " 'nekdo',\n",
       " 'neke',\n",
       " 'nekega',\n",
       " 'neki',\n",
       " 'nekje',\n",
       " 'neko',\n",
       " 'nekoga',\n",
       " 'nekoÄŤ',\n",
       " 'ni',\n",
       " 'nikamor',\n",
       " 'nikdar',\n",
       " 'nikjer',\n",
       " 'nikoli',\n",
       " 'niÄŤ',\n",
       " 'nje',\n",
       " 'njega',\n",
       " 'njegov',\n",
       " 'njegova',\n",
       " 'njegovo',\n",
       " 'njej',\n",
       " 'njemu',\n",
       " 'njen',\n",
       " 'njena',\n",
       " 'njeno',\n",
       " 'nji',\n",
       " 'njih',\n",
       " 'njihov',\n",
       " 'njihova',\n",
       " 'njihovo',\n",
       " 'njiju',\n",
       " 'njim',\n",
       " 'njo',\n",
       " 'njun',\n",
       " 'njuna',\n",
       " 'njuno',\n",
       " 'no',\n",
       " 'nocoj',\n",
       " 'november',\n",
       " 'npr.',\n",
       " 'o',\n",
       " 'ob',\n",
       " 'oba',\n",
       " 'obe',\n",
       " 'oboje',\n",
       " 'od',\n",
       " 'odprt',\n",
       " 'odprta',\n",
       " 'odprti',\n",
       " 'okoli',\n",
       " 'oktober',\n",
       " 'on',\n",
       " 'onadva',\n",
       " 'one',\n",
       " 'oni',\n",
       " 'onidve',\n",
       " 'osem',\n",
       " 'osma',\n",
       " 'osmi',\n",
       " 'osmo',\n",
       " 'oz.',\n",
       " 'p',\n",
       " 'pa',\n",
       " 'pet',\n",
       " 'peta',\n",
       " 'petek',\n",
       " 'peti',\n",
       " 'peto',\n",
       " 'po',\n",
       " 'pod',\n",
       " 'pogosto',\n",
       " 'poleg',\n",
       " 'poln',\n",
       " 'polna',\n",
       " 'polni',\n",
       " 'polno',\n",
       " 'ponavadi',\n",
       " 'ponedeljek',\n",
       " 'ponovno',\n",
       " 'potem',\n",
       " 'povsod',\n",
       " 'pozdravljen',\n",
       " 'pozdravljeni',\n",
       " 'prav',\n",
       " 'prava',\n",
       " 'prave',\n",
       " 'pravi',\n",
       " 'pravo',\n",
       " 'prazen',\n",
       " 'prazna',\n",
       " 'prazno',\n",
       " 'prbl.',\n",
       " 'precej',\n",
       " 'pred',\n",
       " 'prej',\n",
       " 'preko',\n",
       " 'pri',\n",
       " 'pribl.',\n",
       " 'pribliĹľno',\n",
       " 'primer',\n",
       " 'pripravljen',\n",
       " 'pripravljena',\n",
       " 'pripravljeni',\n",
       " 'proti',\n",
       " 'prva',\n",
       " 'prvi',\n",
       " 'prvo',\n",
       " 'r',\n",
       " 'ravno',\n",
       " 'redko',\n",
       " 'res',\n",
       " 'reÄŤ',\n",
       " 's',\n",
       " 'saj',\n",
       " 'sam',\n",
       " 'sama',\n",
       " 'same',\n",
       " 'sami',\n",
       " 'samo',\n",
       " 'se',\n",
       " 'sebe',\n",
       " 'sebi',\n",
       " 'sedaj',\n",
       " 'sedem',\n",
       " 'sedma',\n",
       " 'sedmi',\n",
       " 'sedmo',\n",
       " 'sem',\n",
       " 'september',\n",
       " 'seveda',\n",
       " 'si',\n",
       " 'sicer',\n",
       " 'skoraj',\n",
       " 'skozi',\n",
       " 'slab',\n",
       " 'smo',\n",
       " 'so',\n",
       " 'sobota',\n",
       " 'spet',\n",
       " 'sreda',\n",
       " 'srednja',\n",
       " 'srednji',\n",
       " 'sta',\n",
       " 'ste',\n",
       " 'stran',\n",
       " 'stvar',\n",
       " 'sva',\n",
       " 't',\n",
       " 'ta',\n",
       " 'tak',\n",
       " 'taka',\n",
       " 'take',\n",
       " 'taki',\n",
       " 'tako',\n",
       " 'takoj',\n",
       " 'tam',\n",
       " 'te',\n",
       " 'tebe',\n",
       " 'tebi',\n",
       " 'tega',\n",
       " 'teĹľak',\n",
       " 'teĹľka',\n",
       " 'teĹľki',\n",
       " 'teĹľko',\n",
       " 'ti',\n",
       " 'tista',\n",
       " 'tiste',\n",
       " 'tisti',\n",
       " 'tisto',\n",
       " 'tj.',\n",
       " 'tja',\n",
       " 'to',\n",
       " 'toda',\n",
       " 'torek',\n",
       " 'tretja',\n",
       " 'tretje',\n",
       " 'tretji',\n",
       " 'tri',\n",
       " 'tu',\n",
       " 'tudi',\n",
       " 'tukaj',\n",
       " 'tvoj',\n",
       " 'tvoja',\n",
       " 'tvoje',\n",
       " 'u',\n",
       " 'v',\n",
       " 'vaju',\n",
       " 'vam',\n",
       " 'vas',\n",
       " 'vaĹˇ',\n",
       " 'vaĹˇa',\n",
       " 'vaĹˇe',\n",
       " 've',\n",
       " 'vedno',\n",
       " 'velik',\n",
       " 'velika',\n",
       " 'veliki',\n",
       " 'veliko',\n",
       " 'vendar',\n",
       " 'ves',\n",
       " 'veÄŤ',\n",
       " 'vi',\n",
       " 'vidva',\n",
       " 'vii',\n",
       " 'viii',\n",
       " 'visok',\n",
       " 'visoka',\n",
       " 'visoke',\n",
       " 'visoki',\n",
       " 'vsa',\n",
       " 'vsaj',\n",
       " 'vsak',\n",
       " 'vsaka',\n",
       " 'vsakdo',\n",
       " 'vsake',\n",
       " 'vsaki',\n",
       " 'vsakomur',\n",
       " 'vse',\n",
       " 'vsega',\n",
       " 'vsi',\n",
       " 'vso',\n",
       " 'vÄŤasih',\n",
       " 'vÄŤeraj',\n",
       " 'x',\n",
       " 'z',\n",
       " 'za',\n",
       " 'zadaj',\n",
       " 'zadnji',\n",
       " 'zakaj',\n",
       " 'zaprta',\n",
       " 'zaprti',\n",
       " 'zaprto',\n",
       " 'zdaj',\n",
       " 'zelo',\n",
       " 'zunaj',\n",
       " 'ÄŤ',\n",
       " 'ÄŤe',\n",
       " 'ÄŤesto',\n",
       " 'ÄŤetrta',\n",
       " 'ÄŤetrtek',\n",
       " 'ÄŤetrti',\n",
       " 'ÄŤetrto',\n",
       " 'ÄŤez',\n",
       " 'ÄŤigav',\n",
       " 'Ĺˇ',\n",
       " 'Ĺˇest',\n",
       " 'Ĺˇesta',\n",
       " 'Ĺˇesti',\n",
       " 'Ĺˇesto',\n",
       " 'Ĺˇtiri',\n",
       " 'Ĺľ',\n",
       " 'Ĺľe']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fd4c2b5-a810-4ed5-94eb-61a3853d7de4",
   "metadata": {},
   "source": [
    "### preprocess text, tokenize and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ecba9bce-e86a-4c71-83b3-573e9e38bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_some_texts(columns, df):\n",
    "    text_idxs = [1, 2, 3]#7240, 7241, 8013, 14500, 16500, 16304, 18300,  21750, 34036]\n",
    "    for i in text_idxs:\n",
    "        for column in columns:\n",
    "            print(df[column].iloc[i])\n",
    "#print_some_texts(['text'])\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)            # remove urls\n",
    "    text = re.compile('\\S*@\\S*\\s?').sub(r'', text)  # remove mails\n",
    "    text = re.sub(\"@[A-Za-z0-9]+\",\"\", text)         # remove twitter handle\n",
    "    text = re.sub('\\s+', ' ', text)                 # remove new line\n",
    "    text = re.sub(\"&amp;\",\"\", text)                  # &amp; is a special character for ampersand\n",
    "    text = re.sub('<USER>', '', text)               # remove '<USER>' as there are some such strings as user or url is masked with this string\n",
    "    text = re.sub('<URL>', '', text)\n",
    "    text = re.sub('[^a-zA-Zčšž]', ' ', text)           # Remove punctuations\n",
    "    text = text.lower()                             # Convert to lowercase\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)#remove tags\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)            # remove special characters and digits\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    #print(text)\n",
    "    text = preprocess_text(text)\n",
    "    #print(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = []\n",
    "    # Filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation). (adapted from lab example)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            if token not in stop_words and len(token) > 2:\n",
    "                filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def tokenize_lemmatize_classla(text):\n",
    "    config = {\n",
    "    #'processors': 'tokenize, lemma', # Comma-separated list of processors to use\n",
    "    'lang': 'sl', # Language code for the language to build the Pipeline in\n",
    "    'tokenize_pretokenized': False, # Use pretokenized text as input and disable tokenization\n",
    "    'use_gpu': True,\n",
    "    'type':'nonstandard'    # initialize the default non-standard Slovenian pipeline, use hr for Croatian and sr for Serbian\n",
    "    }\n",
    "    nlp = classla.Pipeline(**config)\n",
    "\n",
    "    doc = nlp(text)     # run the pipeline\n",
    "    lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def classla_preprocess(text, stop_words, nlp):\n",
    "    preprocessed_body = []     # a list of words of a single article\n",
    "    for token in simple_preprocess(text, min_len=3, max_len=25):\n",
    "      # remove all words shorter than three characters\n",
    "      if token not in stop_words:\n",
    "        preprocessed_body.append(token)\n",
    "\n",
    "    doc = nlp(' '.join(preprocessed_body))\n",
    "    lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    return lemmas\n",
    "    \n",
    "def stemming(tokens):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems\n",
    "\n",
    "def lemmatizing(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c6231c70-c499-47d7-bd22-192e13ca2cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 18:43:33 INFO: Loading these models for language: sl (Slovenian):\n",
      "===========================\n",
      "| Processor | Package     |\n",
      "---------------------------\n",
      "| tokenize  | nonstandard |\n",
      "| pos       | nonstandard |\n",
      "| lemma     | nonstandard |\n",
      "===========================\n",
      "\n",
      "2022-10-02 18:43:33 INFO: Use device: cpu\n",
      "2022-10-02 18:43:33 INFO: Loading: tokenize\n",
      "2022-10-02 18:43:33 INFO: Loading: pos\n",
      "2022-10-02 18:43:34 INFO: Loading: lemma\n",
      "2022-10-02 18:43:36 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3h 30min 20s\n",
      "Wall time: 35min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['preprocessed_text']=df['body'].apply(preprocess_text)\n",
    "#df['tokenized_text']= df['preprocessed_text'].apply(tokenize) \n",
    "\n",
    "config = {\n",
    "'processors': 'tokenize, pos, lemma', # Comma-separated list of processors to use\n",
    "'lang': 'sl', # Language code for the language to build the Pipeline in\n",
    "'tokenize_pretokenized': False, # Use pretokenized text as input and disable tokenization\n",
    "'use_gpu': True,\n",
    "'type':'nonstandard'    # initialize the default non-standard Slovenian pipeline, use hr for Croatian and sr for Serbian\n",
    "}\n",
    "nlp = classla.Pipeline(**config)\n",
    "df['lemmatized_text']= df['preprocessed_text'].apply(classla_preprocess, args=(stop_words,nlp)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "54d3be2a-9cd6-4c27-b202-26c064a05cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>media</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243165</th>\n",
       "      <td>Nacionalni štab civilne zaščite je še v ponede...</td>\n",
       "      <td>MMC RTV Slovenija</td>\n",
       "      <td>Na Hrvaškem 213 novookuženih, v BiH-u 13 umrlih</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>nacionalni štab civilne zaščite je še v ponede...</td>\n",
       "      <td>[nacionalen, štab, civilen, zaščita, ponedelje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68701</th>\n",
       "      <td>Župan Radencev Roman Leljak je odredil zaprtje...</td>\n",
       "      <td>MMC RTV Slovenija</td>\n",
       "      <td>Test starša otroka iz vrtca pri Kapeli je bil ...</td>\n",
       "      <td>2020-05-22</td>\n",
       "      <td>upan radencev roman leljak je odredil zaprtje...</td>\n",
       "      <td>[upan, radencev, roman, Leljak, odrediti, zapr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282889</th>\n",
       "      <td>Nagrado Kristine Brenkove podeljuje Zbornica k...</td>\n",
       "      <td>MMC RTV Slovenija</td>\n",
       "      <td>Nagrada Kristine Brenkove za slikanico Timbukt...</td>\n",
       "      <td>2020-10-26</td>\n",
       "      <td>nagrado kristine brenkove podeljuje zbornica k...</td>\n",
       "      <td>[nagrada, kristina, Brenkov, podeljevati, zbor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420542</th>\n",
       "      <td>Inšpekcijski nadzor, redarska služba, notranja...</td>\n",
       "      <td>MMC RTV Slovenija</td>\n",
       "      <td>Pet občin združilo moči, da bi od države dobil...</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>inšpekcijski nadzor redarska služba notranja r...</td>\n",
       "      <td>[inšpekcijski, nadzor, redarski, služba, notra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300849</th>\n",
       "      <td>Za demokrate je to slab obet tudi v prihodnje,...</td>\n",
       "      <td>MMC RTV Slovenija</td>\n",
       "      <td>Izidi volitev v kongrese zveznih držav neprije...</td>\n",
       "      <td>2020-11-07</td>\n",
       "      <td>za demokrate je to slab obet tudi v prihodnje ...</td>\n",
       "      <td>[demokrat, slab, obet, prihodnji, meja, kongre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293120</th>\n",
       "      <td>Naj demokrati še tako napadajo ameriškega pred...</td>\n",
       "      <td>MMC RTV Slovenija</td>\n",
       "      <td>Trump ali Biden, kateri bo (iz)kopal ameriško ...</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>naj demokrati še tako napadajo ameriškega pred...</td>\n",
       "      <td>[demokrat, napadati, ameriški, predsednik, Don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413525</th>\n",
       "      <td>Country pop duet The Shires, ki ga sestavljata...</td>\n",
       "      <td>MMC RTV Slovenija</td>\n",
       "      <td>Radio Si</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>country pop duet the shires ki ga sestavljata ...</td>\n",
       "      <td>[country, pop, duet, the, shires, sestavljati,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78352</th>\n",
       "      <td>\"Zavedam se vznemirjenosti zaradi izjav, vezan...</td>\n",
       "      <td>MMC RTV Slovenija</td>\n",
       "      <td>Kacin: Maske bodo postale sestavni del naše ci...</td>\n",
       "      <td>2020-05-29</td>\n",
       "      <td>zavedam se vznemirjenosti zaradi izjav vezani...</td>\n",
       "      <td>[zavedati, vznemirjenost, izjava, vezan, pripo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200607</th>\n",
       "      <td>V tem času so testirali nekaj manj kot 3000 lj...</td>\n",
       "      <td>MMC RTV Slovenija</td>\n",
       "      <td>Na Hrvaškem 145 novih okužb, iz Istre pozivi L...</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>v tem času so testirali nekaj manj kot ljudi t...</td>\n",
       "      <td>[čas, testirati, manj, človek, trenutno, držav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246134</th>\n",
       "      <td>Pred dnevi je bistriški Impol objavil polletno...</td>\n",
       "      <td>MMC RTV Slovenija</td>\n",
       "      <td>Radio Maribor</td>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>pred dnevi je bistriški impol objavil polletno...</td>\n",
       "      <td>[dan, bistriški, Impol, objaviti, polleten, po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     body              media  \\\n",
       "243165  Nacionalni štab civilne zaščite je še v ponede...  MMC RTV Slovenija   \n",
       "68701   Župan Radencev Roman Leljak je odredil zaprtje...  MMC RTV Slovenija   \n",
       "282889  Nagrado Kristine Brenkove podeljuje Zbornica k...  MMC RTV Slovenija   \n",
       "420542  Inšpekcijski nadzor, redarska služba, notranja...  MMC RTV Slovenija   \n",
       "300849  Za demokrate je to slab obet tudi v prihodnje,...  MMC RTV Slovenija   \n",
       "...                                                   ...                ...   \n",
       "293120  Naj demokrati še tako napadajo ameriškega pred...  MMC RTV Slovenija   \n",
       "413525  Country pop duet The Shires, ki ga sestavljata...  MMC RTV Slovenija   \n",
       "78352   \"Zavedam se vznemirjenosti zaradi izjav, vezan...  MMC RTV Slovenija   \n",
       "200607  V tem času so testirali nekaj manj kot 3000 lj...  MMC RTV Slovenija   \n",
       "246134  Pred dnevi je bistriški Impol objavil polletno...  MMC RTV Slovenija   \n",
       "\n",
       "                                                    title        date  \\\n",
       "243165    Na Hrvaškem 213 novookuženih, v BiH-u 13 umrlih  2020-09-30   \n",
       "68701   Test starša otroka iz vrtca pri Kapeli je bil ...  2020-05-22   \n",
       "282889  Nagrada Kristine Brenkove za slikanico Timbukt...  2020-10-26   \n",
       "420542  Pet občin združilo moči, da bi od države dobil...  2020-01-08   \n",
       "300849  Izidi volitev v kongrese zveznih držav neprije...  2020-11-07   \n",
       "...                                                   ...         ...   \n",
       "293120  Trump ali Biden, kateri bo (iz)kopal ameriško ...  2020-11-02   \n",
       "413525                                           Radio Si  2020-03-03   \n",
       "78352   Kacin: Maske bodo postale sestavni del naše ci...  2020-05-29   \n",
       "200607  Na Hrvaškem 145 novih okužb, iz Istre pozivi L...  2020-09-01   \n",
       "246134                                      Radio Maribor  2020-10-02   \n",
       "\n",
       "                                        preprocessed_text  \\\n",
       "243165  nacionalni štab civilne zaščite je še v ponede...   \n",
       "68701    upan radencev roman leljak je odredil zaprtje...   \n",
       "282889  nagrado kristine brenkove podeljuje zbornica k...   \n",
       "420542  inšpekcijski nadzor redarska služba notranja r...   \n",
       "300849  za demokrate je to slab obet tudi v prihodnje ...   \n",
       "...                                                   ...   \n",
       "293120  naj demokrati še tako napadajo ameriškega pred...   \n",
       "413525  country pop duet the shires ki ga sestavljata ...   \n",
       "78352    zavedam se vznemirjenosti zaradi izjav vezani...   \n",
       "200607  v tem času so testirali nekaj manj kot ljudi t...   \n",
       "246134  pred dnevi je bistriški impol objavil polletno...   \n",
       "\n",
       "                                          lemmatized_text  \n",
       "243165  [nacionalen, štab, civilen, zaščita, ponedelje...  \n",
       "68701   [upan, radencev, roman, Leljak, odrediti, zapr...  \n",
       "282889  [nagrada, kristina, Brenkov, podeljevati, zbor...  \n",
       "420542  [inšpekcijski, nadzor, redarski, služba, notra...  \n",
       "300849  [demokrat, slab, obet, prihodnji, meja, kongre...  \n",
       "...                                                   ...  \n",
       "293120  [demokrat, napadati, ameriški, predsednik, Don...  \n",
       "413525  [country, pop, duet, the, shires, sestavljati,...  \n",
       "78352   [zavedati, vznemirjenost, izjava, vezan, pripo...  \n",
       "200607  [čas, testirati, manj, človek, trenutno, držav...  \n",
       "246134  [dan, bistriški, Impol, objaviti, polleten, po...  \n",
       "\n",
       "[5000 rows x 6 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b8212403-6c72-4cee-9675-e00d941bddc0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['organizator', 'bovški', 'maraton', 'podpisati', 'pogodba', 'sodelovanje', 'lokalen', 'podjetje', 'mines', 'ukvarjati', 'proizvodnja', 'vodovoden', 'izdelek', 'vodilen', 'program', 'sanitaren', 'armatura', 'bliziti', 'podjetje', 'glaven', 'sponzor', 'omenjen', 'pogodba', 'postati', 'generalen', 'pokrovitelj', 'podpis', 'večleten', 'pogodba', 'generalen', 'pokroviteljstvo', 'prireditev', 'preimenovati', 'blitz', 'Bovec', 'maraton', 'sodelovanje', 'podjetje', 'mines', 'odločiti', 'pomagati', 'lokalen', 'skupnost', 'direktor', 'Matej', 'Klavora', 'stran', 'dosti', 'mednarodno', 'pomemben', 'dogodek', 'vidik', 'blagoven', 'znamka', 'sigurno', 'lahko', 'pripomoči', 'glede', 'prepoznavnost', 'predsednik', 'porten', 'društvo', 'bovec', 'maraton', 'Vasja', 'vitez', 'sodelovanje', 'povedati', 'verjeti', 'doprinesti', 'dober', 'rezultat', 'velik', 'število', 'udeleženec', 'Bovec', 'maraton', 'priprava', 'maraton', 'začeti', 'februar', 'takrat', 'začeti', 'pogovarjati', 'ureditev', 'trasa', 'letos', 'blitz', 'točka', 'ukvarjati', 'proizvodnja', 'armatura', 'postaviti', 'točka', 'kjer', 'fizično', 'tuš', 'tekmovalec', 'lahko', 'stuširati', 'tek', 'pojasniti', 'Matej', 'Klavora', 'novost', 'letošnji', 'maraton', 'pokalen', 'tekmovanje', 'moder', 'trak', 'povezati', 'polmaraton', 'slovenski', 'italijanski', 'stran', 'tekmovati', 'Italijan', 'Slovenec', 'moški', 'konkurenca', 'šteti', 'italijan', 'Slovenec', 'ženski', 'ženski', 'stran', 'sosednji', 'zanimivo', 'lahko', 'posameznik', 'tekmovati', 'imeti', 'udeležba', 'tekmovanje', 'razložiti', 'Vasja', 'vitez', 'tekmovanje', 'moder', 'trak', 'januar', 'začeti', 'polmaraton', 'italijanski', 'kocjan', 'Soči', 'del', 'april', 'istrski', 'maraton', 'slediti', 'bovški', 'goriški', 'maraton']\n",
      "Organizatorji bovškega maratona so podpisali pogodbo o sodelovanju z lokalnim podjetjem Mines, ki se ukvarja s proizvodnjo vodovodnih izdelkov, njihov vodilni program so sanitarne armature Blizt.\n",
      "\n",
      "Že pred tem je bilo podjetje eno izmed glavnih sponzorjev, z omenjeno pogodbo pa je postalo generalni pokrovitelj. S podpisom večletne pogodbe o generalnem pokroviteljstvu se je prireditev preimenovala v Blitz Bovec maraton. Za sodelovanje so se v podjetju Mines odločili, saj želijo s tem pomagati lokalni skupnosti. Direktor Matej Klavora: \"Po drugi strani je to dosti mednarodno pomemben dogodek in tudi z vidika naše blagovne znamke sigurno nam lahko kaj pripomore glede prepoznavnosti.\"\n",
      "\n",
      "Predsednik Športnega društva Bovec maraton Vasja Vitez pa je o sodelovanju povedal: \"Verjamem, da bo to res doprineslo dobre rezultate in še večje število udeležencev na Bovec maratonu.\" Priprave na maraton so se začele v februarju. Takrat so se začeli pogovarjati o ureditvi tras, na katerih bodo letos tudi Blitz točke. \"Mi se ukvarjamo s proizvodnjo armatur, tako da bomo postavili dve taki točki, kjer bodo fizično tuši in se bodo tekmovalci lahko tudi stuširali med tekom\", nam je pojasnil Matej Klavora.\n",
      "\n",
      "Novost letošnjega maratona je pokalno tekmovanje za modri trak, ki bo povezalo štiri polmaratone na slovenski in italijanski strani. \"Tekmuje se med Italijani in Slovenci. Tako da v moški konkurenci šteje prvih šest Italijanov in šest Slovencev. V ženski pa tri ženske na naši strani in tri na sosednji. Zanimivo je, da lahko tudi vsi posamezniki tekmujejo, morajo imeti pa vsaj tri udeležbe na tekmovanjih.\" nam je razložil Vasja Vitez.\n",
      "\n",
      "Tekmovanje za modri trak se je januarja že začelo na polmaratonu v italijanskem Škocjanu ob Soči, drugi del bo aprila na istrskem maratonu, sledita še bovški in goriški maraton.\n",
      "organizatorji bovškega maratona so podpisali pogodbo o sodelovanju z lokalnim podjetjem mines ki se ukvarja s proizvodnjo vodovodnih izdelkov njihov vodilni program so sanitarne armature blizt e pred tem je bilo podjetje eno izmed glavnih sponzorjev z omenjeno pogodbo pa je postalo generalni pokrovitelj s podpisom večletne pogodbe o generalnem pokroviteljstvu se je prireditev preimenovala v blitz bovec maraton za sodelovanje so se v podjetju mines odločili saj želijo s tem pomagati lokalni skupnosti direktor matej klavora po drugi strani je to dosti mednarodno pomemben dogodek in tudi z vidika naše blagovne znamke sigurno nam lahko kaj pripomore glede prepoznavnosti predsednik portnega društva bovec maraton vasja vitez pa je o sodelovanju povedal verjamem da bo to res doprineslo dobre rezultate in še večje število udeležencev na bovec maratonu priprave na maraton so se začele v februarju takrat so se začeli pogovarjati o ureditvi tras na katerih bodo letos tudi blitz točke mi se ukvarjamo s proizvodnjo armatur tako da bomo postavili dve taki točki kjer bodo fizično tuši in se bodo tekmovalci lahko tudi stuširali med tekom nam je pojasnil matej klavora novost letošnjega maratona je pokalno tekmovanje za modri trak ki bo povezalo štiri polmaratone na slovenski in italijanski strani tekmuje se med italijani in slovenci tako da v moški konkurenci šteje prvih šest italijanov in šest slovencev v ženski pa tri ženske na naši strani in tri na sosednji zanimivo je da lahko tudi vsi posamezniki tekmujejo morajo imeti pa vsaj tri udeležbe na tekmovanjih nam je razložil vasja vitez tekmovanje za modri trak se je januarja že začelo na polmaratonu v italijanskem kocjanu ob soči drugi del bo aprila na istrskem maratonu sledita še bovški in goriški maraton \n",
      "['Nemec', 'sestaviti', 'constantin', 'Schmid', 'pius', 'paschek', 'Markus', 'eisenbichler', 'Karl', 'Geiger', 'tekma', 'suveren', 'zalomiti', 'povsem', 'konec', 'naslov', 'prvak', 'odločati', 'novopečen', 'svetoven', 'prvak', 'podprvak', 'Egner', 'halvor', 'Granerud', 'skočiti', 'vreči', 'rokavica', 'geiger', 'zadnji', 'polet', 'prvenstvo', 'pristati', 'skromen', 'sreča', 'konkurenca', 'posameznik', 'stran', 'potem', 'točka', 'osvojiti', 'naslov', 'prvak', 'tokrat', 'letnik', 'iziti', 'napet', 'sekunda', 'postati', 'jasno', 'norveški', 'ubraniti', 'naslov', 'prvak', 'oberstdorf', 'daniel', 'Andre', 'Tande', 'Johann', 'Andre', 'Forfang', 'Robert', 'Johansson', 'Granerud', 'zbrati', 'točka', 'Nemec', 'mesto', 'osvojiti', 'Poljak', 'točka', 'skakati', 'sestava', 'Piotr', 'zyla', 'Andrzej', 'stekati', 'kamil', 'stoch', 'dawid', 'kubacka', 'Lanišek', 'dvakrat', 'Slovenec', 'osvojiti', 'mesto', 'izključen', 'timi', 'zajec', 'ekipen', 'tekma', 'skakati', 'Prevc', 'posamičen', 'tekma', 'petek', 'predtekmovalec', 'FIS', 'majhen', 'število', 'ekipa', 'dejstvo', 'domač', 'reprezentanca', 'izključitev', 'zajec', 'nastopiti', 'izjema', 'registracija', 'slovenski', 'tekmovalec', 'ekipen', 'tekma', 'boriti', 'brat', 'domen', 'Prevc', 'bor', 'Pavlovčič', 'Anže', 'Lanišek', 'Domen', 'Prevc', 'Slovenec', 'serija', 'doskočiti', 'meter', 'Prevc', 'leteti', 'meter', 'dolgo', 'dolg', 'polet', 'letošnji', 'planica', 'Pavlovčič', 'pristati', 'Lanišek', 'zablesteti', 'dosežek', 'serija', 'del', 'dolg', 'geiger', 'granerud', 'del', 'brat', 'Prevc', 'uspeti', 'odličen', 'polet', 'domen', 'doskočiti', 'Pavlovčič', 'pristati', 'Lanišek', 'nato', 'zadnji', 'skok', 'izboljšati', 'dolg', 'slovenski', 'daljava', 'dan', 'daljava', 'finalen', 'serija', 'uspeti', 'eisenbichler', 'pristati', 'Prevc', 'dobiti', 'veliko', 'zagon', 'naprej', 'finalen', 'polet', 'pomeniti', 'potrditev', 'tekma', 'vedno', 'znati', 'sestaviti', 'dajati', 'veliko', 'zagon', 'naprej', 'danes', 'veliko', 'pritisk', 'sam', 'naložiti', 'vesel', 'uspešno', 'prenesti', 'uspešen', 'nastop', 'televizija', 'Slovenija', 'dejati', 'Prevc']\n",
      "Nemci v sestavi Constantin Schmid, Pius Paschke, Markus Eisenbichler in Karl Geiger so bili vso tekmo suvereni, zalomilo se jim je povsem na koncu, ko sta o naslovu prvaku odločala novopečena svetovni prvak in podprvak.\n",
      "\n",
      "Egner Halvor Granerud je skočil 234,5 m in vrgel rokavico Geigerju, ki je z zadnjim poletom prvenstva pristal pri zanj skromnih 224,5 m. Če je bila sreča v konkurenci posameznikov na njegovi strani, ko je za pol točke osvojil naslov prvaka, se tokrat 27-letniku ni izšlo. Po nekaj napetih sekundah je postalo jasno: Norveška je ubranila naslov prvaka iz Oberstdorfa 2018.\n",
      "\n",
      "Daniel Andre Tande, Johann Andre Forfang, Robert Johansson in Granerud so zbrali 1727,7 točk, Nemci 1708,5. Tretje mesto so osvojili Poljaki (1665,5 točke), ki so skakali v sestavi Piotr Zyla, Andrzej Stekala, Kamil Stoch in Dawid Kubacki.\n",
      "\n",
      "Lanišek dvakrat prek 230 m\n",
      "\n",
      "Slovenci so osvojili četrto mesto. Namesto izključenega Timija Zajca je na ekipni tekmi skakal Peter Prevc, čeprav je bil na posamični tekmi v petek predtekmovalec. FIS je ob že tako majhnem številu ekip in dejstvu, da domača reprezentanca po izključitvi Zajca sploh ne bi smela nastopiti, vendarle dovolila izjemo za registracijo četrtega slovenskega tekmovalca. Ob njem so se na ekipni tekmi borili še brat Domen Prevc, Bor Pavlovčič in Anže Lanišek. Domen Prevc je kot prvi Slovenec v prvi seriji doskočil pri 211 metrih. Peter Prevc je letel še 10 metrov dlje, kar je njegov najdaljši polet v letošnji Planici. Pavlovčič je pristal pri 200 m, Lanišek pa je zablestel z 231 m, kar je bil tretji dosežek serije. V prvem delu sta bila daljša le Geiger (238 m) in Granerud (235 m).\n",
      "\n",
      "V drugem delu sta bratoma Prevc uspela odlična poleta, Domen je doskočil pri 226 m, Peter pri 218,5 m. Pavlovčič je pristal pri 201 m, Lanišek pa je nato z zadnjim skokom še izboljšal najdaljšo slovensko daljavo dneva - 231,5 m. Daljava finalne serije je sicer uspela Eisenbichlerju, pristal je pri 236,5 m.\n",
      "\n",
      "P. Prevc dobil veliko zagona za naprej\n",
      "\n",
      "\"Finalni polet je zame pomenil potrditev, da se za tekmo še vedno znam sestaviti in to mi daje veliko zagona za naprej. Danes je bilo na meni namreč veliko pritiska, sam sem si ga naložil, in vesel sem, da sem ga uspešno prenesel,\" je po uspešnem nastopu za Televizijo Slovenija dejal Peter Prevc.\n",
      "nemci v sestavi constantin schmid pius paschke markus eisenbichler in karl geiger so bili vso tekmo suvereni zalomilo se jim je povsem na koncu ko sta o naslovu prvaku odločala novopečena svetovni prvak in podprvak egner halvor granerud je skočil m in vrgel rokavico geigerju ki je z zadnjim poletom prvenstva pristal pri zanj skromnih m e je bila sreča v konkurenci posameznikov na njegovi strani ko je za pol točke osvojil naslov prvaka se tokrat letniku ni izšlo po nekaj napetih sekundah je postalo jasno norveška je ubranila naslov prvaka iz oberstdorfa daniel andre tande johann andre forfang robert johansson in granerud so zbrali točk nemci tretje mesto so osvojili poljaki točke ki so skakali v sestavi piotr zyla andrzej stekala kamil stoch in dawid kubacki lanišek dvakrat prek m slovenci so osvojili četrto mesto namesto izključenega timija zajca je na ekipni tekmi skakal peter prevc čeprav je bil na posamični tekmi v petek predtekmovalec fis je ob že tako majhnem številu ekip in dejstvu da domača reprezentanca po izključitvi zajca sploh ne bi smela nastopiti vendarle dovolila izjemo za registracijo četrtega slovenskega tekmovalca ob njem so se na ekipni tekmi borili še brat domen prevc bor pavlovčič in anže lanišek domen prevc je kot prvi slovenec v prvi seriji doskočil pri metrih peter prevc je letel še metrov dlje kar je njegov najdaljši polet v letošnji planici pavlovčič je pristal pri m lanišek pa je zablestel z m kar je bil tretji dosežek serije v prvem delu sta bila daljša le geiger m in granerud m v drugem delu sta bratoma prevc uspela odlična poleta domen je doskočil pri m peter pri m pavlovčič je pristal pri m lanišek pa je nato z zadnjim skokom še izboljšal najdaljšo slovensko daljavo dneva m daljava finalne serije je sicer uspela eisenbichlerju pristal je pri m p prevc dobil veliko zagona za naprej finalni polet je zame pomenil potrditev da se za tekmo še vedno znam sestaviti in to mi daje veliko zagona za naprej danes je bilo na meni namreč veliko pritiska sam sem si ga naložil in vesel sem da sem ga uspešno prenesel je po uspešnem nastopu za televizijo slovenija dejal peter prevc \n",
      "['lik', 'magda', 'glasen', 'hišen', 'pomočnica', 'miranda', 'hobeti', 'Cynthia', 'Nixon', 'Cohenov', 'upodobiti', 'filmski', 'nadaljevanje', 'serija', 'vloga', 'pokazati', 'ženski', 'starost', 'pameten', 'zlodej', 'zelo', 'ukazovalen', 'razumeti', 'spolnost', 'serija', 'potrebovati', 'lik', 'magda', 'povečati', 'okvir', 'ustvarjati', 'imeti', 'tipičen', 'starka', 'plesen', 'dom', 'upokojenec', 'ženski', 'delo', 'trpeti', 'butec', 'leto', 'vloga', 'povedati', 'cosmopolitan', 'dolg', 'kariera', 'igrati', 'pretežno', 'gledališče', 'film', 'televizija', 'preiti', 'leto', 'starost']\n",
      "Lik Magde, glasne hišne pomočnice Mirande Hobbe (Cynthia Nixon), je Cohenova upodobila tudi v obeh filmskih nadaljevanjih serije.\n",
      "\n",
      "\"Vloga je pokazala žensko drugačne starosti, ki je pametna kot zlodej, zelo ukazovalna, poleg tega pa tudi razume spolnost, serija je potrebovala tak lik. Magda je povečala okvir, na katerem so ustvarjali, da ne bi imeli samo tipične starke, kako plesni v nekem domu za upokojence, ampak žensko, ki dela in ne trpi butcev,\" je leta 2018 o vlogi povedala za Cosmopolitan.\n",
      "\n",
      "V svoji dolgi karieri je igrala pretežno v gledališču, k filmu in televiziji pa je prešla šele po 60. letu starosti.\n",
      "lik magde glasne hišne pomočnice mirande hobbe cynthia nixon je cohenova upodobila tudi v obeh filmskih nadaljevanjih serije vloga je pokazala žensko drugačne starosti ki je pametna kot zlodej zelo ukazovalna poleg tega pa tudi razume spolnost serija je potrebovala tak lik magda je povečala okvir na katerem so ustvarjali da ne bi imeli samo tipične starke kako plesni v nekem domu za upokojence ampak žensko ki dela in ne trpi butcev je leta o vlogi povedala za cosmopolitan v svoji dolgi karieri je igrala pretežno v gledališču k filmu in televiziji pa je prešla šele po letu starosti \n"
     ]
    }
   ],
   "source": [
    "print_some_texts([\"lemmatized_text\", \"body\", \"preprocessed_text\" ], df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "357ea9f1-b77b-437c-9986-891c97d1e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed df\n",
    "df.to_parquet('C:/Users/hladn/FAKS/Magistrsko delo/data/eventregistry/df_news_lemmas_5000.parquet.gzip',compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aff9147a-b746-408d-8a76-e1edb24b6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(list(df['lemmatized_text']))\n",
    "corpus = [dictionary.doc2bow(text) for text in list(df['lemmatized_text'])]\n",
    "\n",
    "#pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "#dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fb17f4c7-caa2-48d0-b864-4f7ee31dafac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 22s\n",
      "Wall time: 3min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set training parameters.\n",
    "num_topics = 20\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d0894523-55c2-4e9d-ad76-3132da53ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.030*\"tekma\" + 0.015*\"točka\" + 0.014*\"minuta\" + 0.012*\"sezona\" + 0.009*\"zmaga\" + 0.008*\"liga\" + 0.008*\"ekipa\" + 0.008*\"zadnji\" + 0.007*\"igrati\" + 0.007*\"igra\"')\n",
      "(1, '0.028*\"nov\" + 0.027*\"okužba\" + 0.020*\"človek\" + 0.018*\"koronavirus\" + 0.015*\"država\" + 0.013*\"dan\" + 0.013*\"več\" + 0.012*\"potrditi\" + 0.012*\"število\" + 0.011*\"primer\"')\n",
      "(2, '0.053*\"aplikacija\" + 0.021*\"uporabnik\" + 0.018*\"telefon\" + 0.012*\"kralj\" + 0.009*\"stik\" + 0.009*\"podatek\" + 0.006*\"poroka\" + 0.004*\"Drava\" + 0.004*\"pameten\" + 0.004*\"mobilen\"')\n",
      "(3, '0.012*\"okužba\" + 0.012*\"bolnik\" + 0.011*\"dom\" + 0.010*\"zdravstven\" + 0.009*\"imeti\" + 0.008*\"zaposlen\" + 0.008*\"bolnišnica\" + 0.007*\"nov\" + 0.007*\"lahko\" + 0.007*\"covid\"')\n",
      "(4, '0.022*\"imeti\" + 0.017*\"lahko\" + 0.011*\"zelo\" + 0.009*\"čas\" + 0.009*\"iti\" + 0.007*\"veliko\" + 0.007*\"zdaj\" + 0.007*\"človek\" + 0.007*\"velik\" + 0.007*\"bolj\"')\n",
      "(5, '0.025*\"potres\" + 0.008*\"odpadek\" + 0.007*\"hrvaški\" + 0.007*\"dovolilnica\" + 0.007*\"obveznica\" + 0.006*\"območje\" + 0.006*\"poročati\" + 0.006*\"Zagreb\" + 0.005*\"sosežig\" + 0.005*\"stavba\"')\n",
      "(6, '0.014*\"ZDA\" + 0.012*\"država\" + 0.012*\"ameriški\" + 0.011*\"predsednik\" + 0.010*\"leto\" + 0.009*\"Trump\" + 0.009*\"dejati\" + 0.008*\"volitev\" + 0.007*\"sodišče\" + 0.007*\"več\"')\n",
      "(7, '0.014*\"restavracija\" + 0.013*\"vino\" + 0.010*\"gost\" + 0.007*\"velenjski\" + 0.007*\"turističen\" + 0.006*\"jed\" + 0.006*\"tempelj\" + 0.006*\"turizem\" + 0.006*\"kri\" + 0.006*\"gostinec\"')\n",
      "(8, '0.017*\"leto\" + 0.015*\"evro\" + 0.014*\"odstotek\" + 0.010*\"milijon\" + 0.009*\"podjetje\" + 0.008*\"delo\" + 0.007*\"več\" + 0.006*\"projekt\" + 0.006*\"velik\" + 0.006*\"pomoč\"')\n",
      "(9, '0.010*\"leto\" + 0.009*\"voda\" + 0.008*\"letalo\" + 0.008*\"požar\" + 0.007*\"letalski\" + 0.007*\"vesolje\" + 0.007*\"satelit\" + 0.007*\"vesoljski\" + 0.006*\"velik\" + 0.006*\"letališče\"')\n",
      "(10, '0.015*\"območje\" + 0.014*\"cesta\" + 0.013*\"prehod\" + 0.012*\"gasilec\" + 0.010*\"mejen\" + 0.010*\"veter\" + 0.010*\"Slovenija\" + 0.009*\"objekt\" + 0.009*\"Prevc\" + 0.008*\"voda\"')\n",
      "(11, '0.013*\"kitajski\" + 0.011*\"mesto\" + 0.009*\"protest\" + 0.009*\"nov\" + 0.008*\"vozilo\" + 0.008*\"avtomobil\" + 0.008*\"leto\" + 0.007*\"več\" + 0.007*\"policija\" + 0.006*\"protestnik\"')\n",
      "(12, '0.025*\"šola\" + 0.012*\"otrok\" + 0.011*\"policist\" + 0.009*\"policija\" + 0.008*\"učenec\" + 0.006*\"policijski\" + 0.006*\"šolski\" + 0.006*\"učitelj\" + 0.006*\"pouk\" + 0.006*\"ura\"')\n",
      "(13, '0.020*\"leto\" + 0.012*\"film\" + 0.008*\"slovenski\" + 0.006*\"delo\" + 0.006*\"nov\" + 0.005*\"festival\" + 0.005*\"nagrada\" + 0.004*\"zgodba\" + 0.004*\"knjiga\" + 0.004*\"glasba\"')\n",
      "(14, '0.032*\"hrvaški\" + 0.014*\"pes\" + 0.010*\"kraljev\" + 0.007*\"oblikovalec\" + 0.007*\"Harry\" + 0.006*\"moden\" + 0.005*\"žival\" + 0.005*\"princ\" + 0.005*\"oblikovanje\" + 0.005*\"kraljica\"')\n",
      "(15, '0.028*\"leto\" + 0.012*\"svetoven\" + 0.011*\"tekmovanje\" + 0.010*\"turnir\" + 0.009*\"prvenstvo\" + 0.009*\"šport\" + 0.009*\"igra\" + 0.009*\"olimpijski\" + 0.008*\"zveza\" + 0.007*\"Slovenija\"')\n",
      "(16, '0.015*\"vlada\" + 0.012*\"stranka\" + 0.008*\"predlog\" + 0.008*\"zakon\" + 0.007*\"minister\" + 0.007*\"predsednik\" + 0.007*\"Slovenija\" + 0.006*\"nov\" + 0.006*\"poslanec\" + 0.006*\"javen\"')\n",
      "(17, '0.020*\"dirka\" + 0.012*\"etapa\" + 0.011*\"mesto\" + 0.008*\"cilj\" + 0.008*\"sekunda\" + 0.008*\"zmaga\" + 0.008*\"zadnji\" + 0.008*\"ekipa\" + 0.007*\"dober\" + 0.007*\"kolesar\"')\n",
      "(18, '0.025*\"klub\" + 0.020*\"liga\" + 0.016*\"sezona\" + 0.011*\"igralec\" + 0.009*\"Koper\" + 0.008*\"leto\" + 0.007*\"nov\" + 0.007*\"nogometaš\" + 0.005*\"ekipa\" + 0.005*\"koprski\"')\n",
      "(19, '0.016*\"država\" + 0.014*\"ukrep\" + 0.013*\"vlada\" + 0.010*\"evropski\" + 0.010*\"meja\" + 0.009*\"lahko\" + 0.008*\"Slovenija\" + 0.006*\"veljati\" + 0.006*\"dejati\" + 0.005*\"minister\"')\n"
     ]
    }
   ],
   "source": [
    "topics = model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "de202b06-d6b1-40f6-af02-a482f6bb29a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hladn\\miniconda3\\envs\\mag\\lib\\site-packages\\pyLDAvis\\_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# feed the LDA model into the pyLDAvis instance\n",
    "lda_viz = gensimvis.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "509e34b2-7cf0-49d2-afcc-224b028e6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyLDAvis.display(lda_viz)\n",
    "pyLDAvis.save_html(lda_viz, 'lda.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a6815e-b8a1-410c-819d-d733356e2578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c31ff57-dc8e-444b-b4cf-8313bc51ec45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400bb4b3-46a2-4de2-8e4b-1e99a8e674f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f0ba355-125d-4e87-b5be-a014a79e639b",
   "metadata": {},
   "source": [
    "## classla experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8313d0f8-dce4-4d68-9c0e-d7984ffbf961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 17:08:16 INFO: Loading these models for language: sl (Slovenian):\n",
      "===========================\n",
      "| Processor | Package     |\n",
      "---------------------------\n",
      "| tokenize  | nonstandard |\n",
      "| pos       | nonstandard |\n",
      "| lemma     | nonstandard |\n",
      "===========================\n",
      "\n",
      "2022-10-02 17:08:16 INFO: Use device: cpu\n",
      "2022-10-02 17:08:16 INFO: Loading: tokenize\n",
      "2022-10-02 17:08:16 INFO: Loading: pos\n",
      "2022-10-02 17:08:18 INFO: Loading: lemma\n",
      "2022-10-02 17:08:20 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "'processors': 'tokenize, pos, lemma', # Comma-separated list of processors to use\n",
    "'lang': 'sl', # Language code for the language to build the Pipeline in\n",
    "'tokenize_pretokenized': False, # Use pretokenized text as input and disable tokenization\n",
    "'use_gpu': True,\n",
    "'type':'nonstandard'    # initialize the default non-standard Slovenian pipeline, use hr for Croatian and sr for Serbian\n",
    "}\n",
    "nlp = classla.Pipeline(**config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "848cdd01-fddc-4006-9011-94598ea582e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_longer = \"Tekmovanje za modri trak se je januarja že začelo na polmaratonu v italijanskem Škocjanu ob Soči, drugi del bo aprila na istrskem maratonu, sledita še bovški in goriški maraton. organizatorji bovškega maratona so podpisali pogodbo o sodelovanju z lokalnim podjetjem mines ki se ukvarja s proizvodnjo vodovodnih izdelkov njihov vodilni program so sanitarne armature blizt e pred tem je bilo podjetje eno izmed glavnih sponzorjev z omenjeno pogodbo pa je postalo generalni pokrovitelj s podpisom večletne pogodbe o generalnem pokroviteljstvu se je prireditev preimenovala v blitz bovec maraton za sodelovanje so se v podjetju mines odločili saj želijo s tem pomagati lokalni skupnosti direktor matej klavora po drugi strani je to dosti mednarodno pomemben dogodek in tudi z vidika naše blagovne znamke sigurno nam lahko kaj pripomore glede prepoznavnosti predsednik portnega društva bovec maraton vasja vitez pa je o sodelovanju povedal verjamem da bo to res doprineslo dobre rezultate in še večje število udeležencev na bovec maratonu priprave na maraton so se začele v februarju takrat so se začeli pogovarjati o ureditvi tras na katerih bodo letos tudi blitz točke mi se ukvarjamo s proizvodnjo armatur tako da bomo postavili dve taki točki kjer bodo fizično tuši in se bodo tekmovalci lahko tudi stuširali med tekom nam je pojasnil matej klavora novost letošnjega maratona je pokalno tekmovanje za modri trak ki bo povezalo štiri polmaratone na slovenski in italijanski strani tekmuje se med italijani in slovenci tako da v moški konkurenci šteje prvih šest italijanov in šest slovencev v ženski pa tri ženske na naši strani in tri na sosednji zanimivo je da lahko tudi vsi posamezniki tekmujejo morajo imeti pa vsaj tri udeležbe na tekmovanjih nam je razložil vasja vitez tekmovanje za modri trak se je januarja že začelo na polmaratonu v italijanskem kocjanu ob soči drugi del bo aprila na istrskem maratonu sledita še bovški in goriški maraton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "91512851-0133-4a76-806a-5841c4c06271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9min 1s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(200):\n",
    "    doc = nlp(text_longer)     # run the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "08f5e153-03b5-441d-a317-3de0e229b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_200 = df.sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "387b588c-9b31-49cb-ba91-a79c6e61feac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8min 27s\n",
      "Wall time: 1min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33029     [negotovost, kdaj, lahko, vrniti, teren, povzr...\n",
       "169792    [protestnik, zahtevati, odstop, vlada, Džazira...\n",
       "325416    [ameriški, različica, britanski, televizijski,...\n",
       "405613    [predčasen, volitev, odstotek, vprašan, voliti...\n",
       "10120     [zanimiv, lahko, dojemanje, avtomobilski, znam...\n",
       "                                ...                        \n",
       "368760    [televizija, Slovenija, poročati, neuraden, in...\n",
       "212217    [relativno, miren, etapa, pričakovanje, končat...\n",
       "79539     [človek, covid, hospitaliziran, intenziven, ne...\n",
       "356927    [šprint, kontiolahtij, dober, slovenski, biatl...\n",
       "162755    [družba, news, corporation, ustanoviti, danes,...\n",
       "Name: preprocessed_text, Length: 200, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_200['preprocessed_text'].apply(classla_preprocess, args=(stop_words,nlp)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df9b423e-10ca-4eb0-a8e4-e2e453fd18dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 16:33:48 INFO: Loading these models for language: sl (Slovenian):\n",
      "===========================\n",
      "| Processor | Package     |\n",
      "---------------------------\n",
      "| tokenize  | nonstandard |\n",
      "| pos       | nonstandard |\n",
      "| lemma     | nonstandard |\n",
      "| depparse  | standard    |\n",
      "| ner       | nonstandard |\n",
      "===========================\n",
      "\n",
      "2022-10-02 16:33:48 INFO: Use device: cpu\n",
      "2022-10-02 16:33:48 INFO: Loading: tokenize\n",
      "2022-10-02 16:33:48 INFO: Loading: pos\n",
      "2022-10-02 16:33:49 INFO: Loading: lemma\n",
      "2022-10-02 16:33:51 INFO: Loading: depparse\n",
      "2022-10-02 16:33:52 INFO: Loading: ner\n",
      "2022-10-02 16:33:52 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# newpar id = 1\n",
      "# sent_id = 1.1\n",
      "# text = kva smo mi zural zadn let v zagrebu...\n",
      "1\tkva\tkaj\tPRON\tPq-nsa\tCase=Acc|Gender=Neut|Number=Sing|PronType=Int\t4\tobj\t_\tNER=O\n",
      "2\tsmo\tbiti\tAUX\tVa-r1p-n\tMood=Ind|Number=Plur|Person=1|Polarity=Pos|Tense=Pres|VerbForm=Fin\t4\taux\t_\tNER=O\n",
      "3\tmi\tjaz\tPRON\tPp1-sd--y\tCase=Dat|Number=Plur|Person=1|PronType=Prs|Variant=Short\t4\tiobj\t_\tNER=O\n",
      "4\tzural\tzurati\tVERB\tVmep-pm\tAspect=Perf|Gender=Masc|Number=Plur|VerbForm=Part\t0\troot\t_\tNER=O\n",
      "5\tzadn\tzadnji\tADJ\tAgpnsa\tCase=Acc|Degree=Pos|Gender=Masc|Number=Sing\t6\tamod\t_\tNER=O\n",
      "6\tlet\tleto\tNOUN\tNcnsa\tCase=Acc|Gender=Neut|Number=Sing\t4\tobl\t_\tNER=O\n",
      "7\tv\tv\tADP\tSl\tCase=Loc\t8\tcase\t_\tNER=O\n",
      "8\tzagrebu\tZagreb\tPROPN\tNpmsl\tCase=Loc|Gender=Masc|Number=Sing\t4\tobl\t_\tNER=B-LOC|SpaceAfter=No\n",
      "9\t...\t...\tPUNCT\tZ\t_\t4\tpunct\t_\tNER=O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = classla.Pipeline('sl', type='nonstandard')  # initialize the default non-standard Slovenian pipeline, use hr for Croatian and sr for Serbian\n",
    "doc = nlp(\"kva smo mi zural zadn let v zagrebu...\")     # run the pipeline\n",
    "print(doc.to_conll())  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
